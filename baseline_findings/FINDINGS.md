# Baseline Findings: Multimodal Incongruence Detection

## Task

Given a student's answer to a question about a scientific figure, can an LLM detect
whether the answer is **Correct**, **Partially Correct**, or **Incorrect** — and if wrong,
categorize the error as **Omission**, **Factual**, or **Conceptual**?

---

## Setup

**Model:** `Qwen/Qwen3-VL-8B-Instruct` via Together.ai (same model for all scenarios)
**Dataset:** 174 augmented QA pairs from SPIQA test-A (30 papers), student answers
generated by GPT-4.1 via `icl.py`
**Eval sample:** 50 examples, equal-stratum sampled across (verdict × error category)
**Prompt:** `prompts/incongruence_eval_v3.txt` — includes verdict definitions, reference
answer field, 3 few-shot exemplars (correct / partially correct / incorrect)
**Seed:** 42 (fixed — all scenarios use identical 50 examples)

### Stratum distribution (50 examples)

| Verdict | Error Category | Count |
|---|---|---|
| Correct | N/A | 9 |
| Incorrect | Factual | 8 |
| Incorrect | Conceptual | 8 |
| Incorrect | Omission | 1 |
| Partially Correct | Factual | 8 |
| Partially Correct | Omission | 8 |
| Partially Correct | Conceptual | 8 |

---

## Experimental Scenarios (2 × 4 = 8 conditions)

All scenarios tested with and without the reference (ground truth) answer in the prompt.

| Scenario | Caption | Image | With Answer | Without Answer |
|---|---|---|---|---|
| Text-only | ❌ | ❌ | **64%** | 56% |
| Caption-only | ✅ | ❌ | **64%** | 48% |
| Vision-only | ❌ | ✅ | **58%** | 50% |
| Multimodal | ✅ | ✅ | **58%** | 48% |

Metric: **verdict accuracy** (exact match on Correct / Partially Correct / Incorrect).

---

## Key Findings

### 1. The reference answer is the dominant signal

Removing the reference answer drops every scenario by 6–16pp. The model is primarily
doing answer comparison ("does this match the reference?"), not independent figure
reasoning. This is expected given the task design but limits what we can conclude about
true visual understanding.

### 2. Caption adds zero over question-only when the reference answer is present

Text-only and caption-only both achieve 64%. Once the model has the reference answer to
compare against, the figure caption provides no additional information. The reference
answer already encodes what the figure shows.

### 3. The image slightly hurts accuracy when the reference answer is present

Vision-only (58%) and multimodal (58%) score 6pp below text-only (64%). Adding image
pixels appears to distract the model or introduce noise at 8B scale, rather than helping.
The reference answer is already sufficient context; the image adds nothing.

### 4. Without the reference answer, vision-only beats caption-only

50% vs 48% — the first configuration where the image provides a genuine advantage. When
the model must independently assess correctness from the figure (no reference answer),
seeing the actual image is marginally more useful than reading a textual caption. This
gap is small at 8B scale but would likely widen with a stronger vision model.

### 5. Partially Correct is the consistently hard class

Across all conditions the model tends to collapse Partially Correct into Incorrect. The
3-way distinction (Correct / Partially Correct / Incorrect) is substantially harder than
binary classification. Adding verdict definitions and few-shot exemplars helped (+10pp
over the baseline prompt) but did not solve the problem — this likely requires a larger
model or a different prompting strategy.

### 6. Prompt engineering progression

| Prompt Version | Change | Text-only Accuracy |
|---|---|---|
| v1 | Mixed models (llama-70b + Llama-3.2-11B-Vision), no verdict defs | 50% |
| v2 | Same model (Qwen3-VL-8B) + verdict definitions | 60% |
| v3 | + reference answer + /no_think + equal strata | 58% |
| v4 (final) | + 3 ICL exemplars from icl.py | **64%** |

---

## Limitations

1. **Synthetic ground truth.** Student answers and verdicts were generated by GPT-4.1
   (`icl.py`), not human-annotated. Accuracy measures model-to-model agreement, not
   true coaching ability.

2. **Small n.** 50 examples gives ~7pp confidence intervals. Differences of < 5pp
   should be treated as noise, not signal.

3. **Only 1 example** in the incorrect/omission stratum (dataset constraint).

4. **Weak vision model.** Qwen3-VL-8B is unlikely to reliably extract fine-grained
   visual details (axis scales, table cell values, legend colors) from dense scientific
   figures. A 72B+ model would be needed to draw strong conclusions about visual utility.

---

## Suggested Next Steps

### Tier 1 — Highest expected impact, low cost

**1. Run with a stronger vision model**
Replace `Qwen3-VL-8B-Instruct` with `Qwen/Qwen2.5-VL-72B-Instruct` or GPT-4o (via
Together.ai or OpenAI). At 72B+ scale, the image should start contributing measurably
and the vision-only / multimodal gap over text-only should emerge.

**2. Filter to Factual errors only**
Factual errors require reading specific values from the figure (axis numbers, table
cells, legend labels). Subsetting to these 16 examples would show whether vision helps
specifically where it should — on visually grounded mistakes. Expect a larger vision
advantage in this slice.

**3. Remove the reference answer as a primary condition**
The "without answer" setup is arguably the more realistic coaching scenario (a coach
sees the figure and assesses the student without a pre-written rubric). Run a dedicated
study with this as the primary condition, using a stronger model. This is where the 2×2
modality grid becomes most informative.

### Tier 2 — Methodological improvements

**4. Chain-of-thought visual grounding**
Add a reasoning step that forces the model to describe what it sees in the figure before
assessing the student. Prompt addition:
```
Step 1: Describe the key information visible in the figure relevant to the question.
Step 2: Compare the student's answer to what you described and the reference answer.
Step 3: Output your verdict.
```
This prevents the model from ignoring the image and doing pure text comparison.

**5. Image-grounded few-shot exemplars**
The current ICL exemplars in the prompt are text-only. For vision scenarios, pass the
exemplar images as actual image blocks in multi-turn conversation messages (before the
final user turn). This gives the model concrete examples of visual reasoning, not just
text reasoning.

**6. Human annotation of a subset**
Annotate 50 examples with human verdicts to establish a ceiling and measure inter-rater
agreement. This would validate whether the synthetic GPT-4.1 labels are reliable ground
truth, and give a clearer picture of how far the models are from human-level performance.

### Tier 3 — Scope expansion

**7. Larger sample size**
Run on all 174 examples (or the full SPIQA test set across all parts) for statistical
significance. With 50 examples, differences of < 10pp are within noise.

**8. Per-class accuracy breakdown**
Report separate accuracy for Correct / Partially Correct / Incorrect. The aggregate
number obscures that the model likely gets Correct and Incorrect right most of the time,
while nearly always failing on Partially Correct. A confusion matrix would make this
visible.

**9. Question-type stratification**
Split results by figure type (plot vs table vs diagram) and by error type. Visual
advantage is expected to be much larger for plots (requires reading trends and values)
than for diagrams (where the caption often fully describes the structure).

**10. Fine-tuning**
Fine-tune a small vision-language model (e.g., Qwen3-VL-7B) on a training split of the
augmented SPIQA data. This would test whether the task performance ceiling is a
prompting issue or a fundamental model capacity issue.
