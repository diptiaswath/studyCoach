\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{booktabs}

\iclrfinalcopy

\title{Study Coach: R1. Dataset Proposal and Analysis}

\author{
  Dipti Aswath\thanks{\hspace{4pt}Everyone Contributed Equally} \hspace{2em} Buddhika Kahawitage$^*$ \hspace{2em} Jiashan Cui$^*$ \hspace{2em} Khubi Srinivasan$^*$ \\
  \texttt{\{daswath, bmakehelwala, jiashanc, ksubrama\}@andrew.cmu.edu}
  }

\date{}

\begin{document}
\maketitle

\section{\points{4} Problem Definition and Dataset Choice}

\textbf{Paper:} Pramanick et al., ``SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers'' (NeurIPS 2024 Datasets and Benchmarks Track)

\textbf{Link:} \url{https://huggingface.co/datasets/google/spiqa}

\textbf{Problem Motivation:} Senior ML interviews increasingly expect candidates to demonstrate paper literacy: reading a paper, understanding experimental results, and explaining key figures. Users struggle with interpreting architecture diagrams, training curves, ablation tables, and result visualizations. Existing tools currently support text-based interview prep but lack the ability to help users engage with the visual components of research.

\textbf{Dataset:} SPIQA (Scientific Paper Image Question Answering) is a large-scale QA dataset designed to interpret complex figures and tables within scientific research papers across computer science domains. The full dataset contains 25,859 papers, 152,487 figures, 117,707 tables, and 270,194 question-answer pairs from 19 CS conferences (2018-2023).

\textbf{Our Focus:} We use the Test-A split (666 QA pairs across 118 papers), which was manually filtered and curated to ensure high quality. We augment this into \textbf{SPIQA+} by adding synthetic user answers and structured model evaluations, transforming the task from answer generation to answer evaluation.

\textbf{Prior Work Gap:} The SPIQA paper established that paper context improves figure QA performance (Gemini 1.5 Pro achieves 8+ point L3Score improvements with full context). However, SPIQA only addresses: \textit{Can models generate correct answers?} It does NOT address: \textit{Can models evaluate whether a human's explanation is correct?} This answer evaluation capability is essential for educational applications like interview coaching.

\subsection{\points{0.5} What phenomena or task does this dataset help address?}

This dataset helps address \textbf{incongruence detection} between user (textual) explanations and visual content in scientific figures. Specifically, given a figure and a user's explanation of what it shows, can a multimodal model:
\begin{enumerate}
    \item Detect when the explanation doesn't match the figure content (verdict: correct/partially correct/incorrect)
    \item Categorize the type of error (factual error vs. wrong conclusion)
    \item Provide useful coaching feedback (free-text) that localizes the mismatch
\end{enumerate}

This task directly supports the ``Study Coach'' application where users practice explaining research figures and receive AI-powered feedback on their interpretations.

\subsection{\points{0.5} What about this task is fundamentally multimodal?}

The task requires \textbf{cross-modal alignment and verification} that cannot be accomplished with either modality alone:

\begin{enumerate}
    \item \textbf{Visual grounding is essential:} To evaluate ``BERT achieves 89\% accuracy'' as incorrect, the model must read the actual bar height (84.6\%) from the figure - this information exists only in the visual modality.
    
    \item \textbf{Language understanding is essential:} The user's explanation is textual, and the model must parse claims like ``outperforms'' or ``plateaus'' to know what visual evidence to seek.
    
    \item \textbf{Cross-modal reasoning required:} Wrong conclusion errors (e.g., ``loss decreases throughout training'' when it plateaus) require the model to map verbal interpretations onto visual patterns and assess whether the interpretation is justified - neither extracting numbers nor parsing text alone suffices.
\end{enumerate}

This is fundamentally different from VQA (generate text from image) or image captioning---we're evaluating whether \textit{someone else's} text-image alignment is correct.

\subsection{Hypotheses}

\paragraph{\points{1} Hypothesis 1 (Detection)} 
Multimodal models can classify user explanations as correct/partially correct/incorrect above random baseline ($\geq$50\% on three-class), but will fall short of human performance.

\paragraph{\points{1} Hypothesis 2 (Context)} 
Relevant paper context will improve detection accuracy for reasoning-dependent errors (wrong conclusions, omissions) but not for visually-obvious errors (wrong numbers).

\paragraph{\points{1} Hypothesis 3 (Localization)} 
Models will detect incongruence at higher rates than they can accurately localize and explain it.

\paragraph{Hypothesis 4 (Error Type)} 
Factual errors will be detected more reliably than interpretation errors (wrong conclusions, omissions).

\paragraph{Hypothesis 5 (Figure Type)} 
Detection accuracy will vary significantly across figure types, with tables being easiest (explicit, localized information) and architecture diagrams hardest (distributed spatial relationships).

\clearpage
\section{\points{6} Dataset Analysis}

\subsection{\points{1} Dataset Properties}

We use the Scientific Paper Image Question Answering (SPIQA) dataset, publicly available through Hugging Face (\url{https://huggingface.co/datasets/google/spiqa}).

\textbf{Full Dataset Statistics:}
\begin{table}[htbp]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Metric} & \textbf{Count} \\
\midrule
Papers & 25,859 (from 19 CS conferences, 2018-2023) \\
Figures & 152,487 \\
Tables & 117,707 \\
QA pairs & 270,194 total \\
Avg question length & 12.98 words \\
Avg answer length & 14.56 words \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Dataset Splits:}
\begin{table}[htbp]
\centering
\small
\begin{tabular}{llll}
\toprule
\textbf{Split} & \textbf{Papers} & \textbf{QA Source} & \textbf{Notes} \\
\midrule
Train & $\sim$25,459 & LLM-generated & For fine-tuning (stretch goal) \\
Validation & 200 & LLM-generated & Dev set \\
Test-A & 118 & LLM + manual filter & 666 QAs -- our primary dataset \\
Test-B & varies & Human-written (QASA) & Optional validation \\
Test-C & varies & Human-written (QASPER) & Optional validation \\
\bottomrule
\end{tabular}
\end{table}

We focus on Test-A because: (1) it is the highest-quality evaluation split with manual filtering, (2) our core study uses prompting-based evaluation (no fine-tuning needed), and (3) 666 QA pairs is sufficient for our augmented SPIQA+ dataset of $\sim$200-400 evaluation examples.

\subsection{\points{0.5} Compute Requirements}

\textbf{Token Requirement for Dataset Generation}

We augment SPIQA data set with wrong student answers along with corresponding model explanations as to why a given student answer is incorrect. We plan to use in-context learning on the test-A dataset with a seed of wrong answer examples which have been manually curated. This synthetically generated dataset will contain about 400 new examples. We will be using GPT-4o or a similar closed LLM API for this data-set generation. We make the following assumptions for the tokens required for this task.

\begin{itemize}
    \item Each image cost a fixed amount of tokens (used 85 as per OpenAI documentation\footnote{\url{https://platform.openai.com/docs/guides/images-vision}})
    \item Synthetically generated model explanation as to why the answer is wrong is more verbose as it needs to refer to both the context and the given answer. So we estimate it to be twice the size of ground truth answer explanation.
    \item Three exemplars used in each prompt.
\end{itemize}

Under these assumptions, we will consume the following number of tokens for generating $\sim$400 synthetic wrong answer, explanation examples.

\begin{table}[htbp]
\centering
\begin{tabular}{lr}
\toprule
\textbf{Component} & \textbf{Tokens} \\
\midrule
Input Text & 1,888,192 \\
Input Image & 1,317,330 \\
Output Text & 85,830 \\
\bottomrule
\end{tabular}
\end{table}

With typically cited GPT-4o token pricing\footnote{\url{https://platform.openai.com/docs/pricing}}, cost estimate will be as follows:

{\small\texttt{Cost = 1888192 * 0.0000025 + 1317330 * 0.000008 + 85830 * 0.0000025 = \$15.47}}

\textbf{Token Requirement for Model Testing}

For testing the SPIQA+ dataset we plan to use GPT-4o and Gemini 1.5 Pro. Assuming that we run through all 666 examples of test-A, the following table gives the estimate for the required number of tokens (question posed along with image and captions).

\begin{table}[htbp]
\centering
\begin{tabular}{lr}
\toprule
\textbf{Component} & \textbf{Tokens} \\
\midrule
Input Text & 669,399 \\
Input Image & 712,300 \\
Output Text & 114,629 \\
\bottomrule
\end{tabular}
\end{table}

Cost estimate for GPT-4o will be as follows:

{\small\texttt{Cost = 669399 * 0.0000025 + 712300 * 0.000008 + 114629 * 0.0000025 = \$7.66}}

We expect a cost figure in this range for Gemini 1.5 Pro as well.

\textbf{Hardware Requirements}

We also plan to run inference using LLaVA 1.5 7B open source model. A 7B parameter model typically requires an EC2 instance with at least 16GB--24GB of VRAM for efficient inference, making a single \texttt{g5.xlarge} (NVIDIA A10G) or \texttt{g5.2xlarge} instance the most cost-effective choice for this project.

\subsection{\points{2} Modality Analysis}

Using a small sample of the data (e.g. validation splits), we generate statistics and plots for relevant properties of the data.

\textbf{Property 1: Question Length Distribution}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.6\textwidth]{image2.png}
\caption{Distribution of question lengths in the validation split.}
\end{figure}

\begin{itemize}
    \item Mean: 12.7 words
    \item Median: 12.0 words
    \item Range: 5--68 words
\end{itemize}

Questions are fairly concise, with most clustering around 10--15 words. The tight distribution suggests consistent question formatting.

\textbf{Property 2: Answer Length Distribution}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.6\textwidth]{image5.png}
\caption{Distribution of answer lengths in the validation split.}
\end{figure}

\begin{itemize}
    \item Mean: 14.5 words
    \item Median: 11.0 words
    \item Range: 1--220 words
\end{itemize}

Answer lengths are more variable (std: 16.1). Most answers are short (under 20 words), but there's a long tail of detailed explanations extending to 220 words. The median being lower than the mean confirms this right-skewed distribution.

\textbf{Property 3: Figure Type Distribution}

\begin{figure}[htbp]
\centering
\begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{image1.png}
    \caption{Content type distribution (high-level).}
\end{minipage}
\hfill
\begin{minipage}{0.52\textwidth}
    \centering
    \includegraphics[width=\textwidth]{image4.png}
    \caption{Detailed figure type distribution.}
\end{minipage}
\end{figure}

Content Type (high-level):
\begin{itemize}
    \item Figures: 56.1\% (1,170 questions)
    \item Tables: 43.9\% (915 questions)
\end{itemize}

Detailed Figure Types:
\begin{itemize}
    \item Tables: 44.3\% (923)
    \item Plots: 28.1\% (585)
    \item Schematics: 17.4\% (363)
    \item Photographs: 8.3\% (173)
    \item Other: 1.9\% (39 + 1 code snippet)
\end{itemize}

The dataset has good diversity across visual modalities, with tables and plots dominating but meaningful representation of diagrams and photographs.

\textbf{Property 4: Number of Figures/Tables Referenced per Answer}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{image3.png}
\caption{Distribution of figures per paper (left) and QA pairs per paper (right).}
\end{figure}

Every QA pair references exactly 1 figure/table---there's zero variation in the validation split. This is intentional in SPIQA's design: each question is paired with a single visual element.

The interesting part though is that even though each question only references 1 figure, each paper contains multiple figures (mean: 10.4, range: 1--32). This creates two evaluation scenarios:

\textbf{``Direct QA'' mode (minimal context):}
\begin{itemize}
    \item Input: question + the 1 referenced figure
    \item This is what the reference field points to
    \item Context varies much less here since it is only 1 image per question
\end{itemize}

\textbf{``Full paper'' mode (rich context):}
\begin{itemize}
    \item Input: question + all $\sim$10 figures from the paper
    \item This tests whether the model can find the right figure and ignore distractors
    \item Context varies a lot here depending on the paper
\end{itemize}

The number of figures per paper exactly equals the number of QA pairs per paper (correlation = 1.000). This means:
\begin{itemize}
    \item Each figure gets exactly 1 question
    \item Papers with 20 figures have 20 questions
    \item Papers with 5 figures have 5 questions
\end{itemize}

\textbf{Key Insights for Inference Planning}

\begin{enumerate}
    \item \textbf{Context window requirements:} Most questions + answers fit comfortably in $\sim$30 words, so token usage per inference will be dominated by image encoding, not text.
    \item \textbf{Visual complexity:} Nearly half the questions reference tables (which often have dense text), while the other half reference figures (plots, schematics, photos). This means the chosen models need strong OCR capabilities for tables and spatial reasoning for diagrams.
    \item \textbf{Answer generation:} Median answer is 11 words, but we will need to handle occasional 100+ word explanations. Budget $\sim$50--100 output tokens per inference to be safe.
    \item \textbf{Context variance:} Context does not vary much between questions when only passing the reference image, but when passing the whole paper's context, it can vary significantly as there is substantial variance in the number of figures for each paper.
\end{enumerate}


\subsection{\points{0.5} Metrics Used}

For our answer evaluation task, we use:

\begin{enumerate}
    \item \textbf{Verdict Accuracy:} Three-class classification accuracy (correct/partially correct/incorrect). Also per-class precision, recall, F1.
    \item \textbf{Error Category Accuracy:} For incorrectly-verdicted samples, does the model identify the right error type? Exact-match on category field.
    \item \textbf{Figure-Type Breakdown:} Verdict and error category accuracy by figure type (tables, plots, schematics, etc.).
    \item \textbf{Context Ablation:} Compare accuracy across C1 (figure only), C2 (figure + paragraphs), C3 (full paper).
    \item \textbf{Explanation Quality:} Manual assessment on $\sim$50 samples for localization accuracy and coaching usefulness.
\end{enumerate}

\subsection{\points{2} Baselines}

We select four baselines representing different architectural approaches and capability tiers, reframing their relevance from SPIQA's original answer generation task to our answer evaluation task.

\textbf{1. LLaVA 1.5} \cite{liu2023llava15}\\
Paper: Liu et al., ``Improved Baselines with Visual Instruction Tuning''\\
Link: \url{https://arxiv.org/abs/2310.03744}

LLaVA 1.5 is an open-source multimodal model that we use as a baseline to test whether lightweight vision-language models can detect incongruence between user explanations and figure content. Its substantial fine-tuning gains on SPIQA (+31.59 L3Score) suggest it may learn to distinguish correct from incorrect figure interpretations with domain-specific training.

\textbf{2. GPT-4o} \cite{openai2024gpt4o}\\
Paper: OpenAI, ``GPT-4o Release''\\
Link: \url{https://openai.com/index/hello-gpt-4o/}

GPT-4o achieves the highest performance on SPIQA answer generation (64.00 L3Score on test-A), making it our ceiling reference for answer evaluation. If a model can generate correct answers, it should theoretically also recognize when a user's answer is incorrect---our task tests whether this generation capability transfers to evaluation capability.

\textbf{3. Gemini 1.5 Pro} \cite{reid2024gemini15}\\
Paper: Reid et al., ``Gemini 1.5: Unlocking Multimodal Understanding Across Millions of Tokens of Context''\\
Link: \url{https://arxiv.org/abs/2403.05530}

Gemini 1.5 Pro shows strong context-dependent gains on SPIQA (+8.31 L3Score with full paper context versus figures alone), making it ideal for testing our H2 (context) hypothesis: that paper context helps detect reasoning-dependent errors more than visually-obvious factual errors. Its long-context capability enables our C3 condition without truncation.

\textbf{4. TabRAG} \cite{si2025tabrag}\\
Paper: Si et al., ``TabRAG: Tabular Document Retrieval via Structured Language Representations''\\
Link: \url{https://arxiv.org/html/2511.06582v1}

TabRAG converts table images into structured language representations (JSON/markdown) for improved retrieval and question answering. This directly informs our table-to-structured-text ablation study: if converting tables to structured text improves answer generation, we hypothesize it will also improve error detection---helping Study Coach decide whether to OCR tables before running the coaching agent.


\clearpage

\section{Team}
\subsection{Expertise}
We have the following expertise in the underlying modalities required by this task:
\begin{enumerate}
    \item \textbf{Dipti Aswath:} Experience with production agentic systems using LangGraph agents (ML-Amp) and curriculum development for AI courses. Completed prior 2 courses in the Generative AI certificate program.
    
    \item \textbf{Buddhika Kahawitage:} Prior experience with small language model fine tuning. Familiar with cost estimation and performance analysis. Completed 11967-A Large Language Models: Methods and Applications.
    
    \item \textbf{Jiashan Cui:} Completed coursework in Artificial Intelligence and used OpenAI API in class projects to process and analyze data with large language models. Professional experience using Azure OpenAI to support pipelines that transcribe and process audio and video data.
    
    \item \textbf{Khubi Srinivasan:} Experience with data analysis and visualization. Conducted the modality analysis for this report including statistical characterization of question/answer distributions and figure type breakdowns. Completed prior 2 courses in the Generative AI certificate program.
\end{enumerate}


\clearpage


\bibliography{references}
\bibliographystyle{iclr2024_conference}

\appendix
\section{Appendix}

\subsection{Scope Summary}

\begin{table}[htbp]
\centering
\footnotesize
\begin{tabular}{lll}
\toprule
\textbf{Dimension} & \textbf{Full Design} & \textbf{Course Project Scope} \\
\midrule
Dataset & Full SPIQA (270K QAs) & Test-A only (666 QAs) \\
Augmentation & All splits & Test-A $\rightarrow$ SPIQA+ ($\sim$200--400 4-tuples) \\
Error categories & 4 (factual, omission, wrong concl., no relevancy) & 2 core + 2 stretch \\
Figure types & All & All (natural distribution; analyze by type) \\
Verdicts & 3 (correct / partial / incorrect) & 3 (same) \\
Models & 12 models (as in SPIQA) & 2--3 models \\
Conditions & 4 conditions + table ablation & 3 core (C0, C1, C2) + C3 optional \\
Human annotation & Full manual curation & $\sim$80--130 examples (seed + spot-check) \\
\bottomrule
\end{tabular}
\caption{Scope summary comparing full design versus course project constraints.}
\end{table}

\subsection{Dataset Construction Pipeline}

We augment SPIQA Test-A into SPIQA+ using In-Context Learning (ICL) for synthetic data generation, with a two-stage validation pipeline and a quality gate.

\textbf{Phase 1: Prepare}
\begin{enumerate}
    \item \textbf{Download Test-A:} Fetch SPIQA\_testA.json, test-A images, and extracted paragraphs from Hugging Face. Yields 118 papers, 666 curated QA pairs.
    \item \textbf{Filter for Figure-Grounded QAs:} Select QA pairs where the answer depends on reading the figure. Target $\sim$200--300 figure-grounded QAs.
    \item \textbf{Annotate Figure Types:} Classify each figure as table, plot/chart, schematic, or visualization using an LLM classifier.
\end{enumerate}

\textbf{Phase 2: Generate}
\begin{enumerate}
    \setcounter{enumi}{3}
    \item \textbf{Hand-Curate ICL Seed Set:} Manually create 20--30 high-quality 4-tuple examples (5--8 per error category across figure types).
    \item \textbf{Generate Congruent Samples:} Use SPIQA ground truth answers as ``correct'' user answers.
    \item \textbf{ICL-Based Synthetic Generation:} Prompt an LLM with 2--3 seed examples to generate 1--2 incongruent user answers + structured model responses per QA.
\end{enumerate}

\textbf{Phase 3: Validate}
\begin{enumerate}
    \setcounter{enumi}{6}
    \item \textbf{LLM Validator:} Send each synthetic example to a different model to verify error presence and category.
    \item \textbf{Human Spot-Check:} $\sim$50--100 samples, stratified across error categories and figure types.
    \item \textbf{Quality Gate:} LLM validator must agree with human judgment $\geq$90\% on spot-check subset.
    \item \textbf{Final SPIQA+ Dataset:} $\sim$200--400 validated 4-tuples with figure-type annotations.
\end{enumerate}

\textbf{Total human annotation effort:} $\sim$80--130 examples (20--30 seed set + 50--100 spot-check).

\subsection{SPIQA+ 4-Tuple Structure}

Each example in our augmented SPIQA+ dataset is a 4-tuple:
\begin{table}[htbp]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Field} & \textbf{Source} & \textbf{Description} \\
\midrule
Context & SPIQA & Visual and textual context from the paper \\
Question & SPIQA & The figure-grounded question \\
User Answer & Generated (synthetic) & A student's explanation (correct/partial/incorrect) \\
Model Response & Generated (synthetic GT) & Verdict + error category + explanation \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Structured Model Response Format}

\begin{table}[htbp]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{Values} & \textbf{Purpose} \\
\midrule
Verdict & Correct / Partially Correct / Incorrect & Tests H1 (detection) \\
Error Category & Factual / Wrong Conclusion / None (core) & Tests H4 (error type) \\
Explanation & Free-text coaching feedback & Qualitative evaluation \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Experimental Conditions}

These conditions follow the spirit of SPIQA's three tasks but are adapted for our answer evaluation task. The key question is: what information helps the model evaluate a user's explanation? These conditions systematically ablate input modalities to answer this.

\begin{table}[htbp]
\centering
\footnotesize
\begin{tabular}{llll}
\toprule
\textbf{Condition} & \textbf{Input to Model} & \textbf{What It Tests} \\
\midrule
C0: Text Only & Question + user answer + caption (no figure) & Is the task fundamentally multimodal? \\
C1: Figure Only & Figure image + question + user answer & Can vision alone detect errors? \\
C2: Figure + Paragraphs & C1 + caption + relevant paragraphs & Does textual context help? \\
C3: Full Paper (stretch) & C2 + full paper text & Does broader context help further? \\
\bottomrule
\end{tabular}
\caption{Experimental conditions for modality and context ablation.}
\end{table}

Comparing C0 vs.\ C1 tests whether the task is fundamentally multimodal (Section 1.2), as C0 helps test if language (text) alone can detect errors. 
Following Dodge et al.\ \cite{dodge2012detecting}, who found humans achieve 91\% accuracy on visual text detection without seeing images, we expect C0 to perform above chance but below C1 - validating that cross-modal verification is required.


\subsection{Table-to-Structured-Text Ablation}

The SPIQA authors noted that tables are hard for current MLLMs because models struggle with spatial cell relationships in images. We propose a focused ablation on the table subset: convert table images to structured text (JSON or markdown) and re-run conditions C1 and C2.

\textbf{Research Questions:}
\begin{itemize}
    \item Does structured text improve error detection accuracy on tables?
    \item Does structured text reduce the value of paper context?
\end{itemize}

\textbf{Practical Implication:} If structured text dramatically improves evaluation accuracy on tables, it tells Study Coach to OCR/parse tables before running the coaching agent rather than relying on vision alone.
\end{document}
