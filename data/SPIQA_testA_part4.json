{
  "1606.07384v2": {
    "paper_id": "1606.07384v2",
    "all_figures": {
      "1606.07384v2-Figure1-1.png": {
        "caption": "Experiments with synthetic data: error is reported against the size of the conditional probability table (lower is better). The error is the estimated total variation distance to the ground truth Bayes net. We use the error of MLE without noise as our benchmark. We plot the performance of our algorithm (Filtering), empirical mean with noise (MLE), and RANSAC. We report two settings: the underlying structure of the Bayes net is a random tree (left) or a random graph (right).",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1606.07384v2-Figure2-1.png": {
        "caption": "Experiments with semi-synthetic data: error is reported against the fraction of corrupted samples (lower is better). The error is the estimated total variation distance to the ALARM network. We use the sampling error without noise as a benchmark, and compare the performance of our algorithm (Filtering), empirical mean with noise (MLE), and RANSAC.",
        "content_type": "figure",
        "figure_type": "plot"
      }
    },
    "qa": [
      {
        "question": "How does the performance of the Filtering algorithm compare to the performance of MLE with noise?",
        "answer": "The Filtering algorithm performs better than MLE with noise in both the random tree and random graph settings.",
        "explanation": "The figure shows that the error of the Filtering algorithm is lower than the error of MLE with noise for all values of the number of parameters. This is true for both the random tree and random graph settings.",
        "reference": "1606.07384v2-Figure1-1.png"
      },
      {
        "question": "Which method performs the best when there is a high fraction of corrupted samples?",
        "answer": "RANSAC",
        "explanation": "The plot shows that RANSAC has the lowest error when the fraction of corrupted samples is high.",
        "reference": "1606.07384v2-Figure2-1.png"
      }
    ]
  },
  "1611.04363v2": {
    "paper_id": "1611.04363v2",
    "all_figures": {
      "1611.04363v2-Table1-1.png": {
        "caption": "Table 1: Performance comparison of different methods.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1611.04363v2-Figure2-1.png": {
        "caption": "Graphical representation of the WeakFG model. Variable yi indicates whether expert ei declines the invitation; vi indicates the embedding for expert ei; f(q, ei, vi, yi) and g(ei, ej , yi, yj) represent the local factor function defined for expert ei on query q, and the correlation factor function defined between experts ei and ej .",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1611.04363v2-Figure1-1.png": {
        "caption": "Figure 1: Decline probability of an expert conditioned on whether or not the expert has a correlated “friend” who has already declined on two datasets: QA-Expert and Paper-Reviewer, and our online evaluation of journal reviewer recommendation.",
        "content_type": "figure",
        "figure_type": "plot"
      }
    },
    "qa": [
      {
        "question": "Which method performed best according to the P@1 metric for the QA-Expert task, and how much better did it perform compared to the average P@1 score of the D2V method? ",
        "answer": "The WeakFG method achieved the highest P@1 score for the QA-Expert task with a score of 52.8. This is 23.2% higher than the average P@1 score of the D2V method, which was 29.6. ",
        "explanation": "The table provides the P@1 scores for all methods under the \"QA-Expert\" column. By comparing these values, we can identify WeakFG as the best performing method for this metric. We can then calculate the difference between WeakFG's score and D2V's average score to determine the performance gap.",
        "reference": "1611.04363v2-Table1-1.png"
      },
      {
        "question": "What is the relationship between the decline probability of an expert and whether or not they have a \"friend\" who has already declined?",
        "answer": "The decline probability of an expert is higher if they have a \"friend\" who has already declined.",
        "explanation": "The figure shows that the decline probability for experts with a \"friend\" who has declined is consistently higher than the decline probability for experts without such a friend, across all three datasets. This suggests that there is a correlation between the decline decisions of experts and their friends.",
        "reference": "1611.04363v2-Figure1-1.png"
      }
    ]
  },
  "1612.02803v5": {
    "paper_id": "1612.02803v5",
    "all_figures": {
      "1612.02803v5-Table1-1.png": {
        "caption": "Our contribution compared with Su et al. (2014); Wibisono et al. (2016).",
        "content_type": "table",
        "figure_type": "table."
      },
      "1612.02803v5-Figure1-1.png": {
        "caption": "An illustration of the harmonic oscillators: A massive particle connects to a massless spring. (Top) Undamped harmonic oscillator; (Bottom) Damped harmonic oscillator.",
        "content_type": "figure",
        "figure_type": "other"
      },
      "1612.02803v5-Figure2-1.png": {
        "caption": "The algorithmic iterates and trajectories of a simple quadratic program.",
        "content_type": "figure",
        "figure_type": "other"
      }
    },
    "qa": [
      {
        "question": "What is the equation that describes the motion of a mass attached to a spring?",
        "answer": "The equation that describes the motion of a mass attached to a spring is:\n```\nm d^2 X / dt^2 + kX = 0\n```\nwhere:\n* m is the mass of the object\n* X is the displacement of the object from its equilibrium position\n* k is the spring constant\n* t is time",
        "explanation": "This equation is derived from Newton's second law, which states that the force on an object is equal to its mass times its acceleration. In this case, the force on the object is the force exerted by the spring, which is proportional to the displacement of the object from its equilibrium position.",
        "reference": "1612.02803v5-Figure1-1.png"
      }
    ]
  },
  "1702.08694v3": {
    "paper_id": "1702.08694v3",
    "all_figures": {
      "1702.08694v3-Figure1-1.png": {
        "caption": "Figure 1: Although there is a clear correlation between Features 1 and 2 in the left panel and none in the right panel, after median-based binarization of Feature 1 and Feature 2, the estimated probability of the occurrence of the feature combination (the number of points for which Feature 1 = Feature 2 = 1, or the number of points in the red box) will be exactly the same in both examples. Hence if the left case is a significant interaction, the right uncorrelated case also becomes a significant interaction in binarization-based methods.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1702.08694v3-Figure2-1.png": {
        "caption": "Figure 2: Results on synthetic data with the minor class ratio r1 = 0.5. Regarding the scale of precision and F-measure, see comment at the last paragraph just before Section 3. The number of features is d = 20 in the left column and the sample size is N = 1,000 in the right column. Both xand y-axes are in logarithmic scale. C-Tarone is shown in red circles, the binarization approach in blue triangles.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1702.08694v3-Table1-1.png": {
        "caption": "Contingency tables.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1702.08694v3-Figure5-1.png": {
        "caption": "Figure 5: Results on synthetic data with the minor class ratio r1 = 0.2. The number of features is d = 20 in the left column and the sample size is N = 3,000 in the right column. Both x- and y-axes are in logarithmic scale. C-Tarone is shown in red circles, the binarization approach in blue triangles. Missing points in (b) mean that no significant combination is detected.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1702.08694v3-Figure3-1.png": {
        "caption": "Figure 3: Results on real data. Regarding the scale of precision and F-measure, see the comment at the last paragraph just before Section 3. The y-axis is in logarithmic scale. C-Tarone is shown in red and the binarization approach is shown in blue. Higher (taller) is better in precision, recall, and F-measure, while lower is better in running time.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1702.08694v3-Figure4-1.png": {
        "caption": "Figure 4: The upper bound B(a, b) of the KL divergence (left) and the corresponding p-value (right) with N = 100 with respect to changes in a when b = 0.3.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1702.08694v3-Table2-1.png": {
        "caption": "Table 2: Statistics of real data.",
        "content_type": "table",
        "figure_type": "N/A"
      }
    },
    "qa": [
      {
        "question": "Which method, C-Tarone or Binarization, achieves higher precision when the number of features is small and the number of data points is large?",
        "answer": "C-Tarone.",
        "explanation": "The top left panel of Figure 2 shows that when the number of features is small (around 1e+03) and the number of data points is large (around 1e+05), the red circles (C-Tarone) are higher than the blue triangles (Binarization).",
        "reference": "1702.08694v3-Figure5-1.png"
      },
      {
        "question": "How does the C-Tarone method compare to the binarization method in terms of precision, recall, F-measure, and running time?",
        "answer": "The C-Tarone method has higher precision and F-measure than the binarization method in all datasets. The C-Tarone method has better or competitive recall than the binarization method. The running time of the C-Tarone method is competitive with the binarization method.",
        "explanation": "The figure shows the precision, recall, F-measure, and running time of the C-Tarone method and the binarization method for different datasets. The bars for the C-Tarone method are higher than the bars for the binarization method for precision and F-measure, which means that the C-Tarone method performs better than the binarization method in terms of precision and F-measure. The bars for the C-Tarone method are similar to the bars for the binarization method for recall, which means that the C-Tarone method performs similarly to the binarization method in terms of recall. The bars for the C-Tarone method are similar to the bars for the binarization method for running time, which means that the C-Tarone method performs similarly to the binarization method in terms of running time.",
        "reference": "1702.08694v3-Figure3-1.png"
      },
      {
        "question": "For a fixed value of $b$, how does the maximum achievable KL divergence and the corresponding minimum achievable p-value change with increasing values of $a$?",
        "answer": "The maximum achievable KL divergence initially increases with increasing values of $a$ until it reaches a peak. Then, it decreases with increasing values of $a$. The minimum achievable p-value initially decreases with increasing values of $a$ until it reaches a minimum. Then, it increases with increasing values of $a$.",
        "explanation": "The left panel of the figure shows the maximum achievable KL divergence as a function of $a$ for a fixed value of $b$. The right panel shows the corresponding minimum achievable p-value. The KL divergence initially increases and then decreases with increasing values of $a$, while the p-value initially decreases and then increases. This behavior is consistent with the analysis in the passage, which shows that the KL divergence is monotonically increasing for $a < b$ and monotonically decreasing for $b < a < 1/2$.",
        "reference": "1702.08694v3-Figure4-1.png"
      },
      {
        "question": "Which dataset would likely require the most computational resources for C-Tarone to analyze?",
        "answer": "The \"wdbc\" dataset would likely require the most computational resources for C-Tarone to analyze.",
        "explanation": "The table shows that the \"wdbc\" dataset has the largest number of candidate combinations (search space) at 1,073,741,824. This means that C-Tarone needs to explore over a billion possible combinations of features during its analysis, which is significantly higher than any other dataset listed. The passage also mentions that the search space grows exponentially with the number of features (d). Since \"wdbc\" has a relatively high number of features (30), this contributes to its massive search space and the need for greater computational resources.",
        "reference": "1702.08694v3-Table2-1.png"
      }
    ]
  },
  "1703.04887v4": {
    "paper_id": "1703.04887v4",
    "all_figures": {
      "1703.04887v4-Table3-1.png": {
        "caption": "Table 3: The translation performance of the BRCSGAN with different N for Monte Carlo search. ”-” means that the proposed model shows no improvement than the pre-trained generator or it can not be trained stably. With N set as 0, it is referred to as the pretrained generator. Similarly, we only report results on the RNNSearch and λ is set as 0.7.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1703.04887v4-Figure2-1.png": {
        "caption": "BLEU score on the development set for the BR-CSGAN where the discriminators have different initial accuracy. ”0.6-acc” means the initial accuracy is 0.6. We report the results on the Chinese-English translation tasks. RNNSearch is taken as the generator.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1703.04887v4-Table2-1.png": {
        "caption": "Table 2: BLEU score on Chinese-English and EnglishGerman translation tasks for MRT and BR-CSGAN.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1703.04887v4-Table1-1.png": {
        "caption": "Table 1: BLEU score on Chinese-English and English-German translation tasks. The hyper-parameter λ is selected according to the development set. For the Transformer, following (Vaswani et al., 2017), we report the result of a single model obtained by averaging the 5 checkpoints around the best model selected on the development set.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1703.04887v4-Figure1-1.png": {
        "caption": "The Illustration of the proposed BR-CSGAN. Left: D is trained over the real sentence pairs translated by the human and the generated sentence pairs by G. Note that D is a conditional discriminator. Right: G is trained by police gradient where the final reward is provided by D and Q.",
        "content_type": "figure",
        "figure_type": "** Schematic"
      }
    },
    "qa": [
      {
        "question": "What is the relationship between the number of Monte Carlo samples (N) and the translation performance of the BR-CSGAN model? Why is there a trade-off when choosing the value of N?",
        "answer": "The table and passage show that the translation performance of the BR-CSGAN model generally improves as the number of Monte Carlo samples (N) increases. However, this improvement plateaus after N reaches a certain point (around 20 in this case).\n\nThere is a trade-off when choosing the value of N because increasing N also increases the computational complexity and training time. While a higher N leads to more accurate reward estimations and better performance, it also requires more computational resources and longer training times. Therefore, choosing the optimal N involves balancing the desired performance with the available computational resources and time constraints.",
        "explanation": "Table 2 presents the BLEU scores of the BR-CSGAN model with different N values. As N increases from 15 to 20, the BLEU scores consistently improve across all datasets. However, further increasing N to 25 and 30 shows minimal improvement, indicating a plateau in performance. The passage explains that this is because a higher N leads to more accurate reward estimations, guiding the model towards better performance. However, it also clarifies that higher N values require more sampling, significantly increasing computation time. Therefore, the choice of N involves a trade-off between accuracy and computational efficiency.",
        "reference": "1703.04887v4-Table3-1.png"
      },
      {
        "question": "How does the initial accuracy of the discriminator affect the BLEU score?",
        "answer": "The BLEU score decreases as the initial accuracy of the discriminator increases.",
        "explanation": "The plot shows that the BLEU score decreases as the initial accuracy of the discriminator increases. This is likely because a more accurate discriminator is able to better distinguish between real and generated data, which makes it more difficult for the generator to produce realistic data.",
        "reference": "1703.04887v4-Figure2-1.png"
      },
      {
        "question": "How does BR-CSGAN compare to MRT in terms of translation performance and what is the likely reason for this difference?",
        "answer": "BR-CSGAN consistently outperforms MRT on both Chinese-English and English-German translation tasks, achieving higher BLEU scores.\n\nWhile both methods optimize similar objectives, BR-CSGAN uses a reinforcement learning procedure with a dynamic discriminator to maximize rewards for the generator. This dynamic feedback seems to be more effective than the static objective and random sampling approach used by MRT, leading to better translation performance.",
        "explanation": "The table directly provides the BLEU scores for both methods, allowing for a comparison of their translation performance. The passage then sheds light on the possible reason for this difference by describing the key differences in their training procedures. Specifically, the use of reinforcement learning and a dynamic discriminator in BR-CSGAN is highlighted as a potential advantage over MRT's static objective and random sampling approach.",
        "reference": "1703.04887v4-Table2-1.png"
      },
      {
        "question": "Which model and configuration achieves the best performance on the Chinese-English translation task, and how much improvement does it offer compared to the baseline RNNSearch model?",
        "answer": "The Transformer+BR-CSGAN model with λ=0.8 achieves the best performance on the Chinese-English translation task with an average BLEU score of 42.61. This represents an improvement of 0.81 BLEU points compared to the baseline RNNSearch model.",
        "explanation": "Table 1 shows the BLEU scores for different models and configurations on the Chinese-English translation task. We can compare the average BLEU scores across different models to identify the best performing one. The table also allows us to compare the performance of the BR-CSGAN augmented models with their respective baseline models (RNNSearch and Transformer) by looking at the difference in BLEU scores.",
        "reference": "1703.04887v4-Table1-1.png"
      },
      {
        "question": "What is the role of the discriminator (D) in the proposed BR-CSGAN model?",
        "answer": " The discriminator (D) is responsible for distinguishing between real sentence pairs translated by humans and generated sentence pairs produced by the generator (G). It provides feedback to G in the form of rewards, helping G improve its ability to generate realistic sentence pairs.",
        "explanation": " The left side of the figure shows that D receives both real sentence pairs (x, y_d) and generated sentence pairs (x, y_g). By comparing these pairs, D learns to differentiate between human-translated and machine-generated sentences. This information is then used to reward G for generating more realistic sentence pairs.",
        "reference": "1703.04887v4-Figure1-1.png"
      }
    ]
  },
  "1704.04539v2": {
    "paper_id": "1704.04539v2",
    "all_figures": {
      "1704.04539v2-Figure3-1.png": {
        "caption": "Parsed AMR graph and alignments (dashed lines) for an Italian sentence, a Spanish sentence, a German sentences and a Chinese sentence.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1704.04539v2-Figure4-1.png": {
        "caption": "Parsing examples in several languages involving common translational divergence phenomena: (a) contains a categorical divergence, (b) and (e) conflational divergences, (c) a structural divergence, (d) an head swapping and (f) a thematic divergence.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1704.04539v2-Figure5-1.png": {
        "caption": "Linear regression lines for silver and fullcycle.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1704.04539v2-Table1-1.png": {
        "caption": "Table 1: Silver, gold and full-cycle Smatch scores for projection-based and MT-based systems.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1704.04539v2-Table2-1.png": {
        "caption": "Table 2: BLEU scores for Moses, Nematus and Google Translate (GT) on the (out-of-domain) LDC2015E86 test set",
        "content_type": "table",
        "figure_type": "N/A"
      }
    },
    "qa": [
      {
        "question": "What is the relationship between the Silver Snatch and the Gold Snatch?",
        "answer": "The Silver Snatch and the Gold Snatch are positively correlated. As the Gold Snatch increases, the Silver Snatch also increases.",
        "explanation": "The figure shows two scatter plots, one for the Silver Snatch vs. the Gold Snatch and one for the Full-cycle Snatch vs. the Gold Snatch. The linear regression lines for both plots have a positive slope, indicating a positive correlation between the variables.",
        "reference": "1704.04539v2-Figure5-1.png"
      },
      {
        "question": "Which system performs best on the IT domain in terms of full-cycle Smatch score, and how does its performance compare to the projection-based system in the same domain?",
        "answer": "The GT system achieves the highest full-cycle Smatch score in the IT domain with a score of 59. This is 14 points higher than the projection-based system in the same domain, which scored 45.",
        "explanation": "The table presents the Smatch scores for different systems across various domains, including IT. The \"Cycle\" column specifies the type of evaluation, with \"full-cycle\" referring to the complete parsing process. By looking at the row corresponding to the IT domain and the \"GT\" system under the \"full-cycle\" column, we can identify its score of 59. Comparing this to the score of 45 for the \"Projection\" system in the same column reveals the performance difference.",
        "reference": "1704.04539v2-Table1-1.png"
      },
      {
        "question": "Which translation system performs the best and how does its performance compare to Google Translate (GT)? Is the comparison with GT completely fair? Explain your answer.",
        "answer": "According to Table 2, Moses achieves the highest BLEU scores among the listed translation systems (Moses, Nematus) across all language pairs. However, its performance still falls behind Google Translate (GT) in every case.\n\nThe comparison with GT might not be entirely fair because, as GT has the advantage of being trained on a significantly larger dataset. This suggests that GT's performance advantage might be partially due to its training data rather than solely its inherent capabilities.",
        "explanation": " Question: \n\nBased on Table 2 and the passage, which translation system performs the best and how does its performance compare to Google Translate (GT)? Is the comparison with GT completely fair? Explain your answer. \n\n## Answer: \n\nAccording to Table 2, Moses achieves the highest BLEU scores among the listed translation systems (Moses, Nematus) across all language pairs. However, its performance still falls behind Google Translate (GT) in every case. \n\nThe comparison with GT might not be entirely fair because, as mentioned in the passage, GT has the advantage of being trained on a significantly larger dataset. This suggests that GT's performance advantage might be partially due to its training data rather than solely its inherent capabilities.",
        "reference": "1704.04539v2-Table2-1.png"
      },
      {
        "question": "What is the difference between the parsing trees for \"I like eating\" and \"I like grapes\"?",
        "answer": "The parsing tree for \"I like eating\" has only one argument, while the parsing tree for \"I like grapes\" has two arguments.",
        "explanation": "The parsing tree for \"I like eating\" shows that the verb \"like\" has only one argument, which is the pronoun \"I\". The parsing tree for \"I like grapes\" shows that the verb \"like\" has two arguments: the pronoun \"I\" and the noun \"grapes\". This difference in the number of arguments is an example of a thematic divergence.",
        "reference": "1704.04539v2-Figure4-1.png"
      }
    ]
  },
  "1704.07854v4": {
    "paper_id": "1704.07854v4",
    "all_figures": {
      "1704.07854v4-Figure4-1.png": {
        "caption": "Ablation study for our method. We evaluated the average loss for a test data set of the different data sets discussed in the text. Left: numeric values, again as a graph (center), and a graph of the loss values normalized w.r.t. initial surface loss on the right. Our method achieves very significant and consistent reductions across the very different data sets.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1704.07854v4-Figure5-1.png": {
        "caption": "An example of our deformation learning approach. F. l. t. r.: the result after applying weighted deformations, and with an additional deformation from a trained deformation network. Both show the reference surface in light brown in the background, which is shown again for comparison on the right. The inferred deformation manages to reconstruct large parts of the two central arms which can not be recovered by any weighting of the pre-computed deformations (left).",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1704.07854v4-Table2-1.png": {
        "caption": "Overview of our 2D and 4D simulation and machine learning setups. Timings were measured on a Xeon E5-1630 with 3.7GHz. Res, SDF and Defo denote resolutions for simulation, training, and the NN deformation, respectively; Sim and Train denote simulation and training runtimes. sp, sd, γ1, γ2 denote training steps for parameters, training steps for deformation, and regularization parameters, respectively.",
        "content_type": "table",
        "figure_type": "** Table"
      },
      "1704.07854v4-Figure6-1.png": {
        "caption": "Eight examples of the learned deformations for a flat initial surface. For each pair the reference surfaces are depicted in yellow and the deformed results in blue. The trained model learns to recover a significant portion of the large-scale surface motion over the whole parameters space.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1704.07854v4-Figure7-1.png": {
        "caption": "Each pair shows the reference surface in transparent brown, and in purple on the left the deformed surface after applying the precomputed deformations. These surfaces often significantly deviate from the brown target, i.e. the visible purple regions indicates misalignments. In cyan on the right, our final surfaces based on the inferred deformation field. These deformed surface match the target surface closely, and even recover thin features such as the central peak in (c).",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1704.07854v4-Figure11-1.png": {
        "caption": "This figure illustrates the forward advection process: Both deformation vsum and the correction vinv are initially located at x′ in (a). vinv is applied to yield the correct deformation at location x, as shown in (b).",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1704.07854v4-Figure1-1.png": {
        "caption": "Three liquid surfaces after 60 time steps differing only by ±ε in initial conditions. Even this initially very small difference can lead to large differences in surface position, e.g., the sheet in b) strongly curving downward.",
        "content_type": "figure",
        "figure_type": "Schematic"
      },
      "1704.07854v4-Figure15-1.png": {
        "caption": "Training with different gradient approximations: validation loss with a simplified advection (red), and the correct gradient from forward advection (green). The simplified version does not converge.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1704.07854v4-Figure14-1.png": {
        "caption": "Loss during training both for parameter learning and deformation learning. In yellow we show the loss for the current sample, while the dark line displays the loss evaluated on the validation set.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1704.07854v4-Figure8-1.png": {
        "caption": "a) Liquid drop data set example: several 3D surfaces of a single simulation data point in φα. b) An example splash generated by our method, visualized interactively.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1704.07854v4-Figure2-1.png": {
        "caption": "This illustration gives an overview of our algorithm. It works in two stages, a weighting and refinement stage, each of which employs a neural network to infer a weighting function and a dense deformation field, respectively.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1704.07854v4-Figure16-1.png": {
        "caption": "Different example surfaces from the 2D parameter space of Fig. 13. From left to right: surfaces reconstructed with PCA (purple), weighted deformations using a trained parameter network (pink), the reference surfaces (brown), and on the far right the output of our full method with a deformation network (teal). Note that none of the other methods is able to reconstruct both arms of liquid in the first row, as well as the left sheet in the bottom row. The reference surfaces are shown in light brown in the background for each version.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1704.07854v4-Figure13-1.png": {
        "caption": "The left image illustrates the initial conditions of our two dimensional parameter space setup. It consists of a set of two-dimensional liquid simulations, which vary the position of the liquid drop along x as α1, and its size as α2. The right half shows the data used for training at t = 30. Note the significant amount of variance in positions of small scale features such as the thin sheets. Both images show only a subset of the whole data.",
        "content_type": "figure",
        "figure_type": "Schematic"
      },
      "1704.07854v4-Figure10-1.png": {
        "caption": "Illustration of our deformation alignment procedure.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1704.07854v4-Figure9-1.png": {
        "caption": "a) Three example configurations from our stairs data set. b) The interactive version of the stair setup shown in the demo app. Notice how the flow around the central wall obstacle changes. As the wall is shifted right, the flow increases corresonpondingly.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1704.07854v4-Table1-1.png": {
        "caption": "Performance and setup details of our 4D data sets in the Android app measured on a Samsung S8 device. The ”defo. align” step contains alignment and rescaling of the deformations.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1704.07854v4-Figure18-1.png": {
        "caption": "Two frames generated with our approach (left) and with a direct SDF interpolation using a similar amount of overall memory (right). The latter looses the inital drop shape (a), and removes all splash detail (b). In addition, the direct SDF interpolation leads to strong ghosting artifacts with four repeated patterns.",
        "content_type": "figure",
        "figure_type": "other"
      },
      "1704.07854v4-Figure17-1.png": {
        "caption": "Additional examples of the influence of the deformation network for three different time steps (t = 1, 4, 8 from top to bottom). Each pair shows the reference surface in transparent brown, and in purple on the left the deformed surface after applying the precomputed deformations. These surfaces often significantly deviate from the brown target, i.e. the visible purple regions indicates misalignments. In cyan on the right, our final surfaces based on the inferred deformation field. These deformed surface often match the target surface much more closely.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1704.07854v4-Figure3-1.png": {
        "caption": "An example of our parameter learning approach. F.l.t.r.: the initial undeformed surface, the surface deformed by the weighting from the trained parameter network, and the reference surface only. The reference surface is shown again in the middle in light brown for comparison. The weighted deformations especially match the left liquid arm well, while there are not enough degrees of freedom in the pre-computed deformations to independently raise the surface on the right side.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1704.07854v4-Figure12-1.png": {
        "caption": "Figure 12: Overview of our two neural networks. While the parameter network (left) is simple, consisting of two fully connected layers, its cost functions allows it to learn how to apply multiple long-range, non-linear deformation fields. The deformation network (right), which makes use of several de-convolutional layers, instead learns to generate dense deformation fields to refine the final surface.",
        "content_type": "figure",
        "figure_type": "schematic"
      }
    },
    "qa": [
      {
        "question": "What does the parameter network do to the initial surface?",
        "answer": "The parameter network weights the initial surface, causing it to deform.",
        "explanation": "The figure shows three different surfaces: the initial surface, the surface deformed by the parameter network, and the reference surface. The deformed surface is closer in shape to the reference surface than the initial surface, which suggests that the parameter network is able to learn the desired shape of the surface.",
        "reference": "1704.07854v4-Figure3-1.png"
      },
      {
        "question": "How do the parameter network and the deformation network differ in terms of complexity and function?",
        "answer": "The parameter network is a simple structure with two fully connected layers, while the deformation network is more complex and contains two fully connected layers followed by two or more four-dimensional de-convolution layers. The parameter network learns how to apply multiple long-range, non-linear deformation fields, while the deformation network learns to generate dense deformation fields to refine the final surface.",
        "explanation": "The figure shows the two networks side-by-side. The parameter network is on the left and is shown as a simple structure with two layers. The deformation network is on the right and is shown as a more complex structure with several layers. The figure also includes annotations that describe the function of each network.",
        "reference": "1704.07854v4-Figure12-1.png"
      },
      {
        "question": "Which gradient approximation method leads to a more stable and lower loss value during training?",
        "answer": "The corrected gradient method leads to a more stable and lower loss value during training.",
        "explanation": "The figure shows the validation loss for two different gradient approximation methods: naive gradient and corrected gradient. The corrected gradient method results in a much lower and more stable loss value than the naive gradient method.",
        "reference": "1704.07854v4-Figure15-1.png"
      },
      {
        "question": "How does the flow of water change as the central wall obstacle is shifted to the right?",
        "answer": "The flow of water increases as the central wall obstacle is shifted to the right.",
        "explanation": "The figure shows that the flow of water is more constricted when the central wall obstacle is in the center of the stairs. As the wall is shifted to the right, the flow of water has more space to move through, which results in an increase in flow.",
        "reference": "1704.07854v4-Figure9-1.png"
      },
      {
        "question": "Which of the two scenes, Drop or Staris, requires more computation time for rendering?",
        "answer": "Staris",
        "explanation": "The table shows that the rendering time for Staris is 35ms, while the rendering time for Drop is 21ms.",
        "reference": "1704.07854v4-Table1-1.png"
      },
      {
        "question": "What is the relationship between the resolution of the simulation and the training time?",
        "answer": " The higher the resolution of the simulation, the longer the training time. ",
        "explanation": " The table in the figure shows that the training time increases as the resolution of the simulation increases. For example, the 2D setup with a resolution of 100^2 has a training time of 186 seconds, while the 4D setup with a resolution of 110^3 * 110 has a training time of 186 minutes. ",
        "reference": "1704.07854v4-Table2-1.png"
      },
      {
        "question": "What is the role of the parameter network in the weighting and refinement stage?",
        "answer": "The parameter network is used to infer a weighting function.",
        "explanation": "The parameter network takes as input the chosen point and the initial surface, and outputs a weighting function. This weighting function is then used to weight the pre-computed deformations, which are then applied to the initial surface. The result is a deformed surface that is then fed into the deformation network.",
        "reference": "1704.07854v4-Figure2-1.png"
      },
      {
        "question": "Which of the methods is able to reconstruct the shape of the liquid properly?",
        "answer": "Only the full method with a deformation network is able to produce a perfect reconstruction.",
        "explanation": "The figure shows that the PCA reconstruction (purple) and the weighted deformations using a trained parameter network (pink) are not able to reconstruct both arms of liquid in the first row. The reference surfaces (brown) are shown in the background for each version, and it is clear that only the full method with a deformation network is able to match the reference surface.",
        "reference": "1704.07854v4-Figure16-1.png"
      },
      {
        "question": "How do the initial conditions of the simulations vary?",
        "answer": "The initial conditions of the simulations vary in two dimensions: the position of the liquid drop along the x-axis (α1) and the size of the drop (α2).",
        "explanation": "The left image shows a grid of simulations with different initial conditions. The x-axis represents the position of the drop, and the y-axis represents the size of the drop.",
        "reference": "1704.07854v4-Figure13-1.png"
      }
    ]
  },
  "1705.07164v8": {
    "paper_id": "1705.07164v8",
    "all_figures": {
      "1705.07164v8-Figure1-1.png": {
        "caption": "Training curves by running all approaches. Generator and discriminator losses are plotted in orange and blue lines. mnist fashion-mnist",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1705.07164v8-Figure3-1.png": {
        "caption": "Training curves by running all approaches. Generator and discriminator losses are plotted in orange and blue lines. cifar10 imagenet",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1705.07164v8-Table2-1.png": {
        "caption": "Table 2: Inception scores (IS) obtained by running RWGAN, WGAN and WGAN(g). For cifar10, “begin\" and “end\" refer to IS averaged over first 5 and last 10 epochs. For imagenet, “begin\" and “end\" refer to IS averaged over first 3 and last 5 epochs.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1705.07164v8-Table1-1.png": {
        "caption": "Examples of the function φ and the resulting Bregman cost functions. Note that A 0 is positive semidefinite.",
        "content_type": "table",
        "figure_type": "table"
      }
    },
    "qa": [
      {
        "question": "What can you infer from the training curves for the ACGAN?",
        "answer": "The training curves for the ACGAN show that the generator and discriminator losses both decrease over time. This indicates that the ACGAN is able to learn to generate realistic images.",
        "explanation": "The figure shows the training curves for the ACGAN. The orange line represents the generator loss, and the blue line represents the discriminator loss. The fact that both lines decrease over time indicates that the ACGAN is learning to generate realistic images.",
        "reference": "1705.07164v8-Figure1-1.png"
      },
      {
        "question": "Which model performs better based on the training curves?",
        "answer": "It is difficult to say definitively which model performs better based on the training curves alone. However, it appears that the WGAN(g) model may be performing better than the other models, as its generator and discriminator losses are both lower than the other models.",
        "explanation": "The training curves show the generator and discriminator losses for each model over time. The lower the loss, the better the model is performing.",
        "reference": "1705.07164v8-Figure3-1.png"
      },
      {
        "question": "Which method achieved the highest Inception Score (IS) at the end of training for both CIFAR10 and ImageNet datasets? Did this method also achieve the highest initial IS score?",
        "answer": "For CIFAR10, WGAN achieved the highest IS at the end of training (2.42). However, RWGAN had a higher initial IS score (1.86) compared to WGAN's 1.63. \n\nFor ImageNet, WGAN again achieved the highest final IS (2.80), while RWGAN had a slightly higher initial IS (2.04 vs. 2.00). ",
        "explanation": "The table presents the IS scores for different methods at both the beginning and end of the training process. By comparing the values in the \"end\" columns for both datasets, we can identify which method ultimately achieved the highest score. Additionally, comparing the values in the \"begin\" columns reveals which method started with the highest score.",
        "reference": "1705.07164v8-Table2-1.png"
      },
      {
        "question": "What is the difference between the Euclidean and Mahalanobis Bregman cost functions?",
        "answer": "The Euclidean Bregman cost function is simply the squared difference between two points, while the Mahalanobis Bregman cost function takes into account the covariance of the data.",
        "explanation": "The table shows that the Euclidean Bregman cost function is given by ||x - y||^2, while the Mahalanobis Bregman cost function is given by (x - y)^T A (x - y), where A is a positive semi-definite matrix that represents the covariance of the data. This means that the Mahalanobis Bregman cost function gives more weight to differences in directions where the data is more spread out.",
        "reference": "1705.07164v8-Table1-1.png"
      }
    ]
  },
  "1705.09882v2": {
    "paper_id": "1705.09882v2",
    "all_figures": {
      "1705.09882v2-Figure3-1.png": {
        "caption": "Fig. 3. Our model architecture consists of a frame-level feature embedding fCNN , which provides input to both a recurrent layer fLSTM and the Reinforced Temporal Attention (RTA) unit fw (highlighted in red). The classifier is attached to the hidden state ht and its video prediction is the weighted sum of single-frame predictions, where the weights wt for each frame t are predicted by the RTA unit.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1705.09882v2-Figure5-1.png": {
        "caption": "Comparison of our RGB-to-Depth transfer with Yosinski et al. [90] in terms of top-1 accuracy on DPI-T. In this ablation study the x axis represents the number of layers whose weights are frozen (left) or fine-tuned (right) starting from the bottom.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1705.09882v2-Figure1-1.png": {
        "caption": "Filter responses from “conv1” (upper right), “conv2” (bottom left) and “conv3” (bottom right) layers for a given frame from the TUM GAID data using (a) a framework for person re-identification from RGB [82] and (b) the feature embedding fCNN of our framework, which is drawn in Fig. 3 and exclusively utilizes depth data.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1705.09882v2-Figure7-1.png": {
        "caption": "Cumulative Matching Curves (CMC) on TUM-GAID for the scenario that the individuals wear clothes which are not provided during training.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1705.09882v2-Table2-1.png": {
        "caption": "Top-1 re-identification accuracy (top-1, %) and normalized Area Under the Curve (nAUC, %) on TUM-GAID in newclothes scenario with single-shot (ss) and multi-shot (ms) evaluation",
        "content_type": "table",
        "figure_type": "table"
      },
      "1705.09882v2-Figure6-1.png": {
        "caption": "Example sequence with the predicted Bernoulli parameter printed.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1705.09882v2-Table1-1.png": {
        "caption": "Table 1. Single-shot and multi-shot person re-identification performance on the test set of DPI-T, BIWI and IIT PAVIS. Dashes indicate that no published result is available",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1705.09882v2-Figure4-1.png": {
        "caption": "Fig. 4. Our split-rate RGB-to-Depth transfer compared with Yosinski et al. [90]. At the top, the two models are trained from scratch with RGB and Depth data. Next we show the “R3D” instances (i.e. the bottom 3 layers’ weights from RGB remain frozen or slowly changing) for both methods, following the notation of [90]. The color of each layer refers to the initialization and the number below is the relative learning rate (the best performing one in bold). The key differences are summarized in the text.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1705.09882v2-Figure2-1.png": {
        "caption": "The cropped color image (left), the grayscale depth representation Dg p (center) and the result after background subtraction (right) using the body index information Bp from skeleton tracking.",
        "content_type": "figure",
        "figure_type": "** photograph(s)"
      }
    },
    "qa": [
      {
        "question": "Which method achieves the highest Top-1 Accuracy for multi-shot person re-identification on the BIWI dataset, and how does it compare to the best single-shot method on the same dataset?",
        "answer": "The proposed method with RTA attention achieves the highest Top-1 Accuracy for multi-shot person re-identification on the BIWI dataset with a score of 50.0%. This is significantly higher than the best single-shot method on the same dataset, which is our method (CNN) with a score of 25.4%.",
        "explanation": "Table 1 shows the Top-1 Accuracy for different person re-identification methods on various datasets, categorized by single-shot and multi-shot approaches. By looking at the BIWI column and comparing the highest values in both the single-shot and multi-shot sections, we can determine which method performs best in each category and compare their performance.",
        "reference": "1705.09882v2-Table1-1.png"
      },
      {
        "question": "How does the proposed split-rate RGB-to-Depth transfer scheme differ from the R3D [90] method of Yosinski et al.?",
        "answer": "The proposed split-rate RGB-to-Depth transfer scheme differs from the R3D [90] method in two ways. First, the proposed method uses a different learning rate for the bottom three layers of the network. Second, the proposed method uses a different initialization for the weights of the bottom three layers of the network.",
        "explanation": "The figure shows the two methods side-by-side. The R3D [90] method uses the same learning rate for all layers of the network, while the proposed method uses a lower learning rate for the bottom three layers. Additionally, the R3D [90] method initializes the weights of the bottom three layers with random values, while the proposed method initializes the weights of the bottom three layers with the weights from a pre-trained RGB network.",
        "reference": "1705.09882v2-Figure4-1.png"
      },
      {
        "question": "What is the difference between the grayscale depth representation and the result after background subtraction?",
        "answer": " The grayscale depth representation shows the depth of each pixel in the image, with darker pixels representing closer objects and lighter pixels representing further objects. The result after background subtraction shows only the foreground object, with the background removed.",
        "explanation": " The figure shows the original image, the grayscale depth representation, and the result after background subtraction. The grayscale depth representation shows that the person in the foreground is closer to the camera than the person in the background. The result after background subtraction shows that the background has been removed, leaving only the person in the foreground.",
        "reference": "1705.09882v2-Figure2-1.png"
      },
      {
        "question": "Which modality achieved the highest top-1 accuracy in the multi-shot evaluation on TUM-GAID?",
        "answer": "Body Depth & Head RGB (ms: LSTM & RTA)",
        "explanation": "The table shows the top-1 accuracy for each modality in both single-shot and multi-shot evaluations. The highest top-1 accuracy in the multi-shot evaluation is 88.1%, which is achieved by the Body Depth & Head RGB (ms: LSTM & RTA) modality.",
        "reference": "1705.09882v2-Table2-1.png"
      },
      {
        "question": "What is the relationship between the Bernoulli parameter and the image?",
        "answer": "The Bernoulli parameter is a measure of the probability of a pixel being foreground or background. The higher the Bernoulli parameter, the more likely the pixel is to be foreground. This is reflected in the images, where the pixels with higher Bernoulli parameters are more likely to be part of the person's silhouette.",
        "explanation": "The Bernoulli parameter is a key component of the GrabCut algorithm, which is used to segment images into foreground and background regions. The algorithm works by iteratively assigning pixels to either the foreground or background, based on their similarity to neighboring pixels and the Bernoulli parameter.",
        "reference": "1705.09882v2-Figure6-1.png"
      },
      {
        "question": "Which part of the model is responsible for deciding which frames are most important for the re-identification task?",
        "answer": "The Reinforced Temporal Attention (RTA) unit.",
        "explanation": "The RTA unit is highlighted in red in the figure, and the caption states that it \"predicts the weights $w_t$ for each frame $t$\". These weights are used to calculate a weighted sum of single-frame predictions, which is the final output of the model. This means that the RTA unit is responsible for deciding which frames are most important for the re-identification task.",
        "reference": "1705.09882v2-Figure3-1.png"
      },
      {
        "question": "How does the performance of our RGB-to-Depth transfer compare to Yosinski et al. [90] in terms of top-1 accuracy on DPI-T when all layers are fine-tuned?",
        "answer": "The proposed RGB-to-Depth transfer performs slightly better than Yosinski et al. [90] in terms of top-1 accuracy on DPI-T when all layers are fine-tuned.",
        "explanation": "The right panel of the figure shows the top-1 re-identification accuracy for different methods when different numbers of layers are fine-tuned. When all layers are fine-tuned (x=7), our RGB-to-Depth transfer achieves a top-1 accuracy of about 75%, while Yosinski et al. [90] achieves a top-1 accuracy of about 73%.",
        "reference": "1705.09882v2-Figure5-1.png"
      },
      {
        "question": "What is the difference between the filter responses from the “conv1”, “conv2” and “conv3” layers for a given frame from the TUM GAID data using a framework for person re-identification from RGB and the feature embedding fCNN of a framework that utilizes depth data?",
        "answer": "The filter responses from the “conv1”, “conv2” and “conv3” layers for a given frame from the TUM GAID data using a framework for person re-identification from RGB are more detailed and contain more information than the filter responses from the fCNN of a framework that utilizes depth data. This is because RGB images contain more information than depth images.",
        "explanation": "The figure shows the filter responses from the “conv1”, “conv2” and “conv3” layers for a given frame from the TUM GAID data using a framework for person re-identification from RGB and the feature embedding fCNN of a framework that utilizes depth data. The filter responses from the RGB framework are more detailed and contain more information than the filter responses from the depth framework.",
        "reference": "1705.09882v2-Figure1-1.png"
      }
    ]
  },
  "1706.00827v2": {
    "paper_id": "1706.00827v2",
    "all_figures": {
      "1706.00827v2-Figure1-1.png": {
        "caption": "Multi-class multi-instance fitting examples. Results on simultaneous plane and cylinder (top left), line and circle fitting (top right), motion (bottom left) and plane segmentation (bottom right).",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1706.00827v2-Table3-1.png": {
        "caption": "Misclassification errors (%, average and median) for two-view plane segmentation on all the 19 pairs from AdelaideRMF test pairs using fixed parameters.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1706.00827v2-Table1-1.png": {
        "caption": "Table 1: The number of false positive (FP) and false negative (FN) instances for simultaneous line and circle fitting.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1706.00827v2-Table2-1.png": {
        "caption": "Misclassification error (%) for the two-view plane segmentation on AdelaideRMF test pairs: (1) johnsonna, (2) johnsonnb, (3) ladysymon, (4) neem, (5) oldclassicswing, (6) sene.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1706.00827v2-Table4-1.png": {
        "caption": "Misclassification errors (%) for two-view motion segmentation on the AdelaideRMF dataset. All the methods were tuned separately for each video by the authors. Tested image pairs: (1) cubechips, (2) cubetoy, (3) breadcube, (4) gamebiscuit, (5) breadtoycar, (6) biscuitbookbox, (7) breadcubechips, (8) cubebreadtoychips.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1706.00827v2-Table5-1.png": {
        "caption": "Misclassification errors (%, average and median) for two-view motion segmentation on all the 21 pairs from the AdelaideRMF dataset using fixed parameters.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1706.00827v2-Table6-1.png": {
        "caption": "Table 6: Misclassification error (%) of simultaneous plane and cylinder fitting to LIDAR data. See Fig. 6 for examples.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1706.00827v2-Figure3-1.png": {
        "caption": "Comparison of PEARL and Multi-X. Three random lines sampled at 100 locations, plus 200 outliers. Parameters of both methods are: hmax = 3, and the outlier threshold is (a) 6 and (b) 3 pixels.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1706.00827v2-Figure2-1.png": {
        "caption": "(Left) Three lines each generating 100 points with zero-mean Gaussian noise added, plus 50 outliers. (Right) 1000 line instances generated from random point pairs, the ground truth instance parameters (red dots) and the modes (green) provided by Mean-Shift shown in the model parameter domain: α angle – vertical, offset – horizontal axis.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1706.00827v2-Figure4-1.png": {
        "caption": "AdelaideRMF (top) and Multi-H (bot.) examples. Colors indicate the planes Multi-X assigned points to.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1706.00827v2-Figure5-1.png": {
        "caption": "AdelaideRMF (top) and Hopkins (bot.) examples. Color indicates the motion Multi-X assigned a point to.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1706.00827v2-Table7-1.png": {
        "caption": "Misclassification errors (%, average and median) for multi-motion detection on 51 videos of Hopkins dataset: (1) Traffic2 – 2 motions, 31 videos, (2) Traffic3 – 3 motions, 7 videos, (3) Others2 – 2 motions, 11 videos, (4) Others3 – 3 motions, 2 videos, (5) All – 51 videos.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1706.00827v2-Table8-1.png": {
        "caption": "Table 8: Processing times (sec) of Multi-X (M) and TLinkage (T) for the problem of fitting (1) lines and circles, (2) homographies, (3) two-view motions, (4) video motions, and (5) planes and cylinders. The number of data points is shown in the first column.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1706.00827v2-Figure6-1.png": {
        "caption": "Results of simultaneous plane and cylinder fitting to LIDAR point cloud in two scenes. Segmented scenes visualized from different viewpoints. There is only one cylinder on the two scenes: the pole of the traffic sign on the top. Color indicates the instance Multi-X assigned a point to.",
        "content_type": "figure",
        "figure_type": "schematic"
      }
    },
    "qa": [
      {
        "question": "Which method achieved the most accurate results for simultaneous line and circle fitting?",
        "answer": "Multi-X achieved the most accurate results for simultaneous line and circle fitting.",
        "explanation": "Table 1 shows the number of false positive (FP) and false negative (FN) instances for each method. Ideally, we want both FP and FN to be as low as possible, indicating fewer falsely detected and missed features. As shown in the table, Multi-X is the only method that achieved **zero** false positives across all test cases (columns 1, 2, and 3). Additionally, it had the lowest number of false negatives in two out of three cases and tied for the lowest in the remaining case. This demonstrates that Multi-X consistently produced the most accurate results compared to the other methods.",
        "reference": "1706.00827v2-Table1-1.png"
      },
      {
        "question": "Which method has the lowest average misclassification error for the cubechips image pair?",
        "answer": "Multi-X",
        "explanation": "The table shows the average and minimum misclassification errors for each method on each image pair. For the cubechips image pair, Multi-X has the lowest average misclassification error of 3.45%.",
        "reference": "1706.00827v2-Table4-1.png"
      },
      {
        "question": "What is the difference between AdelaideRMF and Multi-H?",
        "answer": "AdelaideRMF tends to assign points to more planes than Multi-H.",
        "explanation": "In the top two images, which are examples of AdelaideRMF, the points are assigned to many different planes, as indicated by the different colors. In the bottom two images, which are examples of Multi-H, the points are assigned to fewer planes.",
        "reference": "1706.00827v2-Figure4-1.png"
      },
      {
        "question": "What is the relationship between the color of the points and the motion of the object?",
        "answer": "The color of the points indicates the motion that the Multi-X algorithm assigned to each point.",
        "explanation": "The caption states that the color of the points indicates the motion that Multi-X assigned to a point. For example, in the top left image, the blue points on the Rubik's Cube are likely assigned to a different motion than the green points on the poker chips.",
        "reference": "1706.00827v2-Figure5-1.png"
      },
      {
        "question": "Which of the five methods tested had the lowest average misclassification error?",
        "answer": "Multi-X",
        "explanation": "The table shows the average misclassification error for each method. Multi-X has the lowest average error of 2.97%.",
        "reference": "1706.00827v2-Table5-1.png"
      },
      {
        "question": "In which scenario did Multi-X perform worse than another method in terms of misclassification error for simultaneous plane and cylinder fitting?",
        "answer": "Multi-X performed worse than PEARL in test case (6), with a misclassification error of 21.72% compared to PEARL's 17.35%. ",
        "explanation": "Table 2 presents the misclassification error percentages for different methods, including Multi-X and PEARL, across seven test cases. By comparing the values in the \"Multi-X\" and \"PEARL\" columns, we can identify instances where Multi-X underperformed. In test case (6), Multi-X has a higher error rate than PEARL, indicating its relatively poorer performance in that specific scenario.",
        "reference": "1706.00827v2-Table6-1.png"
      },
      {
        "question": "How does the Mean-Shift algorithm perform in the presence of outliers?",
        "answer": "The Mean-Shift algorithm is robust to outliers.",
        "explanation": "The left panel of the figure shows three lines with outliers. The right panel shows the results of the Mean-Shift algorithm, which has correctly identified the three lines despite the presence of outliers. This is because the Mean-Shift algorithm is based on the density of points, and outliers do not significantly affect the density.",
        "reference": "1706.00827v2-Figure2-1.png"
      },
      {
        "question": "Which method has the lowest average misclassification error?",
        "answer": "Multi-X",
        "explanation": "The table shows the average and median misclassification errors for different methods. The lowest average misclassification error is for Multi-X, which has an average error of 9.72%.",
        "reference": "1706.00827v2-Table3-1.png"
      },
      {
        "question": "Which algorithm is generally faster for fitting planes and cylinders: Multi-X or T-Linkage?",
        "answer": "Multi-X is generally faster for fitting planes and cylinders compared to T-Linkage.",
        "explanation": "Looking at column (5) in Table 1, which shows the processing times for fitting planes and cylinders, we can see that the processing time for Multi-X is consistently lower than that of T-Linkage for all three data sizes (100, 500, and 1000). For example, with 500 data points, Multi-X takes 3.8 seconds while T-Linkage takes 15.9 seconds. This trend holds true for the other data sizes as well, indicating that Multi-X offers superior performance in terms of speed for this specific fitting task.",
        "reference": "1706.00827v2-Table8-1.png"
      }
    ]
  },
  "1706.08146v3": {
    "paper_id": "1706.08146v3",
    "all_figures": {
      "1706.08146v3-Figure5-1.png": {
        "caption": "Figure 5: NMF reconstruction error vs. projection matrix column sparsity.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1706.08146v3-Figure6-1.png": {
        "caption": "Figure 6: Accuracy of tensor decomposition on compressed EEG data. Left: Normalized reconstruction error; dashed line indicates baseline reconstruction error on original data. Right: Median Pearson correlations between recovered factors and factors computed from original data.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1706.08146v3-Figure1-1.png": {
        "caption": "Figure 1: Schematic illustration of compressed matrix factorization. (i) The matrix M̃ is a compressed version of the full data matrix M . (ii) We directly factorize M̃ to obtain matrices W̃ and H̃. (iii) Finally, we approximate the left factor of M via sparse recovery on each column of W̃ .",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1706.08146v3-Figure4-1.png": {
        "caption": "Figure 4: Visualization of a factor from the tensor decomposition of EEG data that correlates with the onset of seizures in a patient (red dotted lines). The factor recovered from a 5× compressed version of the tensor (bottom) retains the peaks that are indicative of seizures.",
        "content_type": "figure",
        "figure_type": "** plot"
      },
      "1706.08146v3-Table1-1.png": {
        "caption": "Table 1: Summary of DNA microarray gene expression datasets, along with runtime (seconds) for each stage of the NMF pipeline on compressed data. Factorize-Recover runs only r instances of sparse recovery, as opposed to the m instances used by the alternative, Recover-Factorize.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1706.08146v3-Figure2-1.png": {
        "caption": "Approximation errors Err(X,X∗) := ‖X −X∗‖F /‖X∗‖F for sparse PCA and NMF on synthetic data with varying column sparsity k of W and projection dimension d. The values of d correspond to 10×, 5×, and 2.5× compression respectively. Err(W̃ , PW ) measures the distance between factors in the compressed domain: low error here is necessary for accurate sparse recovery. Err(Ŵ ,W ) measures the error after sparse recovery: the recovered factors Ŵ typically incur only slightly higher error than the oracle lower bound (dotted lines) where PW is known exactly.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1706.08146v3-Figure3-1.png": {
        "caption": "Figure 3: Normalized reconstruction errors ‖Ŵ Ĥ −M‖F /‖M‖F for NMF on gene expression data with varying compression factors n/d. FR (blue, solid) is Factorize-Recover, RF (orange, dotted) is RecoverFactorize. The horizontal dashed line is the error when M is decomposed in the original space. Perhaps surprisingly, when n/d > 3, we observe a reduction in reconstruction error when compressed data is first factorized. See the text for further discussion.",
        "content_type": "figure",
        "figure_type": "plot"
      }
    },
    "qa": [
      {
        "question": "What is the relationship between projection sparsity and normalized reconstruction error?",
        "answer": "The normalized reconstruction error decreases as the projection sparsity increases, up to a certain point. After that, the error starts to increase again.",
        "explanation": "The figure shows a plot of the normalized reconstruction error versus the projection sparsity. The error decreases rapidly at first, then levels off and starts to increase again. This suggests that there is an optimal value of projection sparsity that minimizes the reconstruction error.",
        "reference": "1706.08146v3-Figure5-1.png"
      },
      {
        "question": "What is the relationship between compression factor and reconstruction error?",
        "answer": "The reconstruction error increases as the compression factor increases.",
        "explanation": "The left panel of Figure 1 shows that the normalized reconstruction error increases as the compression factor increases. This is because compressing the data reduces the amount of information that is available, which makes it more difficult to reconstruct the original data accurately.",
        "reference": "1706.08146v3-Figure6-1.png"
      },
      {
        "question": "What are the three steps involved in compressed matrix factorization?",
        "answer": "The three steps involved in compressed matrix factorization are: \n\n1. Compress the full data matrix M to obtain a compressed matrix M̃. \n2. Factorize M̃ to obtain matrices W̃ and H̃. \n3. Approximate the left factor of M via sparse recovery on each column of W̃.",
        "explanation": "The figure shows the three steps involved in compressed matrix factorization.",
        "reference": "1706.08146v3-Figure1-1.png"
      },
      {
        "question": "Which dataset would likely benefit the most from using the Fac.-Recover approach instead of Recover-Fac. in terms of computational efficiency?",
        "answer": "The Leukemia dataset would likely benefit the most from using the Fac.-Recover approach.",
        "explanation": "The passage states that the computation time for NMF is dominated by the cost of solving instances of the LP, which is directly related to the number of features in the dataset. Table 1 shows that the Leukemia dataset has by far the largest number of features (54,675) compared to the other two datasets. Since \\textsc{Fac.-Recover} only needs to run sparse recovery for a smaller number of features (r) compared to the full set of features (m) required by \\textsc{Recover-Fac.}, it would likely offer significant computational savings for the Leukemia dataset.",
        "reference": "1706.08146v3-Table1-1.png"
      },
      {
        "question": "What is the effect of increasing the projection dimension d on the approximation error for sparse PCA and NMF?",
        "answer": "Increasing the projection dimension d decreases the approximation error for both sparse PCA and NMF.",
        "explanation": "The figure shows that the approximation error decreases as the projection dimension d increases. This is because a higher projection dimension allows for a more accurate representation of the original data.",
        "reference": "1706.08146v3-Figure2-1.png"
      },
      {
        "question": "Which method achieves lower approximation error when the compression factor is greater than 3?",
        "answer": "Factorize-Recover",
        "explanation": "The figure shows that the blue line (Factorize-Recover) is below the orange line (Recover-Factorize) when the compression factor is greater than 3.",
        "reference": "1706.08146v3-Figure3-1.png"
      }
    ]
  },
  "1707.01922v5": {
    "paper_id": "1707.01922v5",
    "all_figures": {
      "1707.01922v5-Figure3-1.png": {
        "caption": "An overview of the ZDDA testing procedure. We use the SUN RGB-D [36] images for illustration. Different from the color coding in Fig. 2, the colors here are purely used to distinguish different CNNs/classifiers/predictions",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1707.01922v5-Table7-1.png": {
        "caption": "Table 7. Performance comparison with different numbers of classes in scene classification. The reported numbers are classification accuracy (%). The color of each cell reflects the performance ranking in each column, where darker color means better performance. PRGB-D represents the task-irrelevant RGB-D pairs",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1707.01922v5-Table6-1.png": {
        "caption": "Table 6. The performance comparison of the domain adaptation task MNIST→MNIST-M. The color of each cell reflects the performance ranking (darker is better). For ZDDA2, we report the best overall accuracy from Table 5. All the listed methods except ZDDA2 use the MNIST-M training data. Without the access to the MNIST-M training data, ZDDA2 can still achieve the accuracy comparable to those of the competing methods (even outperform most of them) in this task",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1707.01922v5-Figure1-1.png": {
        "caption": "Fig. 1. We propose zero-shot deep domain adaptation (ZDDA) for domain adaptation and sensor fusion. ZDDA learns from the task-irrelevant dual-domain pairs when the task-relevant target-domain training data is unavailable. In this example domain adaptation task (MNIST [27]→MNIST-M [13]), the task-irrelevant gray-RGB pairs are from the Fashion-MNIST [46] dataset and the Fashion-MNIST-M dataset (the colored version of the Fashion-MNIST [46] dataset with the details in Sec. 4.1)",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1707.01922v5-Table3-1.png": {
        "caption": "Table 3. The statistics of the datasets we use. For NIST, we use the “by class” dataset, remove the digits, and treat uppercase and lowercase letters as different classes. For EMNIST, we use the “EMNIST Letters” split which only contains the letters. We create the colored datasets from the original ones using Ganin’s method [13] (see Sec. 4.1 for details). We refer to each dataset by the corresponding dataset ID (e.g. DN and DN -M refer to the NIST and the NIST-M datasets, respectively)",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1707.01922v5-Table5-1.png": {
        "caption": "Table 5. The overall / average per class accuracy (%) of the domain adaptation tasks (gray scale images → RGB images) formed by the datasets in Table 3, where we introduce the dataset IDs and use them to refer to the datasets here. The middle four rows show the performance of ZDDA2. The color of each cell reflects the performance ranking in each column, where darker is better. The number in the parenthesis of the middle four rows is the semantic similarity between the T-R and T-I datasets measured by word2vec [29], where larger numbers represent higher semantic similarity. The T-R target-domain training data is only available for the row “target only”",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1707.01922v5-Figure4-1.png": {
        "caption": "Performance comparison between the two sensor fusion methods with black images as the noisy images. We compare the classification accuracy (%) of (a) naive fusion and (b) ZDDA3 under different noise levels in both RGB and depth testing data. (c) shows that ZDDA3 outperforms the naive fusion under most conditions",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1707.01922v5-Table1-1.png": {
        "caption": "Table 1. Problem setting comparison between ZDDA, unsupervised domain adaptation (UDA), multi-view learning (MVL), and domain generalization (DG)",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1707.01922v5-Table9-1.png": {
        "caption": "Table 9. Validation of ZDDA’s performance with different base network architectures in scene classification. The reported numbers are classification accuracy (%). The definition of PRGB-D and the representation of the cell color in each column are the same as those in Table 7",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1707.01922v5-Table4-1.png": {
        "caption": "Table 4. The base network architecture (BNA) we use in our experiments. For each BNA, We specify the layer separating the source/target CNN and the source classifier in Fig. 2. The layer name in the right column is based on the official Caffe [24] and SqueezeNet v1.1 [23] implementation of each BNA",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1707.01922v5-Table2-1.png": {
        "caption": "Table 2. Working condition comparison between ZDDA and other existing methods. Among all the listed methods, only ZDDA can work under all four conditions",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1707.01922v5-Table8-1.png": {
        "caption": "Table 8. Validation of ZDDA’s performance (in mean classification accuracy (%)) with different training/testing splits and choices of classes in scene classification. GN stands for GoogleNet [38]. The definition of PRGB-D and the representation of the cell color in each column are the same as those in Table 7",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1707.01922v5-Figure2-1.png": {
        "caption": "Fig. 2. An overview of the ZDDA training procedure. We use the images from the SUN RGB-D [36] dataset for illustration. ZDDA simulates the target-domain representation using the source-domain data, builds a joint network with the supervision from the source domain, and trains a sensor fusion network. In step 1, we choose to train s1 and fix t, but we can also train t and fix s1 to simulate the target-domain representation. In step 2, t can also be trainable instead of being fixed, but we choose to fix it to make the number of trainable parameters manageable. The details are explained in Sec. 3",
        "content_type": "figure",
        "figure_type": "** schematic"
      }
    },
    "qa": [
      {
        "question": "What is the difference between testing domain adaptation and testing sensor fusion?",
        "answer": "In testing domain adaptation, the source and target CNNs are trained on different domains, and the joint classifier is used to predict the class of the target data. In testing sensor fusion, the source and target CNNs are trained on the same domain, and the joint classifier is used to predict the class of the target data using both the source and target data.",
        "explanation": "Figure (a) shows the testing procedure for domain adaptation, while Figure (b) shows the testing procedure for sensor fusion.",
        "reference": "1707.01922v5-Figure3-1.png"
      },
      {
        "question": "Which method performs better in terms of classification accuracy?",
        "answer": "ZDDA3",
        "explanation": "Figure (c) shows the difference in classification accuracy between ZDDA3 and naive fusion. In most cases, the difference is positive, which means that ZDDA3 performs better than naive fusion.",
        "reference": "1707.01922v5-Figure4-1.png"
      },
      {
        "question": "What is the role of the task-irrelevant data in ZDDA?",
        "answer": "The task-irrelevant data is used to simulate the RGB representation using the gray scale image. This allows ZDDA to learn a joint network that can be used to classify digits in both the gray scale and RGB domains.",
        "explanation": "The figure shows that the task-irrelevant data consists of gray-scale and RGB image pairs. These pairs are used to train a network that can convert gray-scale images to RGB images. This network is then used to classify digits in the RGB domain, even though no RGB training data is available.",
        "reference": "1707.01922v5-Figure1-1.png"
      },
      {
        "question": "What is the key difference between ZDDA and UDA/MVL in terms of the available training data?",
        "answer": "The key difference lies in the availability of target-domain training data. While UDA and MVL methods require T-R training data from the target domain, ZDDA does not. ZDDA only requires T-R training data from a single source domain.",
        "explanation": "Table 1 clearly shows this distinction in the first row. Both UDA and MVL have a green \"Y\" under the column \"given T-R target-domain training data?\", indicating that they require such data. In contrast, ZDDA has a red \"N\" under the same column, signifying that it does not need target-domain training data. The passage further emphasizes this difference by stating that \"in ZDDA, T-R target-domain training data is unavailable and the only available T-R training data is in one source domain.\"",
        "reference": "1707.01922v5-Table1-1.png"
      },
      {
        "question": "If you are performing domain adaptation with ZDDA using AlexNet as the base network architecture and $D_F$ as the target domain, which layers of the network would be considered part of the source CNN and which would be part of the source classifier?",
        "answer": "In this scenario, the source CNN would consist of the AlexNet architecture up to and including the \"fc7\" layer. The remaining layers of AlexNet would then be used as the source classifier.",
        "explanation": "Table 1 provides information about the base network architectures (BNA) used in the experiments and specifies the layer that separates the source/target CNN from the source classifier. Looking at the row for AlexNet, we see that \"fc7\" is listed as the dividing layer. This means that for tasks using AlexNet, the source CNN comprises the network up to and including \"fc7\", while the remaining layers form the source classifier.",
        "reference": "1707.01922v5-Table4-1.png"
      },
      {
        "question": "Which of the following statements about the training procedure of ZDDA is true?\n\n(a) ZDDA simulates the target-domain representation using the source-domain data. (b) ZDDA builds a joint network with the supervision from the target domain. (c) ZDDA trains a sensor fusion network in step 1. (d) ZDDA trains a sensor fusion network in step 2.",
        "answer": "(a) ZDDA simulates the target-domain representation using the source-domain data.",
        "explanation": "The passage states that \"ZDDA simulates the target-domain representation using the source-domain data.\" This is shown in step 1 of the figure, where the target CNN is used to simulate the target-domain representation using the source-domain data.",
        "reference": "1707.01922v5-Figure2-1.png"
      }
    ]
  },
  "1708.01425v4": {
    "paper_id": "1708.01425v4",
    "all_figures": {
      "1708.01425v4-Figure2-1.png": {
        "caption": "Overview of the methodology of reconstructing implicit warrants for argument reasoning comprehension.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1708.01425v4-Figure3-1.png": {
        "caption": "Figure 3: Cohen’s κ agreement for stance annotation on 98 comments. As a trade-off between reducing costs (i.e., discarding fewer instances) and increasing reliability, we chose 5 annotators and a threshold of 0.95 for this task, which resulted in κ = 0.58 (moderate to substantial agreement).",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1708.01425v4-Table2-1.png": {
        "caption": "Accuracy of each approach (humans and systems) on the development set and test set, respectively.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1708.01425v4-Figure4-1.png": {
        "caption": "Human upper bounds on the argument reasoning comprehension task with respect to education and formal training in reasoning, logic, or argumentation. For each configuration, the mean values are displayed together with the number of participants (above the bar) and with their standard deviations (error bars).",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1708.01425v4-Table1-1.png": {
        "caption": "Details and statistics of the datasets resulting from the eight steps of our methodology implemented in a crowdsourcing process. *Input instances were filtered by their ‘logic score’ assigned in Step 6, such that the weakest 30% were discarded. A more detailed description is available in the readme file of the source code.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1708.01425v4-Figure5-1.png": {
        "caption": "Figure 5: Intra-warrant attention. Only the attention vector for the warrant W1 is shown; the attention vector for W0 is constructed analogously. Grey areas represent a modification with additional context.",
        "content_type": "figure",
        "figure_type": "Schematic"
      }
    },
    "qa": [
      {
        "question": "What are the different steps involved in reconstructing implicit warrants for argument reasoning comprehension?",
        "answer": "The different steps involved in reconstructing implicit warrants for argument reasoning comprehension are:\n1. Sampling comments\n2. Stance annotation\n3. Reason span annotations\n4. Reason gist summarization\n5. Reason disambiguation\n6. Alternative warrant\n7. Alternative warrant validation\n8. Warrant for original claim\n9. Warrant validation",
        "explanation": "The figure shows a schematic of the methodology for reconstructing implicit warrants for argument reasoning comprehension. The figure shows the different steps involved in the process, from sampling comments to validating the warrant for the original claim.",
        "reference": "1708.01425v4-Figure2-1.png"
      },
      {
        "question": "What is the relationship between the number of workers per \"expert\" and Cohen's kappa agreement for stance annotation?",
        "answer": "The Cohen's kappa agreement for stance annotation increases as the number of workers per \"expert\" increases.",
        "explanation": "The figure shows that the Cohen's kappa agreement increases as the number of workers per \"expert\" increases, regardless of the MACE threshold. This suggests that having more workers per \"expert\" leads to more reliable annotations.",
        "reference": "1708.01425v4-Figure3-1.png"
      },
      {
        "question": "Which approach performs best on the development set?",
        "answer": "Intra-warrant attention with context.",
        "explanation": "The table shows that the Intra-warrant attention with context approach has the highest accuracy on the development set, with a score of 0.638.",
        "reference": "1708.01425v4-Table2-1.png"
      },
      {
        "question": "Does formal training in reasoning, logic, or argumentation seem to have a significant effect on argument reasoning comprehension accuracy for people with graduate degrees?",
        "answer": "No, it does not appear to have a significant effect.",
        "explanation": "The figure shows that the accuracy for people with graduate degrees who have no training, some training, and extensive training is all around 80%. The error bars for these groups also overlap, indicating that the differences between the groups are not statistically significant.",
        "reference": "1708.01425v4-Figure4-1.png"
      },
      {
        "question": "Which step in the methodology resulted in the largest decrease in the size of the dataset?",
        "answer": "Step 4, Reason disambiguation.",
        "explanation": "The table shows that the input data for Step 4 had a size of 5,119, while the output data had a size of 1,955. This is a decrease of over 60%.",
        "reference": "1708.01425v4-Table1-1.png"
      },
      {
        "question": "How does the intra-warrant attention mechanism work?",
        "answer": "The intra-warrant attention mechanism uses a BiLSTM to encode the reason and claim, and then provides this encoded information as an attention vector to LSTM layers for each warrant. The attention vector allows the model to focus on specific parts of the reason and claim that are most relevant to each warrant.",
        "explanation": "The figure shows how the intra-warrant attention mechanism works. The reason and claim are first encoded using a BiLSTM. This encoded information is then used to create an attention vector, which is then provided to LSTM layers for each warrant. The attention vector allows the model to focus on specific parts of the reason and claim that are most relevant to each warrant.",
        "reference": "1708.01425v4-Figure5-1.png"
      }
    ]
  },
  "1708.06832v3": {
    "paper_id": "1708.06832v3",
    "all_figures": {
      "1708.06832v3-Figure3-1.png": {
        "caption": "(a) Average relative percentage increase in error from OPT on CIFAR and SVHN at 1/4, 1/2, 3/4 and 1 of the total cost. E.g., the bottom right entry means that if OPT has a 10% final error rate, then AdaLoss has about 10.27%. (b) Test error rates at different fraction of the total costs on ResANN50 and DenseANN169.",
        "content_type": "figure",
        "figure_type": "table"
      },
      "1708.06832v3-Figure1-1.png": {
        "caption": "(a) The common ANN training strategy increases final errors from the optimal (green vs. blue), which decreases exponentially slowly. By learning to focus more on the final auxiliary losses, the proposed adaptive loss weights make a small ANN (orange) to outperform a large one (green) that has non-adaptive weights. (b) Anytime neural networks contain auxiliary predictions and losses, ŷi and `i, for intermediate feature unit fi.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1708.06832v3-Figure4-1.png": {
        "caption": "(a-e) Comparing small networks with AdaLoss versus big ones using CONST. With AdaLoss, the small networks achieve the same accuracy levels faster than large networks with CONST. (f) ANNs performance are mostly decided by underlying models, but AdaLoss is beneficial regardless models.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1708.06832v3-Figure5-1.png": {
        "caption": "(a) EANN performs better if the ANNs use AdaLoss instead of CONST. (b) EANN outperforms linear ensembles of DNNs on ILSVRC. (c) The learned adaptive weights of the same model on three data-sets.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1708.06832v3-Figure2-1.png": {
        "caption": "(a) CONST scheme is increasingly worse than the optimal at deep layers. AdaLoss performs about equally well on all layers in comparison to the OPT. (b) EANN computes its ANNs in order of their depths. An anytime result is used if it is better than all previous ones on a validation set (layers in light blue).",
        "content_type": "figure",
        "figure_type": "plot"
      }
    },
    "qa": [
      {
        "question": "Which method achieves the lowest error rate on ILSVRC at 1/4 of the total cost?",
        "answer": "MSDNNet38",
        "explanation": "The table in Figure (b) shows the error rates of different methods on ILSVRC at different fractions of the total cost. MSDNNet38 + AdaLoss has the lowest error rate of 28.0 at 1/4 of the total cost.",
        "reference": "1708.06832v3-Figure3-1.png"
      },
      {
        "question": "What is the effect of increasing the budget in FLOPS on the test Top-1 error rate for the three different training strategies?",
        "answer": "The test Top-1 error rate decreases as the budget in FLOPS increases for all three training strategies.",
        "explanation": "The figure shows that the test Top-1 error rate decreases as the budget in FLOPS increases for all three training strategies. This is because increasing the budget in FLOPS allows the models to train for longer and learn more complex features.",
        "reference": "1708.06832v3-Figure1-1.png"
      },
      {
        "question": "Which model performs the best on CIFAR100 and ILSVRC datasets?",
        "answer": "EANN with AdaLoss performs the best on both CIFAR100 and ILSVRC datasets.",
        "explanation": "This can be seen in Figure (a) and (b), where EANN with AdaLoss consistently has the lowest error rate compared to other models.",
        "reference": "1708.06832v3-Figure5-1.png"
      }
    ]
  },
  "1709.08294v3": {
    "paper_id": "1709.08294v3",
    "all_figures": {
      "1709.08294v3-Table2-1.png": {
        "caption": "Table 2: Test error rates on document classification tasks (in percentages). S-model indicates that the model has one single convolutional filter, while M-model indicates that the model has multiple convolutional filters. Results marked with ∗ are reported by (Zhang et al., 2015), † are reported by (Conneau et al., 2016), and ‡ are reported by (Lin et al., 2017).",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1709.08294v3-Table4-1.png": {
        "caption": "Table 4: Results of our models on SelQA dataset, compared with previous CNN-based methods. Results marked with ∗ are from (Jurczyk et al., 2016), and marked with ‡ are from (Santos et al., 2017).",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1709.08294v3-Table3-1.png": {
        "caption": "Table 3: Results of our models on WikiQA dataset, compared with previous CNN-based methods.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1709.08294v3-Figure1-1.png": {
        "caption": "Figure 1: The general ACNN framework. Notably, the input sentences to filter generating module and convolution module could be different (see Section 3.3).",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1709.08294v3-Table5-1.png": {
        "caption": "Results on the Quora Question Pairs dataset.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1709.08294v3-Figure3-1.png": {
        "caption": "Comprehensive study of the proposed ACNN framework, including (a) the number of filters (Yelp dataset), and (b) performance vs question types (WikiQA dataset), and (c) t-SNE visualization of learned filter weights (DBpedia dataset).",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1709.08294v3-Figure2-1.png": {
        "caption": "Figure 2: Schematic description of Adaptive Question Answering (AdaQA) model.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1709.08294v3-Table1-1.png": {
        "caption": "Table 1: Dataset statistics.",
        "content_type": "table",
        "figure_type": "N/A"
      }
    },
    "qa": [
      {
        "question": "How does the performance of the two-way AdaQA model compare to the one-way AdaQA model and other CNN-based baseline models on the WikiQA dataset?",
        "answer": "The two-way AdaQA model significantly outperforms the one-way AdaQA model and all other CNN-based baseline models on the WikiQA dataset. This is evident from the higher MAP and MRR values achieved by the two-way model (0.7107 and 0.7304 respectively) compared to the one-way model (0.7005 and 0.7161) and the baseline models.",
        "explanation": "Table 1 presents the MAP and MRR scores for various models on the WikiQA dataset. By comparing the scores of the two-way AdaQA model with those of the one-way AdaQA and other CNN-based models, we can directly observe the performance difference. The higher scores of the two-way model indicate its superior performance in answer selection on this dataset.",
        "reference": "1709.08294v3-Table3-1.png"
      },
      {
        "question": "How does the ACNN framework learn context-sensitive filters?",
        "answer": "The ACNN framework learns context-sensitive filters through two modules: the filter generation module and the adaptive convolution module. The filter generation module produces a set of filters conditioned on the input sentence, while the adaptive convolution module applies the generated filters to an input sentence. The two modules are jointly differentiable, and the overall architecture can be trained in an end-to-end manner.",
        "explanation": "The figure shows the general ACNN framework, with the filter generation module on the left and the convolution module on the right. The arrows between the modules indicate the flow of information, with the filter generation module producing filters based on the input sentence and the convolution module applying those filters to the input sentence.",
        "reference": "1709.08294v3-Figure1-1.png"
      },
      {
        "question": "Which model performed the best on the Quora Question Pairs dataset?",
        "answer": "AdaQA (two-way) + att.",
        "explanation": "The table shows the accuracy of different models on the Quora Question Pairs dataset. The AdaQA (two-way) + att. model has the highest accuracy of 0.8794.",
        "reference": "1709.08294v3-Table5-1.png"
      },
      {
        "question": "Based on the figure, which type of question does ACNN perform the best on?",
        "answer": "ACNN performs best on \"Who\" questions.",
        "explanation": "Figure (b) shows the performance of ACNN and AdaQA on different question types. We can see that ACNN performs better than AdaQA on all question types, and it performs the best on \"Who\" questions.",
        "reference": "1709.08294v3-Figure3-1.png"
      },
      {
        "question": "How does the Adaptive Question Answering (AdaQA) model generate context-aware filters?",
        "answer": "The AdaQA model generates context-aware filters through the filter generation module. This module takes the question and answer as input and outputs a set of filters that are specific to the question and answer pair.",
        "explanation": "The figure shows that the filter generation module is located below the question and answer embedding modules. The question and answer embeddings are fed into the filter generation module, which then outputs the context-aware filters. These filters are then used by the convolution module to encode the question and answer.",
        "reference": "1709.08294v3-Figure2-1.png"
      },
      {
        "question": "Based on Table 1, which dataset has the largest vocabulary size and how does this compare to the average number of words per document in that dataset?",
        "answer": "The Yelp P. dataset has the largest vocabulary size with 25,709 unique words. This is significantly larger than the average number of words per document in the dataset, which is 138.",
        "explanation": "The table provides information about the vocabulary size and average number of words for each dataset. By comparing these values for each dataset, we can see that the Yelp P. dataset has the largest vocabulary size, indicating a wider variety of unique words used in the documents. However, the average document length is relatively short, suggesting that individual documents may not utilize the full range of vocabulary available. This discrepancy could be due to factors such as frequent use of stop words or a high degree of variation in document length within the dataset.",
        "reference": "1709.08294v3-Table1-1.png"
      },
      {
        "question": "Can you explain why the authors claim that their S-ACNN model with a single filter is \"much more expressive\" than the basic S-CNN model, even though it doesn't achieve the best overall performance on either dataset?",
        "answer": "The authors claim that S-ACNN is more expressive than S-CNN because, despite having only one filter, it significantly outperforms S-CNN on both datasets. This suggests that the filter-generation module in ACNN allows for greater flexibility and adaptability, enabling the model to better capture the specific features of each sentence.",
        "explanation": "Table~\\ref{tab:topic} shows the test error rates for different models on the Yelp P. and DBpedia datasets. While S-ACNN doesn't achieve the lowest error rate overall, it shows a substantial improvement over S-CNN, which also uses a single filter. This relative improvement suggests that the adaptivity of the filter in S-ACNN allows it to generate more relevant and informative representations for different sentences, leading to better performance even with limited modeling capacity.",
        "reference": "1709.08294v3-Table2-1.png"
      },
      {
        "question": "Which model performs best on the SelQA dataset and how does it compare to the baseline CNN model reported in Jurczyk et al. (2016)?",
        "answer": "The AdaQA (two-way) + att. model achieves the best performance on the SelQA dataset with a MAP score of 0.9021 and an MRR score of 0.9103. Compared to the baseline CNN model from Jurczyk et al. (2016) which has a MAP score of 0.8320 and an MRR score of 0.8420, the AdaQA (two-way) + att. model demonstrates a significant improvement in both metrics.",
        "explanation": "Table 2 presents the performance of different models on the SelQA dataset, including several baseline models and the proposed AdaQA variants. By comparing the MAP and MRR scores, we can identify the best performing model. The AdaQA (two-way) + att. model shows the highest scores in both metrics, indicating its superior performance. Furthermore, by comparing these scores to the baseline CNN model reported in Jurczyk et al. (2016), we can quantify the improvement achieved by the proposed model.",
        "reference": "1709.08294v3-Table4-1.png"
      }
    ]
  },
  "1802.07222v1": {
    "paper_id": "1802.07222v1",
    "all_figures": {
      "1802.07222v1-Figure4-1.png": {
        "caption": "Algorithm 1 when Theorem 2 holds.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1802.07222v1-Figure5-1.png": {
        "caption": "007’s accuracy for varying drop rates.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1802.07222v1-Figure6-1.png": {
        "caption": "007’s accuracy for varying noise levels.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1802.07222v1-Figure3-1.png": {
        "caption": "When Theorem 2 holds.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1802.07222v1-Table1-1.png": {
        "caption": "Number of ICMPs per second per switch (T ). We see max(T )≤ Tmax.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1802.07222v1-Figure1-1.png": {
        "caption": "Observations from a production network: (a) CDF of the number of flows with at least one retransmission; (b) CDF of the fraction of drops belonging to each flow in each 30 second interval.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1802.07222v1-Figure2-1.png": {
        "caption": "Overview of 007 architecture",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1802.07222v1-Figure16-1.png": {
        "caption": "Illustration of notation for Clos topology used in the proof of Lemma 2",
        "content_type": "figure",
        "figure_type": "** Schematic"
      },
      "1802.07222v1-Figure8-1.png": {
        "caption": "007’s accuracy under skewed traffic.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1802.07222v1-Figure12-1.png": {
        "caption": "Algorithm 1 with multiple failures. The drop rates on the links are heavily skewed.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1802.07222v1-Figure7-1.png": {
        "caption": "Varying the number of connections.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1802.07222v1-Figure11-1.png": {
        "caption": "Impact of link location on Algorithm 1.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1802.07222v1-Figure9-1.png": {
        "caption": "Impact of a hot ToR on 007’s accuracy.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1802.07222v1-Figure10-1.png": {
        "caption": "Algorithm 1 with single failure.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1802.07222v1-Table2-1.png": {
        "caption": "Notation and nomenclature",
        "content_type": "table",
        "figure_type": "table"
      },
      "1802.07222v1-Figure13-1.png": {
        "caption": "Distribution of the difference between votes on bad links and the maximum vote on good links for different bad link drop rates.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1802.07222v1-Figure15-1.png": {
        "caption": "Simple tomography example.",
        "content_type": "figure",
        "figure_type": "Schematic"
      },
      "1802.07222v1-Figure14-1.png": {
        "caption": "Number of network related reboots in a day.",
        "content_type": "figure",
        "figure_type": "plot"
      }
    },
    "qa": [
      {
        "question": "During which hours of the day did the most network-related reboots occur?",
        "answer": "The most network-related reboots occurred between 18:00 and 20:00.",
        "explanation": "The plot shows the number of network-related reboots as a function of the hour of the day. The peak of the plot occurs between 18:00 and 20:00, indicating that the most reboots occurred during this time period.",
        "reference": "1802.07222v1-Figure14-1.png"
      }
    ]
  },
  "1803.02750v3": {
    "paper_id": "1803.02750v3",
    "all_figures": {
      "1803.02750v3-Figure1-1.png": {
        "caption": "Experiment setup: 15 nodes in a partial mesh topology replicating an always-growing set. The left plot depicts the number of elements being sent throughout the experiment, while the right plot shows the CPU processing time ratio with respect to state-based. Not only does delta-based synchronization not improve state-based in terms of state transmission, it even incurs a substantial processing overhead.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1803.02750v3-TableI-1.png": {
        "caption": "TABLE I: Description of micro-benchmarks.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1803.02750v3-Figure6-1.png": {
        "caption": "Network topologies employed: a 15-node partial-mesh (to the left) and a 15-node tree (to the right).",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1803.02750v3-Figure7-1.png": {
        "caption": "Transmission of GSet and GCounter with respect to delta-based BP+RR – tree and mesh topologies.",
        "content_type": "figure",
        "figure_type": "plot."
      },
      "1803.02750v3-Figure13-1.png": {
        "caption": "Hasse diagram of P({a, b}) P({a, b}), a nondistributive lattice.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1803.02750v3-Figure2-1.png": {
        "caption": "Specifications of two data types, replica i ∈ I.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1803.02750v3-Figure3-1.png": {
        "caption": "Hasse diagram of two data types.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1803.02750v3-Figure8-1.png": {
        "caption": "Transmission of GMap 10%, 30%, 60% and 100% – tree and mesh topologies.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1803.02750v3-Figure4-1.png": {
        "caption": "Delta-based synchronization of a GSet with 2 replicas A,B ∈ I. Underlined elements represent the BP optimization.",
        "content_type": "figure",
        "figure_type": "** Schematic."
      },
      "1803.02750v3-Figure5-1.png": {
        "caption": "Delta-based synchronization of a GSet with 4 replicas A,B,C,D ∈ I. The overlined element represents the RR optimization.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1803.02750v3-Figure9-1.png": {
        "caption": "Metadata required per node when synchronizing a GSet in a mesh topology. Each node has 4 neighbours (as in Figure 6) and each node identifier has size 20B.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1803.02750v3-Figure10-1.png": {
        "caption": "Average memory ratio with respect to BP+RR for GCounter, GSet, GMap 10% and 100% – mesh topology",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1803.02750v3-TableII-1.png": {
        "caption": "TABLE II: Retwis workload characterization: for each operation, the number of CRDT updates performed and its workload percentage.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1803.02750v3-TableIII-1.png": {
        "caption": "TABLE III: Composition techniques that yield lattices satisfying DCC and distributive lattices, given lattices A and B, chain C, partial order P and (unordered) set U .",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1803.02750v3-TableIV-1.png": {
        "caption": "TABLE IV: Composition techniques that yield finite ideals or quotients, given lattices A and B, chain C, partial order P , all satisfying DCC, and (unordered) set U .",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1803.02750v3-Figure12-1.png": {
        "caption": "CPU overhead of classic delta-based when compared to delta-based BP+RR.",
        "content_type": "figure",
        "figure_type": "plot"
      }
    },
    "qa": [
      {
        "question": "Which synchronization method is the most efficient in terms of CPU processing time?",
        "answer": "The proposed method compared to state-based and delta-based methods.",
        "explanation": "The right plot shows that this paper's method has the lowest CPU processing time ratio compared to the other two methods.",
        "reference": "1803.02750v3-Figure1-1.png"
      },
      {
        "question": "What is the difference between GCounter and GSet?",
        "answer": "GCounter measures the number of times an event has occurred, while GSet measures the number of unique elements in a set.",
        "explanation": "The table shows that GCounter is incremented each time a periodic event occurs, while GSet is only incremented when a unique element is added. This means that GCounter can be used to count the total number of events, while GSet can be used to count the number of unique events.",
        "reference": "1803.02750v3-TableI-1.png"
      },
      {
        "question": "Which of the algorithms is most efficient in terms of transmission in a tree topology?",
        "answer": "Op-based GSet.",
        "explanation": "The figure shows that Op-based GSet has the lowest transmission ratio compared to all other algorithms in a tree topology.",
        "reference": "1803.02750v3-Figure7-1.png"
      },
      {
        "question": "What is the role of the RR optimization in the delta-based synchronization of a GSet?",
        "answer": "The RR optimization helps to reduce the number of messages that need to be exchanged between replicas.",
        "explanation": "The figure shows that the RR optimization allows replica C to send a single message to replica D, which contains the updates from both replicas A and B. This is because replica C knows that replica D has already received the updates from replica B. Without the RR optimization, replica C would have to send two messages, one to replica D and one to replica B.",
        "reference": "1803.02750v3-Figure5-1.png"
      },
      {
        "question": "How does the average metadata required per node for the Op-based BP+RR approach change as the number of nodes in the network increases?",
        "answer": "The average metadata required per node for the Op-based BP+RR approach increases as the number of nodes in the network increases.",
        "explanation": "This can be seen in the figure, where the height of the yellow bar for the Op-based BP+RR approach increases from 0.2 MB for 16 nodes to 155.5 MB for 64 nodes.",
        "reference": "1803.02750v3-Figure9-1.png"
      },
      {
        "question": "Which algorithm performs the best in terms of average memory ratio with respect to BP+RR for GMap 10%?",
        "answer": "Delta-based BP+RR",
        "explanation": "The figure shows that the bar for Delta-based BP+RR is the shortest for GMap 10%, indicating that it has the lowest average memory ratio with respect to BP+RR.",
        "reference": "1803.02750v3-Figure10-1.png"
      },
      {
        "question": "What is the difference between the `inc_i(p)` and `inc_i'(p)` operations in the Grow-only Counter data type?",
        "answer": "The `inc_i(p)` operation increments the value associated with the key `i` in the counter `p`, while the `inc_i'(p)` operation increments the value associated with the key `i` in the counter `p` only if the key `i` is not already present in the counter.",
        "explanation": "The figure shows the specifications of the Grow-only Counter data type. The `inc_i(p)` operation is defined as `p[i ↦ p(i) + 1]`, which means that it increments the value associated with the key `i` in the counter `p`. The `inc_i'(p)` operation is defined as `[{i ↦ p(i) + 1}]`, which means that it increments the value associated with the key `i` in the counter `p` only if the key `i` is not already present in the counter.",
        "reference": "1803.02750v3-Figure2-1.png"
      },
      {
        "question": "Which topology has the highest transmission rate for GMap 100%?",
        "answer": "Mesh",
        "explanation": "The figure shows that the transmission rate for GMap 100% is higher for the mesh topology than for the tree topology. This can be seen by comparing the curves for the two topologies in the top right panel of the figure.",
        "reference": "1803.02750v3-Figure8-1.png"
      },
      {
        "question": "If a user with 100 followers posts a tweet, how many CRDT updates will be performed in total, and what percentage of the overall workload does this represent?",
        "answer": "Posting a tweet will result in 1 + 100 = 101 CRDT updates. This represents 35% of the overall workload.",
        "explanation": "The table shows that a \"Post Tweet\" operation requires 1 + #Followers updates. In this case, the user has 100 followers, so the total updates are 1 + 100. The table also tells us that \"Post Tweet\" operations make up 35% of the total workload.",
        "reference": "1803.02750v3-TableII-1.png"
      },
      {
        "question": "If we use the lexicographic product with a chain as the first component and a distributive lattice as the second component to design a CRDT, will the resulting CRDT lattice be guaranteed to be distributive and satisfy the descending chain condition (DCC)?",
        "answer": "Yes, the resulting CRDT lattice will be guaranteed to be both distributive and satisfy the DCC.",
        "explanation": "Table 1 shows that when the first component of the lexicographic product is a chain and the second component is a distributive lattice, the resulting lattice is both distributive and satisfies the DCC. This is indicated by the checkmarks in the corresponding cells of the table. The passage further emphasizes this point by highlighting that this specific use of the lexicographic product is typical in designing CRDTs and that the resulting construct inherits distributivity from the second component.",
        "reference": "1803.02750v3-TableIII-1.png"
      },
      {
        "question": "How does the CPU overhead of classic delta-based compare to delta-based BP+RR as the Zipf coefficient increases?",
        "answer": "The CPU overhead of classic delta-based is consistently higher than that of delta-based BP+RR as the Zipf coefficient increases.",
        "explanation": "The plot shows the CPU overhead of both classic delta-based and delta-based BP+RR for different Zipf coefficients. The green line represents classic delta-based, and the blue line represents delta-based BP+RR. As the Zipf coefficient increases, the CPU overhead of both algorithms increases, but the CPU overhead of classic delta-based increases at a faster rate.",
        "reference": "1803.02750v3-Figure12-1.png"
      }
    ]
  },
  "1803.05776v2": {
    "paper_id": "1803.05776v2",
    "all_figures": {
      "1803.05776v2-Figure1-1.png": {
        "caption": "Results for the cerebellum data (a) Adjacency matrix, (b) NMSE for testing data as a function of training data size at SNR=10dB, and (c) at SNR=0dB.",
        "content_type": "figure",
        "figure_type": "plot"
      }
    },
    "qa": [
      {
        "question": "Which model performs the best at SNR=0dB?",
        "answer": "GPG-K",
        "explanation": "The NMSE for GPG-K is the lowest at SNR=0dB, as shown in Figure (c).",
        "reference": "1803.05776v2-Figure1-1.png"
      }
    ]
  },
  "1804.04410v2": {
    "paper_id": "1804.04410v2",
    "all_figures": {
      "1804.04410v2-Figure1-1.png": {
        "caption": "Figure 1: A telescoping architecture employed in Bing’s retrieval system. Documents are scanned using a pre-defined match plan. Matched documents are passed through additional rank-and-prune stages.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1804.04410v2-Figure2-1.png": {
        "caption": "Figure 2: The reduction in index blocks accessed from the learned policy for CAT2 queries on the weighted set. We intentionally leave out the actual page access numbers on the y-axis because of confidentiality. The queries on the x-axis are sorted by page access independently for each treatment.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1804.04410v2-Table1-1.png": {
        "caption": "Table 1: Changes in NCG and the index blocks accessed u from our learned policy relative to production baselines. In both categories, we observe significant reduction in index blocks accessed, although at the cost of some loss in relevance in case of CAT1. All the differences in NCG and u are statistically significant (p < 0.01). Coverage of CAT2 queries in the unweighted set is too low to report numbers.",
        "content_type": "table",
        "figure_type": "N/A"
      }
    },
    "qa": [
      {
        "question": "How are documents ranked and pruned in the telescoping architecture?",
        "answer": "Documents are first matched using a pre-defined match plan. Then, they are passed through additional rank-and-prune stages, which are implemented as a cascade of machine learning models.",
        "explanation": "The figure shows the telescoping architecture, with the matching stage (L0) at the bottom and the rank-and-prune stages (L1, L2) above it. The arrows indicate the flow of documents through the system.",
        "reference": "1804.04410v2-Figure1-1.png"
      },
      {
        "question": "How does the RL policy compare to the baseline in terms of index blocks accessed?",
        "answer": "The RL policy accesses fewer index blocks than the baseline.",
        "explanation": "The figure shows that the RL policy line is always below the baseline line, which means that the RL policy accesses fewer index blocks than the baseline for all queries.",
        "reference": "1804.04410v2-Figure2-1.png"
      },
      {
        "question": "How does the performance of the learned policy compare to the production baseline for CAT2 queries in terms of relevance and efficiency?",
        "answer": "For CAT2 queries, the learned policy shows a slight improvement in relevance (NCG) for the weighted set and a significant reduction in index blocks accessed for both weighted and unweighted sets.",
        "explanation": "Table 1 shows the changes in NCG (a measure of relevance) and index blocks accessed (a measure of efficiency) for both CAT1 and CAT2 queries. Comparing the values for CAT2 to the baseline (0%), we see a small positive change in NCG for the weighted set (+0.2%) and a significant decrease in index blocks accessed for both sets (-22.7% for weighted and unspecified for unweighted). This indicates that the learned policy maintains similar relevance while significantly improving efficiency for CAT2 queries.",
        "reference": "1804.04410v2-Table1-1.png"
      }
    ]
  },
  "1804.05995v2": {
    "paper_id": "1804.05995v2",
    "all_figures": {
      "1804.05995v2-Figure3-1.png": {
        "caption": "Category network for the stanford, california Wikipedia article (A). In this example, the two leftmost categories (assigned to A via the instanceof edges) are both sub-",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1804.05995v2-Table1-1.png": {
        "caption": "Top 5 section recommendations for the Wikipedia article lausanne, according to various methods.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1804.05995v2-Figure5-1.png": {
        "caption": "Precision and recall as a function of the number of recommended sections k , for all methods we evaluate.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1804.05995v2-Figure4-1.png": {
        "caption": "Percentage of categories (y-axis) that can generate at least x recommendations using the section-count–based",
        "content_type": "figure",
        "figure_type": "plot"
      }
    },
    "qa": [
      {
        "question": "What are the top 5 section recommendations for the Wikipedia article on Lausanne according to the category-section counts method?",
        "answer": "The top 5 section recommendations for the Wikipedia article on Lausanne according to the category-section counts method are HISTORY, DEMOGRAPHICS, ECONOMY, EDUCATION, and POLITICS.",
        "explanation": "The table in the figure shows the top 5 section recommendations for the Wikipedia article on Lausanne according to four different methods. The category-section counts method is one of these methods. The top 5 recommendations for this method are listed in the third column of the table.",
        "reference": "1804.05995v2-Table1-1.png"
      },
      {
        "question": "What is the trend in precision and recall as the number of recommended sections k increases?",
        "answer": "Precision generally decreases and recall generally increases as k increases.",
        "explanation": "This can be seen in all four subplots of the figure. For example, in (a), the blue line representing precision starts at a high value for small k and then decreases as k increases, while the red line representing recall starts at a low value for small k and then increases as k increases.",
        "reference": "1804.05995v2-Figure5-1.png"
      },
      {
        "question": "What is the percentage of categories that can generate at least 10 recommendations using the section-count-based method?",
        "answer": "Around 68%.",
        "explanation": "The figure shows that the percentage of categories that can generate at least 10 recommendations is 60%. This can be seen by looking at the bar for length 10 on the x-axis and following it up to the y-axis.",
        "reference": "1804.05995v2-Figure4-1.png"
      }
    ]
  },
  "1805.00912v4": {
    "paper_id": "1805.00912v4",
    "all_figures": {
      "1805.00912v4-Figure1-1.png": {
        "caption": "(a) Memory consumption and (b) time cost vs. sequence length on synthetic data; (c) memory load (x-axis), inference time on dev set (y-axis) and test accuracy on the SNLI dataset.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1805.00912v4-Table3-1.png": {
        "caption": "Table 3: An ablation study of MTSA on SNLI.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1805.00912v4-Table1-1.png": {
        "caption": "Table 1: Experimental results for different methods with comparative parameter number on SNLI. |θ|: the number of parameters (excluding word embedding part); Time/Epoch: averaged training time per epoch with batch size 128; Inf. Time: averaged dev inference time with batch size 128; Memory: memory load on synthetic data of sequence length 64 and batch size 64 with back-propagation considered; Train Acc. and Test Acc.: the accuracies on training/test sets. All state-of-the-art methods in leaderboard are listed in Table 1&2 up to Sep. 2018.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1805.00912v4-Table2-1.png": {
        "caption": "Experimental results on sentence-encoding based SNLI and MultiNLI benchmark tasks. “Transfer” denotes pretrained language model on large corpus for transfer learning, which detailed by Radford et al. (2018). References: a(Nie and Bansal, 2017), b(Chen et al., 2018), c(Talman et al., 2018).",
        "content_type": "table",
        "figure_type": "table"
      },
      "1805.00912v4-Table4-1.png": {
        "caption": "Table 4: Experimental Results of SRL for single models on CoNLL-05 with gold predicates. ∗Multi-head baseline is equivalent to the model in Tan et al. (2017). For fair comparisons, first, we use the hyper-parameters provided by Tan et al. (2017) instead of tuning them; second, all listed models are independent of external linguistics information, e.g., PoS, dependency parsing.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1805.00912v4-Table5-1.png": {
        "caption": "Experimental results on five sentence classification benchmarks. References: a(Mikolov et al., 2013), b(Kiros et al., 2015), c(Kalchbrenner et al., 2014), d(Lei and Zhang, 2017).",
        "content_type": "table",
        "figure_type": "table"
      },
      "1805.00912v4-Figure2-1.png": {
        "caption": "Tensorized self-attention (TSA) Mechanism.",
        "content_type": "figure",
        "figure_type": "Schematic"
      },
      "1805.00912v4-Table6-1.png": {
        "caption": "Table 6: Results for the Transformer with either multihead self-attention or proposed MTSA. The reported BLEU values for Setup 1 and 2 are the mean of 5 and 3 runs respectively.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1805.00912v4-Figure3-1.png": {
        "caption": "Heatmaps for normalized token2token and source2token alignment scores with forward and backward masks. Each row shows three types of scores associated with the tokens from a same sentence: token2token alignment scores in TSA with forward mask (left), token2token alignment scores in TSA with backward masks (middle), source2token alignment scores at token level for the two heads with forward and backward masks (right). The tokens in x-axis and y-axis are the dependent and governor tokens, respectively.",
        "content_type": "figure",
        "figure_type": "plot"
      }
    },
    "qa": [
      {
        "question": "Which model has the lowest memory consumption and time cost on synthetic data?",
        "answer": "MTSA",
        "explanation": "Figure (a) and (b) show that the MTSA model has the lowest memory consumption and time cost for all sequence lengths.",
        "reference": "1805.00912v4-Figure1-1.png"
      },
      {
        "question": "Which model has the highest test accuracy on the SNLI dataset, and how does its training time per epoch compare to the MTSA model?",
        "answer": "The model with the highest test accuracy is MTSA, with an accuracy of 86.3%. Its training time per epoch is 180s, which is faster than the training time of several other models with lower accuracy, such as Bi-LSTM (854s), Bi-GRU (850s), and DiSA (390s).",
        "explanation": "The table shows the test accuracy and training time per epoch for various models. By comparing these values across different models, we can identify the model with the best performance and understand its relative training efficiency. In this case, MTSA achieves the highest test accuracy while also having a relatively fast training time per epoch.",
        "reference": "1805.00912v4-Table1-1.png"
      },
      {
        "question": "Which model performed best on the SNLI test set?",
        "answer": "The Transfer + MTSA model performed best on the SNLI test set with an accuracy of 86.9%.",
        "explanation": "The table shows the accuracy of different models on the SNLI and MultiNLI benchmark tasks. The Transfer + MTSA model has the highest accuracy on the SNLI test set.",
        "reference": "1805.00912v4-Table2-1.png"
      },
      {
        "question": "Based on the table, how does MTSA compare to the Bi-LSTM and Multi-CNN baselines in terms of performance and training time?",
        "answer": "MTSA outperforms both Bi-LSTM and Multi-CNN baselines across all evaluation metrics (P, R, F1, and Comp.) on all three test sets (Development, WSJ Test, and Brown Test). While MTSA achieves the highest scores, its training time is comparable to Multi-head and Multi-CNN, and significantly faster than Bi-LSTM.",
        "explanation": "The table displays the performance of various SRL models on different test sets, along with their training times. By comparing the values in the MTSA row to those in the Bi-LSTM and Multi-CNN rows, we can see that MTSA consistently achieves higher scores across all metrics while maintaining a similar training time to Multi-CNN and a much faster training time than Bi-LSTM. This demonstrates the superior performance and efficiency of MTSA compared to these two baselines.",
        "reference": "1805.00912v4-Table4-1.png"
      },
      {
        "question": "What is the purpose of the positional mask in the TSA mechanism?",
        "answer": "The positional mask is used to provide information about the relative position of tokens in the input sequence. This information is used to compute the attention weights, which determine how much each token attends to each other token.",
        "explanation": "The figure shows that the positional mask is used as input to the Softmax function, which computes the attention weights. The positional mask is a matrix of values that indicate the relative position of each token in the input sequence. This information is used to compute the attention weights, which determine how much each token attends to each other token.",
        "reference": "1805.00912v4-Figure2-1.png"
      }
    ]
  },
  "1805.04687v2": {
    "paper_id": "1805.04687v2",
    "all_figures": {
      "1805.04687v2-Table4-1.png": {
        "caption": "Table 4: Domain discrepancy experiments with object detection. We take the images from one domain and report testing results in AP on the same domain or the opposite domain. We can observe significant domain discrepancies, especially between daytime and nighttime.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1805.04687v2-Figure9-1.png": {
        "caption": "Visual comparisons of the same model (DRN [35]) trained on different datasets. We find that there is a dramatic domain shift between Cityscapes and our new dataset. For example, due to infrastructure difference, the model trained on Cityscapes is confused by some simple categories such as sky and traffic signs.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1805.04687v2-Figure11-1.png": {
        "caption": "Drivable area prediction by segmentation. The segmentation predicts the drivable area with lanes well, as shown in the top row. Also, we find that the segmentation model learns to interpolate in areas that has no lane markings.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1805.04687v2-Figure10-1.png": {
        "caption": "Distribution of images in weather, scene, and day hours categories.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1805.04687v2-Figure1-1.png": {
        "caption": "Overview of our dataset. Our dataset includes a diverse set of driving videos under various weather conditions, time, and scene types. The dataset also comes with a rich set of annotations: scene tagging, object bounding box, lane marking, drivable area, full-frame semantic and instance segmentation, multiple object tracking, and multiple object tracking with segmentation.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1805.04687v2-Table7-1.png": {
        "caption": "Table 7: Evaluation results for multiple object tracking cascaded with object detection. AP is the detection metric. Even though the tracking set has much more boxes, the model can still benefit from the diverse instance examples in the detection set.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1805.04687v2-Table5-1.png": {
        "caption": "Table 5: Evaluation results of homogeneous multitask learning on lane marking and drivable area segmentation. We train lane marking, drivable area segmentation and the joint training of both on training splits with 10K, 20K, and the full 70K images.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1805.04687v2-Table6-1.png": {
        "caption": "Table 6: Evaluation results for instance segmentation when joint training with the object detection set. Additional localization supervision can improve instance segmentation significantly.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1805.04687v2-Table11-1.png": {
        "caption": "Table 11: Annotations of the BDD100K MOT dataset by category.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1805.04687v2-Table10-1.png": {
        "caption": "Table 10: Comparisons on number of pedestrians with other datasets. The statistics are based on the training set in each dataset.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1805.04687v2-Table13-1.png": {
        "caption": "Full evaluation results of the domain discrepancy experiments with object detection.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1805.04687v2-Table14-1.png": {
        "caption": "Table 14: Full evaluation results of the individual lane marking task and the joint training of lane marking and the drivable area detection. We report the ODS-F scores with different thresholds τ = 1, 2, 10 pixels of direction, continuity as well as each category.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1805.04687v2-Figure4-1.png": {
        "caption": "Figure 4: Image tagging classification results using DLA-34.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1805.04687v2-Figure2-1.png": {
        "caption": "Figure 2: Geographical distribution of our data sources. Each dot represents the starting location of every video clip. Our videos are from many cities and regions in the populous areas in the US.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1805.04687v2-Figure3-1.png": {
        "caption": "Instance statistics of our object categories. (a) Number of instances of each category, which follows a long-tail distribution. (b) Roughly half of the instances are occluded. (c) About 7% of the instances are truncated.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1805.04687v2-Figure15-1.png": {
        "caption": "Example annotations for BDD100K MOTS. Frames are down-sampled for visualization.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1805.04687v2-Figure14-1.png": {
        "caption": "Figure 14: Distribution of classes in semantic instance segmentation. It presents a long-tail effect with more than 10 cars and poles per image, but only tens of trains in the whole dataset.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1805.04687v2-Table8-1.png": {
        "caption": "Table 8: Evaluation results for semantic segmentation. We explore segmentation joint-training with different tasks. Detection can improve the overall accuracy of segmentation, although their output structures are different. However, although Lane and Drivable area improve the segmentation of road and sidewalk, the overall accuracy drops.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1805.04687v2-Table9-1.png": {
        "caption": "Table 9: MOTS evaluation results. Both instance segmentation AP and MOTS evaluation metrics are reported. Instance segmentation tracking is very hard to label, but we are able to use object detection, tracking, and instance segmentation to improve segmentation tracking accuracy significantly.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1805.04687v2-Figure5-1.png": {
        "caption": "Examples of lane marking annotations. Red lanes are vertical and blue lanes are parallel. Left: we label all the visible lane boundaries. Middle: not all marking edges are lanes for vehicles to follow, such as pedestrian crossing. Right: parallel lanes can also be along the current driving direction.",
        "content_type": "figure",
        "figure_type": "** photograph(s)"
      },
      "1805.04687v2-Figure6-1.png": {
        "caption": "Examples of drivable areas. Red regions are directly drivable and the blue ones are alternative. Although drivable areas can be confined within lane markings, they are also related to locations of other vehicles shown in the right two columns.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1805.04687v2-Table1-1.png": {
        "caption": "Lane marking statistics. Our lane marking annotations are significantly richer and are more diverse.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1805.04687v2-Figure12-1.png": {
        "caption": "Distribution of different types of lane markings and drivable areas.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1805.04687v2-Figure13-1.png": {
        "caption": "Trajectories of example driving videos.",
        "content_type": "figure",
        "figure_type": "Plot."
      },
      "1805.04687v2-Table12-1.png": {
        "caption": "Table 12: Annotations of BDD100K MOTS by category.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1805.04687v2-Figure8-1.png": {
        "caption": "Number of occlusions by track (left) and number of occluded frames for each occlusion (right). Our dataset covers complicated occlusion and reappearing patterns.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1805.04687v2-Table3-1.png": {
        "caption": "Table 3: Comparisons with other MOTS and VOS datasets.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1805.04687v2-Figure7-1.png": {
        "caption": "Cumulative distributions of the box size (left), the ratio between the max and min box size for each track (middle) and track length (right). Our dataset is more diverse in object scale.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1805.04687v2-Table2-1.png": {
        "caption": "Table 2: MOT datasets statistics of training and validation sets. Our dataset has more sequences, frames, identities as well as more box annotations.",
        "content_type": "table",
        "figure_type": "N/A"
      }
    },
    "qa": [
      {
        "question": "How does increasing the training set size affect the performance of the lane marking and drivable area segmentation tasks?",
        "answer": "Increasing the training set size generally leads to improved performance for both lane marking and drivable area segmentation tasks.",
        "explanation": "The table shows the evaluation results for models trained on different sized datasets (10K, 20K, and 70K images). For both lane marking (ODS-F metric) and drivable area segmentation (IoU metric), the performance generally improves as the training set size increases. This is evident when comparing the \"mean\" values across the different training set sizes for each task. \n\nFor example, the mean ODS-F for lane marking increases from 45.41% with 10K images to 54.36% with 20K images and further to 54.48% with 70K images. Similarly, the mean IoU for drivable area segmentation increases from 64.23% with 10K images to 71.13% with 20K images and 71.37% with 70K images. \n\nThis trend suggests that having more training data allows the model to learn more effectively and generalize better, resulting in improved performance on both tasks.",
        "reference": "1805.04687v2-Table5-1.png"
      },
      {
        "question": "How does joint training with the object detection set affect instance segmentation performance, and what is the likely reason for this effect?",
        "answer": "Joint training with the object detection set improves instance segmentation performance significantly. This is evidenced by the increase in AP, AP50, and AP75 metrics in Table 1 when comparing \"Inst-Seg\" and \"Inst-Seg + Det\" rows.\n\nThe passage explains that this improvement is likely due to the richer diversity of images and object examples in the detection set. This allows the instance segmentation model to learn better object appearance features and localization.",
        "explanation": "Table 1 directly shows the performance improvement through the increased metric values. The passage further clarifies the reason behind this improvement by highlighting the difference in dataset sizes and the benefits of additional object examples for feature learning.",
        "reference": "1805.04687v2-Table6-1.png"
      },
      {
        "question": "Based on Table 1, which category of objects has the largest total number of annotations in the BDD100K MOT dataset, considering both bounding boxes and instance tracks? ",
        "answer": "Cars have the largest total number of annotations.",
        "explanation": "Table 1 shows the number of annotations for different categories in the BDD100K MOT dataset, including both bounding boxes (\"Boxes\") and instance tracks (\"Tracks\"). By adding the values for \"car\" in both rows, we find that cars have a total of approximately 2.7 million annotations (97K tracks + 2.6M boxes), which is greater than any other category.",
        "reference": "1805.04687v2-Table11-1.png"
      },
      {
        "question": "How does the segmentation model perform in areas with no lane markings?",
        "answer": "The segmentation model learns to interpolate in areas that have no lane markings.",
        "explanation": "This can be seen in the bottom row of the figure, where the model correctly predicts the drivable area even though there are no lane markings present.",
        "reference": "1805.04687v2-Figure11-1.png"
      },
      {
        "question": "What is the most common type of scene in the dataset?",
        "answer": "City Street",
        "explanation": "The figure shows that City Street has the highest number of instances, with approximately 61,960 instances.",
        "reference": "1805.04687v2-Figure10-1.png"
      },
      {
        "question": "What are the different types of annotations that are included in the dataset?",
        "answer": "The dataset includes a rich set of annotations: scene tagging, object bounding box, lane marking, drivable area, full-frame semantic and instance segmentation, multiple object tracking, and multiple object tracking with segmentation.",
        "explanation": "The figure shows different types of annotations that are included in the dataset. For example, the bounding boxes around the cars indicate the object bounding box annotations, the different colored lines on the road indicate the lane marking annotations, and the different colored areas indicate the drivable area annotations.",
        "reference": "1805.04687v2-Figure1-1.png"
      },
      {
        "question": "Explain why the model trained on both MOT and detection sets has a higher number of identity switches (IDS) compared to the model trained only on the MOT set, even though it achieves better performance in terms of AP, MOTA, and MOTP?",
        "answer": "While the model trained on both MOT and detection sets shows improved performance in detection and tracking metrics (AP, MOTA, MOTP), it also exhibits a higher number of identity switches (IDS). This can be attributed to the increased diversity of instances introduced by the detection set. Although the MOT set provides a larger number of bounding boxes for training, the detection set adds varied examples that may lead to more frequent identity switches during tracking, even as it improves the model's overall performance.",
        "explanation": "The table shows that the model trained on both MOT and detection sets (\"MOT + Det\") has a higher IDS value (9098) compared to the model trained only on the MOT set (8386). This might seem contradictory at first, given the improvement in other metrics. However, the passage provides the key insight: the detection set introduces more diverse examples. This diversity can lead to situations where the model struggles to maintain consistent identities across frames, resulting in more identity switches.",
        "reference": "1805.04687v2-Table7-1.png"
      },
      {
        "question": "Which domain discrepancy has a larger impact on object detection performance: city vs. non-city or daytime vs. nighttime?",
        "answer": "Daytime vs. nighttime has a larger impact on object detection performance.",
        "explanation": "The table shows the Average Precision (AP) for object detection across different training and testing domains. While there is a noticeable difference in AP between city and non-city images, the drop in performance is much more significant when models trained on daytime images are tested on nighttime images, and vice versa. This observation is further supported by the passage, which explicitly mentions that \"the gap between daytime and nighttime is much bigger\" compared to the city vs. non-city discrepancy.",
        "reference": "1805.04687v2-Table4-1.png"
      },
      {
        "question": "Why does the proposed dataset have a lower number of persons per image compared to the Cityscapes dataset, even though it has a much higher total number of pedestrians?",
        "answer": "The proposed dataset contains non-city scenes like highways, which typically have fewer pedestrians per image compared to cityscapes.",
        "explanation": "Table 1 shows that the \"Ours\" dataset has a significantly higher total number of pedestrians (86,047) compared to Cityscapes (19,654). However, the passage explains that the \"Ours\" dataset also includes non-city scenes like highways. These scenes usually have fewer pedestrians compared to cityscapes, which explains why the number of persons per image is lower for \"Ours\" (1.2) than Cityscapes (7.0).",
        "reference": "1805.04687v2-Table10-1.png"
      },
      {
        "question": "How does the performance of lane marking detection change with different thresholds (τ) for direction, continuity, and category?",
        "answer": "As the threshold (τ) increases, the ODS-F scores for direction, continuity, and category generally increase as well. This indicates that the model performs better in detecting lane markings with higher thresholds, meaning it can tolerate larger deviations from the ground truth annotations.",
        "explanation": "The table presents ODS-F scores for different thresholds (τ = 1, 2, 10 pixels) across various training sets and lane marking attributes. By comparing the scores across different thresholds within each attribute and training set, we can observe the general trend of increasing performance with increasing thresholds. For example, for Lane 10K training set, the average ODS-F score for direction increases from 28.38 at τ = 1 to 36.19 at τ = 2 and 49.29 at τ = 10. This pattern holds true for most attributes and training sets, indicating that the model performs better with higher thresholds.",
        "reference": "1805.04687v2-Table14-1.png"
      },
      {
        "question": "Which weather condition has the highest classification accuracy?",
        "answer": "Clear weather.",
        "explanation": "The figure shows the classification accuracy for different weather conditions. The bar for clear weather is the highest, indicating that it has the highest accuracy.",
        "reference": "1805.04687v2-Figure4-1.png"
      },
      {
        "question": "Which type of object is the most common in the dataset?",
        "answer": "Cars are the most common object in the dataset.",
        "explanation": "The figure shows that there are almost 60 thousand car instances, which is more than any other type of object.",
        "reference": "1805.04687v2-Figure14-1.png"
      },
      {
        "question": "Based on the table, which approach achieved the highest mean IoU for semantic segmentation, and how did it perform compared to the baseline Sem-Seg model?",
        "answer": "The **Sem-Seg + Det** approach achieved the highest mean IoU of 58.3, which is an improvement of 1.4 points compared to the baseline Sem-Seg model with a mean IoU of 56.9.",
        "explanation": "The table shows the mean IoU scores for different semantic segmentation approaches. We can see that the \"Sem-Seg + Det\" model has the highest score in the \"mean IoU\" column, indicating it achieved the best overall performance. Additionally, by comparing this score to the \"Sem-Seg\" model, we can quantify the improvement achieved by adding object detection to the training process.",
        "reference": "1805.04687v2-Table8-1.png"
      },
      {
        "question": "Which training approach achieved the best balance between minimizing false negatives (FN) and false positives (FP) in object detection, while also maintaining a high MOTSA score?",
        "answer": "The training approach \"Det + T + I + S\" achieved the best balance between minimizing false negatives (FN) and false positives (FP) in object detection, while also maintaining a high MOTSA score.",
        "explanation": "The table shows that \"Det + T + I + S\" achieved the lowest FN count (5132) while having a relatively low FP count (6228) compared to other approaches. Although \"MOT (T) + MOTS\" has a slightly lower FP count, its FN count is significantly higher. Additionally, \"Det + T + I + S\" achieved the highest MOTSA score (41.4), indicating good overall performance in multi-object tracking and segmentation. Therefore, considering both FN, FP, and MOTSA, \"Det + T + I + S\" demonstrates the best balance.",
        "reference": "1805.04687v2-Table9-1.png"
      },
      {
        "question": "Which category of objects has the highest total number of annotations, and is there evidence that this category might be more challenging to annotate accurately?",
        "answer": "The category with the highest total number of annotations is \"Masks,\" with 129K annotations. There is evidence that this category might be more challenging to annotate accurately because it also has the highest number of annotations in the \"Occluded\" subcategory, indicating that a large portion of these objects are partially hidden in the images.",
        "explanation": "Table 1 provides the breakdown of annotations for different categories of objects in the BDD100K MOTS dataset. The table shows the total number of annotations as well as subcategories like \"Occluded,\" which refers to objects that are partially hidden. By comparing the total number of annotations and the number of occluded annotations for each category, we can gain insight into the potential difficulty of accurately annotating each category. The \"Masks\" category stands out with both the highest total number and the highest number of occluded annotations, suggesting that annotating masks accurately might be more challenging due to frequent occlusions.",
        "reference": "1805.04687v2-Table12-1.png"
      },
      {
        "question": "What percentage of occlusions last for more than 10 frames?",
        "answer": "Approximately 80%",
        "explanation": "The figure on the right shows the cumulative distribution function (CDF) of occlusion durations. The CDF shows the percentage of occlusions that last for a certain number of frames or less. The x-axis shows the number of occluded frames, and the y-axis shows the percentage of occlusions. The CDF reaches 80% at approximately 10 frames, which means that 80% of occlusions last for more than 10 frames.",
        "reference": "1805.04687v2-Figure8-1.png"
      },
      {
        "question": "Why are MOTS datasets like KITTI MOTS and MOTS Challenge smaller in size compared to VOS datasets like YouTube VOS, even though BDD100K MOTS has a comparable number of annotations?",
        "answer": "MOTS datasets require denser annotations per frame because they involve both segmentation and tracking of multiple objects in crowded scenes. This results in smaller dataset sizes compared to VOS datasets, which typically focus on segmenting a single or fewer objects. ",
        "explanation": "The table shows that KITTI MOTS and MOTS Challenge have significantly fewer frames and sequences compared to YouTube VOS. However, their \"Ann./Fr.\" (annotations per frame) values are much higher, indicating denser annotations. BDD100K MOTS, while being larger than other MOTS datasets, maintains a high \"Ann./Fr.\" value, explaining why its size is comparable to YouTube VOS despite having fewer frames and sequences.",
        "reference": "1805.04687v2-Table3-1.png"
      },
      {
        "question": "What are the three main geographical regions where the data for this study was collected?",
        "answer": "New York, San Francisco Bay Area, and Berkeley.",
        "explanation": "The figure shows the geographical distribution of the data sources, with each dot representing the starting location of a video clip. The dots are concentrated in three main regions: New York, San Francisco Bay Area, and Berkeley.",
        "reference": "1805.04687v2-Figure2-1.png"
      },
      {
        "question": "Which category of object is the least common in the dataset?",
        "answer": "Train",
        "explanation": "The figure shows the number of instances for each category of object. The category with the lowest bar is \"Train,\" indicating that it is the least common.",
        "reference": "1805.04687v2-Figure3-1.png"
      },
      {
        "question": "How does the BDD100K dataset compare to the KITTI and MOT17 datasets in terms of size and complexity?",
        "answer": "The BDD100K dataset is significantly larger and more complex than both the KITTI and MOT17 datasets. It contains roughly 40 times more frames, 16 times more sequences, and 13 times more identities than KITTI. Compared to MOT17, BDD100K has about 10 times more frames, 80 times more sequences, and 8 times more identities. This increase in size and complexity makes BDD100K a more challenging and comprehensive benchmark for multiple object tracking algorithms. ",
        "explanation": "Table~\\ref{tab:box_tracking_label_comp} provides a direct comparison of these datasets, showing the number of frames, sequences, identities, and bounding boxes for each. By analyzing these numbers, we can see that BDD100K significantly surpasses the other two datasets in terms of scale and diversity of tracked objects.",
        "reference": "1805.04687v2-Table2-1.png"
      },
      {
        "question": "Which dataset has the most lane marking annotations?",
        "answer": "BDD100K",
        "explanation": "The table shows that BDD100K has 100,000 lane marking annotations, which is more than any other dataset in the table.",
        "reference": "1805.04687v2-Table1-1.png"
      }
    ]
  },
  "1805.08465v3": {
    "paper_id": "1805.08465v3",
    "all_figures": {
      "1805.08465v3-Table1-1.png": {
        "caption": "Table 1: Performance comparison of image steganography. In the experiment, we use SIR (dB) to quantify the distortion of both cover(C) and secret(S) images, where larger value of SIR indicates better performance. For each row, the SIR value will be highlighted if it outperforms other methods under a given parameter.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1805.08465v3-Figure2-1.png": {
        "caption": "Illustration on robustness of Reshuffled-TD on non-ideal experimental conditions. Sub-figure (a) shows the tSIR performance of the method which is influenced by Gaussian noise. In the sub-figure, group 1 corresponds the rank which satisfies the exact-recovery condition, while the rank in group 2 is not. Sub-figure (b) shows the estimation accuracy of the number of components, and the values shown above the bars are the corresponding tSIR performance of the proposed method.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1805.08465v3-Figure1-1.png": {
        "caption": "Results on synthetic data to validate our exact-recovery results. We vary different experiment parameters, such as rank r, size n and number of the components N . In each plot, the darker areas denote the worse reconstruction (tSIR ≤ 15dB) while the white areas denote a good recovery (tSIR ≥ 25dB). The gray boundary shows the phase transition, while the red line denotes the phase transition predicted by our theoretical bound derived in Corollary 1.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1805.08465v3-Figure3-1.png": {
        "caption": "Figure 3: An example to illustrate the performance of image steganography by different methods. In the figure, the first row shows the original cover images (the first column) and the container images generated by different methods; The second row shows the original secret images (the first column) and its recovery by different methods.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      }
    },
    "qa": [
      {
        "question": "Which steganography method achieves the best performance in terms of distortion for both cover and secret images when embedding 2 bits per channel?",
        "answer": "The proposed method achieves the best performance for both cover and secret images when embedding 2 bits per channel.",
        "explanation": "The table shows the Signal to Interference Ratio (SIR) for different steganography methods, with higher values indicating better performance (less distortion). When comparing the SIR values for 2 bits/chn across all datasets, the \"Ours\" column consistently shows the highest or second-highest values for both cover (C) and secret (S) images. This indicates that the proposed method offers superior performance in terms of minimizing distortion for both types of images at this embedding rate.",
        "reference": "1805.08465v3-Table1-1.png"
      }
    ]
  },
  "1809.01246v1": {
    "paper_id": "1809.01246v1",
    "all_figures": {
      "1809.01246v1-Figure5-1.png": {
        "caption": "The square hashing",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1809.01246v1-Figure6-1.png": {
        "caption": "Fig. 6. A sample of the modified version of data structure",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1809.01246v1-Figure12-1.png": {
        "caption": "True Negative Recall of Reachability Queries",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1809.01246v1-Figure13-1.png": {
        "caption": "Buffer Percentage",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1809.01246v1-Figure1-1.png": {
        "caption": "Fig. 1. A sample graph stream",
        "content_type": "figure",
        "figure_type": "** Schematic"
      },
      "1809.01246v1-Figure11-1.png": {
        "caption": "Average Relative Error of Node Queries",
        "content_type": "figure",
        "figure_type": "** Plot"
      },
      "1809.01246v1-Figure10-1.png": {
        "caption": "Average Precision of 1-hop Successor Queries",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1809.01246v1-Figure2-1.png": {
        "caption": "Fig. 2. A sample map function",
        "content_type": "figure",
        "figure_type": ""
      },
      "1809.01246v1-Figure8-1.png": {
        "caption": "Average Relative Error of Edge Queries",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1809.01246v1-TableI-1.png": {
        "caption": "TABLE I UPDATE SPEED (MIPS)",
        "content_type": "table",
        "figure_type": "table"
      },
      "1809.01246v1-Figure9-1.png": {
        "caption": "Average Precision of 1-hop Precursor Queries",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1809.01246v1-Figure4-1.png": {
        "caption": "Fig. 4. A sample of the basic version of data structure",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1809.01246v1-Figure3-1.png": {
        "caption": "Influence of M on Accuracy",
        "content_type": "figure",
        "figure_type": "plot"
      }
    },
    "qa": [
      {
        "question": "How does the square hashing process work?",
        "answer": "Square hashing is a process that uses two hash functions to map a source/destination pair to a bucket in a two-dimensional array. The first hash function, h_i(s), maps the source address to a row in the array, and the second hash function, h_i(d), maps the destination address to a column in the array. The intersection of the row and column is the bucket where the fingerprint is stored.",
        "explanation": "The figure shows a two-dimensional array of buckets. Each bucket can be either empty, occupied, or mapped. An occupied bucket contains a fingerprint, which is a hash of the source and destination addresses. A mapped bucket is a bucket that has been assigned to a source/destination pair but does not yet contain a fingerprint. An empty bucket has not been assigned to a source/destination pair. The figure also shows an example of how a fingerprint is stored in a bucket. The fingerprint <f(s), f(d)> is stored in the bucket at the intersection of row 1 and column 3.",
        "reference": "1809.01246v1-Figure5-1.png"
      },
      {
        "question": "What is the relationship between the buffer percentage and the width of the room?",
        "answer": "The buffer percentage decreases as the width of the room increases.",
        "explanation": "The figure shows that the buffer percentage decreases as the width of the room increases for all three data sets. This is because as the width of the room increases, there is more space for the data to be stored, and therefore less need for buffering.",
        "reference": "1809.01246v1-Figure13-1.png"
      },
      {
        "question": "How are edges aggregated in the graph sketch $G_h$?",
        "answer": "Edges are aggregated by adding their weights together.",
        "explanation": "The figure shows the graph stream $S$ and the corresponding graph sketch $G_h$. In the graph stream, there are two edges between nodes $a$ and $c$, with weights 1 and 4. In the graph sketch, these two edges are combined into a single edge with weight 5.",
        "reference": "1809.01246v1-Figure1-1.png"
      },
      {
        "question": "How does the Average Relative Error (ARE) of node queries change as the width increases for different configurations of GSS and TCM?",
        "answer": "The ARE of node queries generally decreases as the width increases for all configurations of GSS and TCM. However, there are some fluctuations in the ARE for some configurations.",
        "explanation": "The figure shows the ARE of node queries for different configurations of GSS and TCM on five different datasets. The x-axis of each plot represents the width, and the y-axis represents the ARE. The lines in each plot represent the different configurations of GSS and TCM. The figure shows that the ARE generally decreases as the width increases, which indicates that the accuracy of the node queries improves as the width increases.",
        "reference": "1809.01246v1-Figure11-1.png"
      },
      {
        "question": "What is the relationship between the table and the graph sketch in the figure?",
        "answer": "The table provides the mapping between the nodes in the original graph and their corresponding hash values, which are used to create the graph sketch.",
        "explanation": " The table shows the hash values (H(v)) for each node (a-g) in the original graph. These hash values are used to map the nodes to their corresponding nodes in the graph sketch. For example, nodes a and d both have a hash value of 2, so they are combined into a single node in the graph sketch.\n\n**Figure type:** Schematic",
        "reference": "1809.01246v1-Figure2-1.png"
      },
      {
        "question": "Which graph shows the largest improvement in accuracy for the TCM(8*memory) method compared to the GSS(fsize=12) method?",
        "answer": "The graph for the Caida-networkflow dataset shows the largest improvement in accuracy for the TCM(8*memory) method compared to the GSS(fsize=12) method.",
        "explanation": "The graph for the Caida-networkflow dataset shows the largest difference in the y-axis values between the two methods.",
        "reference": "1809.01246v1-Figure8-1.png"
      },
      {
        "question": "Which data structure is the fastest for updating on the email-EuAll dataset?",
        "answer": "GSS (no sampling)",
        "explanation": "The table shows the update speed of different data structures on different datasets. The update speed is measured in MIPS (millions of instructions per second). The higher the MIPS, the faster the update speed. For the email-EuAll dataset, GSS has the highest MIPS (2.2887), which means it is the fastest data structure for updating on this dataset.",
        "reference": "1809.01246v1-TableI-1.png"
      },
      {
        "question": "How does the average precision of TCM(256*memory) compare to the other two algorithms in the email-EuAll dataset?",
        "answer": "The average precision of TCM(256*memory) is lower than the other two algorithms in the email-EuAll dataset.",
        "explanation": "The figure shows the average precision of three algorithms for five different datasets. The average precision of TCM(256*memory) is shown as a blue line. In the email-EuAll dataset, the blue line is below the other two lines, indicating that the average precision of TCM(256*memory) is lower than the other two algorithms.",
        "reference": "1809.01246v1-Figure9-1.png"
      },
      {
        "question": "Which query type has the highest accuracy when M/|V| is small?",
        "answer": "Edge query.",
        "explanation": "The figure shows that the edge query has a higher correct rate than the other two query types when M/|V| is small. This can be seen by comparing the heights of the surfaces in the three plots.",
        "reference": "1809.01246v1-Figure3-1.png"
      }
    ]
  },
  "1809.03449v3": {
    "paper_id": "1809.03449v3",
    "all_figures": {
      "1809.03449v3-Figure4-1.png": {
        "caption": "With KAR, SAN, and QANet (without data augmentation) trained on the training subsets, we evaluate their performance on AddOneSent.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1809.03449v3-Figure2-1.png": {
        "caption": "With KAR, SAN, and QANet (without data augmentation) trained on the training subsets, we evaluate their performance on the development set.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1809.03449v3-Figure3-1.png": {
        "caption": "With KAR, SAN, and QANet (without data augmentation) trained on the training subsets, we evaluate their performance on AddSent.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1809.03449v3-Table3-1.png": {
        "caption": "With κ set to different values in the data enrichment method, we calculate the average number of inter-word semantic connections per word as an estimation of the amount of general knowledge, and evaluate the performance of KAR on the development set.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1809.03449v3-Table2-1.png": {
        "caption": "Model comparison based on SQuAD 1.1 and two of its adversarial sets: AddSent and AddOneSent. All the numbers are up to date as of October 18, 2018. Note that SQuAD 2.0 (Rajpurkar et al., 2018) is not involved in this paper, because it requires MRC models to deal with the problem of answer triggering, but this paper is aimed at improving the hunger for data and robustness to noise of MRC models.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1809.03449v3-Table1-1.png": {
        "caption": "Two examples about the importance of inter-word semantic connections to the reading comprehension ability of human beings: in the first one, we can find the answer because we know “facilitate” is a synonym of “help”; in the second one, we can find the answer because we know “Brooklyn” is a hyponym of “borough”.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1809.03449v3-Figure1-1.png": {
        "caption": "An end-to-end MRC model: Knowledge Aided Reader (KAR)",
        "content_type": "figure",
        "figure_type": "schematic"
      }
    },
    "qa": [
      {
        "question": "How does the average number of inter-word semantic connections per word change as the value of κ increases?",
        "answer": "The average number of inter-word semantic connections per word increases as the value of κ increases.",
        "explanation": "The table shows that the average number of inter-word semantic connections per word increases from 0.39 to 5.58 as the value of κ increases from 0 to 5. This suggests that increasing the value of κ leads to a greater number of semantic connections between words.",
        "reference": "1809.03449v3-Table3-1.png"
      },
      {
        "question": "Which model performed the best on the AddOneSent dataset?",
        "answer": "KAR",
        "explanation": "The table shows that KAR achieved the highest F1 score of 72.3 on the AddOneSent dataset.",
        "reference": "1809.03449v3-Table2-1.png"
      },
      {
        "question": "What is the role of the Knowledge Aided Similarity Matrix in the KAR model?",
        "answer": "The Knowledge Aided Similarity Matrix is used to compute the similarity between the question and passage context embeddings. This similarity score is then used to weight the passage context embeddings, giving more weight to those parts of the passage that are most relevant to the question.",
        "explanation": "The figure shows that the Knowledge Aided Similarity Matrix is used to compute the similarity between the enhanced question context embeddings and the enhanced passage context embeddings. This similarity score is then used to weight the passage context embeddings, giving more weight to those parts of the passage that are most relevant to the question.",
        "reference": "1809.03449v3-Figure1-1.png"
      },
      {
        "question": "How does the performance of KAR, SAN, and QANet (without data augmentation) change as the proportion of available training examples decreases?",
        "answer": "The performance of all three models decreases as the proportion of available training examples decreases.",
        "explanation": "The figure shows that the F1 score of all three models decreases as the proportion of available training examples decreases. This indicates that the models are less accurate when they are trained on less data.",
        "reference": "1809.03449v3-Figure4-1.png"
      },
      {
        "question": "Which model performs the best when trained on 60% of the training data?",
        "answer": "KAR",
        "explanation": "The figure shows that the F1 score for KAR is higher than the F1 score for SAN and QANet when trained on 60% of the training data.",
        "reference": "1809.03449v3-Figure2-1.png"
      },
      {
        "question": "Which of the three models, KAR, SAN, or QANet (without data augmentation), performs the best on AddSent when trained on the full training set?",
        "answer": "KAR",
        "explanation": "The figure shows the F1 score of each model on AddSent for different proportions of the training set. At 100% of the training set, KAR has the highest F1 score.",
        "reference": "1809.03449v3-Figure3-1.png"
      }
    ]
  },
  "1811.02721v3": {
    "paper_id": "1811.02721v3",
    "all_figures": {
      "1811.02721v3-Table4-1.png": {
        "caption": "Table 4: Comparison of TCP/IP links",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1811.02721v3-Table5-1.png": {
        "caption": "Table 5: Header overhead with 6LoWPAN fragmentation",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1811.02721v3-Figure9-1.png": {
        "caption": "Effect of batching on power consumption",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1811.02721v3-Figure10-1.png": {
        "caption": "Performance with injected packet loss",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1811.02721v3-Table1-1.png": {
        "caption": "Table 1: Impact of techniques to run full-scale TCP in LLNs",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1811.02721v3-Figure3-1.png": {
        "caption": "TCP goodput over one IEEE 802.15.4 hop",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1811.02721v3-Figure13-1.png": {
        "caption": "Hamilton-based ultrasonic anemometer",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1811.02721v3-Figure8-1.png": {
        "caption": "Goodput: CoAP vs. HTTP/TCP",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1811.02721v3-Figure7-1.png": {
        "caption": "Latency of web request: CoAP vs. HTTP/TCP",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1811.02721v3-Figure12-1.png": {
        "caption": "CoAP, CoCoA, and TCP with four competing flows",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1811.02721v3-Figure5-1.png": {
        "caption": "Effect of varying time between link-layer retransmissions. Reported “segment loss” is the loss rate of TCP segments, not individual IEEE 802.15.4 frames. It includes only losses not masked by link-layer retries.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1811.02721v3-Table6-1.png": {
        "caption": "Table 6: Comparison of TCPlp to existing TCP implementations used in network studies over IEEE 802.15.4 networks.7 Goodput figures obtained by reading graphs in the original paper (rather than stated numbers) are marked with the ≈ symbol.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1811.02721v3-Table2-1.png": {
        "caption": "Table 2: Comparison of the platforms we used (Hamilton and Firestorm) to TelosB and Raspberry Pi",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1811.02721v3-Table7-1.png": {
        "caption": "Table 7: Performance in the testbed over a full day, averaged over multiple trials. The ideal protocol (§8.2.2) would have a radio DC of≈ 0.63%–0.70% under similarly lossy conditions.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1811.02721v3-Figure11-1.png": {
        "caption": "Radio duty cycle of TCP and CoAP in a lossy wireless environment, in one representative trial (losses are caused by natural human activity)",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1811.02721v3-Figure6-1.png": {
        "caption": "Congestion behavior of TCP over IEEE 802.15.4",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1811.02721v3-Table8-1.png": {
        "caption": "Table 8: Memory usage of TCPlp on TinyOS. Our implementation of TCPlp spans three modules: (1) protocol implementation, (2) event scheduler that injects callbacks into userspace, and (3) userland library.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1811.02721v3-Table9-1.png": {
        "caption": "Table 9: Comparison of core features among embedded TCP stacks: uIP (Contiki), BLIP (TinyOS), GNRC (RIOT), and",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1811.02721v3-Figure1-1.png": {
        "caption": "Snapshot of uplink routes in OpenThread topology at transmission power of -8 dBm (5 hops). Node 1 is the border router with Internet connectivity.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1811.02721v3-Table3-1.png": {
        "caption": "Table 3: Memory usage of TCPlp on RIOT OS. We also include RIOT’s posix_sockets module, used by TCPlp to provide a Unix-like interface.",
        "content_type": "table",
        "figure_type": "N/A"
      }
    },
    "qa": [
      {
        "question": "How does batching affect the radio duty cycle and CPU duty cycle?",
        "answer": "Batching reduces both the radio duty cycle and CPU duty cycle.",
        "explanation": "The figure shows that the radio duty cycle and CPU duty cycle are both lower when batching is used, compared to when no batching is used. This is because batching allows the device to send and receive data in larger chunks, which reduces the number of times the radio and CPU need to be activated.",
        "reference": "1811.02721v3-Figure9-1.png"
      },
      {
        "question": "Which technique was most effective at reducing memory consumption in both send and receive buffers?",
        "answer": "The \"Resource Constraints\" challenge was addressed with two techniques: \"Zero-Copy Send\" and \"In-Place Reass.\" The first led to a 50% reduction in send buffer memory usage, while the second achieved a 38% reduction in receive buffer memory. Therefore, Zero-Copy Send was slightly more effective in reducing overall memory consumption.",
        "explanation": "The table explicitly lists the observed improvements for each technique applied to specific challenges. By comparing the percentage reductions in memory usage for both send and receive buffers, we can determine which technique had a greater overall impact on memory consumption.",
        "reference": "1811.02721v3-Table1-1.png"
      },
      {
        "question": "Based on Table 1 and the passage, why does TCP perform poorly on IEEE 802.15.4 networks compared to other network types listed? ",
        "answer": "TCP performs poorly on IEEE 802.15.4 networks because the Maximum Transmission Unit (MTU) for these networks is significantly smaller than other network types. This small MTU size results in a high percentage of overhead due to the TCP/IP headers, exceeding 50%. ",
        "explanation": "Table 1 shows that the MTU for 802.15.4 networks is only 104-116 bytes, while other networks like Fast Ethernet and Wi-Fi have an MTU of 1500 bytes. The passage explains that TCP/IP headers consume a significant portion of the available MTU in 802.15.4 frames. This high overhead percentage leads to inefficient data transmission and consequently, poor TCP performance.",
        "reference": "1811.02721v3-Table4-1.png"
      },
      {
        "question": "Why is relying on fragmentation effective for reducing header overhead?",
        "answer": "Relying on fragmentation is effective because the TCP/IP headers are only included in the first fragment, not in subsequent fragments. This significantly reduces the overhead in later fragments.",
        "explanation": "Table 1 shows the header overhead for different protocols with 6LoWPAN fragmentation. While the first frame carries the full overhead of all headers (38-107 bytes), subsequent fragments only have the overhead of 6LoWPAN and 802.15.4 headers (16-35 bytes). This reduction in overhead improves efficiency by utilizing less bandwidth for header information.",
        "reference": "1811.02721v3-Table5-1.png"
      },
      {
        "question": "How does varying the buffer size affect TCP goodput?",
        "answer": "Increasing the buffer size generally leads to increased TCP goodput, but only up to a certain point.",
        "explanation": "The figure shows that TCP goodput increases as the buffer size increases, but then plateaus. This is because a larger buffer allows for more data to be sent before the sender has to wait for an acknowledgement from the receiver. However, if the buffer is too large, it can lead to increased latency and decreased performance.",
        "reference": "1811.02721v3-Figure3-1.png"
      },
      {
        "question": "What is the function of the Hamilton-based PCB in the ultrasonic anemometer?",
        "answer": "The Hamilton-based PCB is the electronic control board of the anemometer. It houses the microcontroller, sensors, and other electronic components that are necessary for the anemometer to function.",
        "explanation": "The photograph shows the Hamilton-based PCB with its various electronic components.",
        "reference": "1811.02721v3-Figure13-1.png"
      },
      {
        "question": "What is the difference in response time between CoAP and HTTP for a response size of 50 KiB?",
        "answer": "The difference in response time between CoAP and HTTP for a response size of 50 KiB is approximately 20 seconds.",
        "explanation": "The figure shows the response time for CoAP and HTTP for different response sizes. For a response size of 50 KiB, the boxplot for CoAP shows a median response time of approximately 20 milliseconds, while the boxplot for HTTP shows a median response time of approximately 40 milliseconds.",
        "reference": "1811.02721v3-Figure8-1.png"
      },
      {
        "question": "How does the maximum link delay affect the segment loss rate and goodput in a TCP connection with one hop?",
        "answer": "As the maximum link delay increases, the segment loss rate increases and the goodput decreases.",
        "explanation": "In Figure (a), we can see that the segment loss rate increases as the maximum link delay increases. This is because the longer the delay, the more likely it is that a segment will be lost. Additionally, the goodput decreases as the maximum link delay increases. This is because the longer the delay, the less data can be transmitted in a given amount of time.",
        "reference": "1811.02721v3-Figure5-1.png"
      },
      {
        "question": "What protocol has a higher radio duty cycle in the first 7 hours of the trial?",
        "answer": "TCP",
        "explanation": "The green line, which represents TCP, is higher than the blue line, which represents CoAP, for the first 7 hours of the trial.",
        "reference": "1811.02721v3-Figure11-1.png"
      },
      {
        "question": "How does the maximum link delay affect the number of TCP timeouts and fast retransmissions?",
        "answer": "The number of TCP timeouts and fast retransmissions decreases as the maximum link delay increases.",
        "explanation": "The figure shows that the number of timeouts and fast retransmissions decreases as the maximum link delay increases. This is because, with a longer link delay, there is more time for the sender to receive an acknowledgment before retransmitting the packet.",
        "reference": "1811.02721v3-Figure6-1.png"
      },
      {
        "question": "Which module of TCPlp consumes the most memory in the active RAM on TinyOS, and how much memory does it utilize?",
        "answer": "The protocol implementation module consumes the most memory in the active RAM on TinyOS, utilizing 488 bytes.",
        "explanation": "Table 2 provides a breakdown of \\sys{}'s memory usage on TinyOS, categorized by module and memory type. Looking at the \"RAM (Active)\" row, we can compare the memory consumption of each module. The \"Protocol\" column shows the highest value of 488 bytes, indicating that the protocol implementation module uses the most active RAM compared to the event scheduler and user library modules.",
        "reference": "1811.02721v3-Table8-1.png"
      },
      {
        "question": "How does the reliability of CoAP compare to TCPlp and what potential factors contribute to this difference?",
        "answer": "Table 1 shows that CoAP has slightly higher reliability (99.5%) compared to TCPlp (99.3%). While both protocols perform well, this difference could be attributed to several factors, including:\n\nRetransmission mechanisms: CoAP employs a built-in retransmission mechanism for lost packets, while TCPlp relies on the underlying network layer for retransmissions. This could give CoAP an edge in recovering lost packets and achieving higher reliability.\nCongestion control: CoAP includes mechanisms to adapt to network congestion, potentially reducing packet loss and improving reliability.\nPacket size: CoAP typically uses smaller packets compared to TCPlp. Smaller packets are less prone to loss in wireless networks, potentially contributing to CoAP's slightly higher reliability.",
        "explanation": "The table directly provides the reliability percentages for both CoAP and \\sys{}, allowing for a direct comparison. Additionally, the information about radio duty cycle and CPU duty cycle can be used to infer possible reasons for the difference in reliability. While the table doesn't explicitly state the contributing factors, it provides clues that, when combined with knowledge about the protocols themselves, allow for a reasoned explanation.",
        "reference": "1811.02721v3-Table7-1.png"
      },
      {
        "question": "How does the memory usage of the RIOT OS posix_sockets module compare to the memory used by the protocol and socket layer combined, for both active and passive connections?",
        "answer": "The posix_sockets module consistently uses less memory than the combined usage of the protocol and socket layer. For an active connection, it requires about 5468 B compared to 19972 B + 6216 B = 26188 B for the other layers. Similarly, for a passive connection, it uses 5468 B compared to 19972 B + 6216 B = 26188 B for the other layers.",
        "explanation": "Table 1 provides the memory usage breakdown for `sys` on RIOT OS, including the individual contributions of the protocol, socket layer, and `posix_sockets` module. By comparing the values in the table, we can see that the `posix_sockets` module, while providing a Unix-like interface, consumes less memory than the combined total of the protocol and socket layer for both active and passive connections. This information supports the passage's claim that `sys` fits well within available memory despite its advanced features.",
        "reference": "1811.02721v3-Table3-1.png"
      },
      {
        "question": "Which TCP stack provides the most complete implementation of core TCP features, and which stack lacks the most features?",
        "answer": "The TCP stack presented in this paper (TCPlp) provides the most complete implementation of core TCP features, including flow control, congestion control, RTT estimation, MSS option, OOO reassembly, and various advanced features like timestamps and selective ACKs. In contrast, BLIP lacks the most features, as it does not implement congestion control, RTT estimation, or several other functionalities present in other stacks.",
        "explanation": "The table directly compares the feature sets of different embedded TCP stacks. By analyzing the \"Yes\" and \"No\" entries, we can identify which functionalities are present or absent in each stack. The passage also provides additional information, highlighting the specific limitations of uIP and BLIP. Combining the table and passage allows us to comprehensively assess the completeness of each stack's implementation and identify the best and worst options in terms of feature richness.",
        "reference": "1811.02721v3-Table9-1.png"
      },
      {
        "question": "How many hops are there between the Hamilton and the Internet?",
        "answer": "5 hops",
        "explanation": "The caption states that the figure shows a snapshot of uplink routes in an OpenThread topology at a transmission power of -8 dBm (5 hops). This means that there are 5 hops between the Hamilton and the Internet.",
        "reference": "1811.02721v3-Figure1-1.png"
      }
    ]
  },
  "1811.08481v2": {
    "paper_id": "1811.08481v2",
    "all_figures": {
      "1811.08481v2-Figure1-1.png": {
        "caption": "UnCoRd generalizes without QA training to novel properties and relations (top), and to real-world domain (bottom).",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1811.08481v2-Figure5-1.png": {
        "caption": "Generated graphs for a free form question (from the VQA [8] dataset). Blue text: accurate concepts, red: inaccurate.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1811.08481v2-Table4-1.png": {
        "caption": "Accuracy of graph representation for VQA [8] sample, given for the different UnCoRd mappers. As expected, training on more diverse data allows better generalization across domains.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1811.08481v2-Table5-1.png": {
        "caption": "Accuracy of CLEVR dataset question answering by UnCoRd using the different question-to-graph mappers",
        "content_type": "table",
        "figure_type": "table"
      },
      "1811.08481v2-Figure6-1.png": {
        "caption": "Examples for answering different question types on CLEVR images: (a) taken from CLEVR, (b) includes ’different color’ relation, (c) uses a quantifier, and (d) a simple property existence (+ ’all’ quantifier) question.",
        "content_type": "figure",
        "figure_type": "** photograph(s)"
      },
      "1811.08481v2-Figure2-1.png": {
        "caption": "A schematic illustration of our method. The first stage (1) maps the question into a graph representation using a sequence-tosequence LSTM based model. At the second stage (2), the recursive answering procedure follows the graph, searching for a valid assignment in the image. At each step, the handled node is set and objects (extracted using mask R-CNN) are examined according to the node’s requirements (utilizing corresponding visual estimators). If succeeded, a new node is set (according to a DFS traversal) and the function is called again to handle the unassigned subgraph. The Example illustrates the flow: ’check node (a)’ → ’relation success’ → ’check node (b)’ → answer.",
        "content_type": "figure",
        "figure_type": "** Schematic"
      },
      "1811.08481v2-Table7-1.png": {
        "caption": "Answering accuracy for 100 questions outside the VQA v2 domain (including quantifiers, comparisons, multiple relation chains and multiple relations and properties) on COCO images.",
        "content_type": "table",
        "figure_type": "table"
      },
      "1811.08481v2-Figure8-1.png": {
        "caption": "Examples of UnCoRd successes in answering questions outside the VQA v2 domain on COCO images.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1811.08481v2-Table8-1.png": {
        "caption": "Answering accuracy for 100 questions sampled from VQA v2 dataset (on terms with visual estimators in UnCoRd).",
        "content_type": "table",
        "figure_type": "table"
      },
      "1811.08481v2-Figure9-1.png": {
        "caption": "Examples of UnCoRd failures in answering questions outside the VQA v2 domain on COCO images.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1811.08481v2-Figure10-1.png": {
        "caption": "Examples of UnCoRd answers to VQA v2 questions (including ’full’ answers when they add information).",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1811.08481v2-Figure3-1.png": {
        "caption": "Left: A CLEVR question and a corresponding graph. Right: A modified question and a corresponding graph, mapped using Extended-Enhanced model. The accuracy of the modified representation is confirmed, as it matches the original accurate graph (with modified graph concepts).",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1811.08481v2-Figure4-1.png": {
        "caption": "Instance segmentation example for CLEVR data. Left: GT (approximated from scene data), Right: results.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1811.08481v2-Table3-1.png": {
        "caption": "Accuracy of question-to-graph mapping for all data types",
        "content_type": "table",
        "figure_type": "table"
      },
      "1811.08481v2-Table1-1.png": {
        "caption": "CLEVR estimators results on CLEVR validation set",
        "content_type": "table",
        "figure_type": "table"
      },
      "1811.08481v2-Table2-1.png": {
        "caption": "CLEVR QA accuracy for state-of-the-art methods",
        "content_type": "table",
        "figure_type": "Table."
      }
    },
    "qa": [
      {
        "question": "Which method has the highest overall accuracy for answering questions about images on the 100 questions sampled from VQA v2 dataset?",
        "answer": "UnCoRd-VG-E",
        "explanation": "The table shows the accuracy of two methods for answering questions about images. The overall accuracy of UnCoRd-VG-E is 92.0, which is higher than the overall accuracy of Pythia (77.0).",
        "reference": "1811.08481v2-Table8-1.png"
      },
      {
        "question": "Explain how the answering procedure works.",
        "answer": "The answering procedure follows the question graph, searching for a valid assignment in the image. At each step, the handled node is set and objects (extracted using Mask R-CNN) are examined according to the node’s requirements (utilizing corresponding visual estimators). If successful, a new node is set (according to a DFS traversal) and the function is called again to handle the unassigned subgraph.",
        "explanation": " The figure shows the answering procedure as a flowchart. The process starts with the question graph, which is created by the Question to Graph Mapper. The answering procedure then follows the graph, checking each node against the objects in the image. If a node is successful, the procedure moves on to the next node. If a node is not successful, the procedure backtracks and tries a different object. The process continues until all nodes in the graph have been checked. The final answer is then returned.",
        "reference": "1811.08481v2-Figure2-1.png"
      },
      {
        "question": "Which method has the highest overall accuracy?",
        "answer": "UnCoRd-VG-E",
        "explanation": "The table shows the overall accuracy for each method. UnCoRd-VG-E has the highest overall accuracy of 76.0.",
        "reference": "1811.08481v2-Table7-1.png"
      },
      {
        "question": "What is the effect of training on more diverse data on the accuracy of graph representation for VQA?",
        "answer": "Training on more diverse data improves the accuracy of graph representation for VQA.",
        "explanation": "The table shows that the accuracy of the graph representation increases as the amount of training data increases. The accuracy is highest for the VG mapper, which was trained on the most diverse data.",
        "reference": "1811.08481v2-Table4-1.png"
      },
      {
        "question": "Which estimator achieves the highest accuracy on the CLEVR validation set?",
        "answer": "Size estimator.",
        "explanation": "The table shows that the Size estimator achieves 100% accuracy, which is the highest among all the estimators.",
        "reference": "1811.08481v2-Table1-1.png"
      },
      {
        "question": "Which method achieves the highest overall accuracy on the validation set?",
        "answer": "UnCoRd-None-B.",
        "explanation": "The table shows the overall accuracy of different methods on the validation set. UnCoRd-None-B has the highest accuracy of 99.8%.",
        "reference": "1811.08481v2-Table2-1.png"
      }
    ]
  },
  "1812.00281v3": {
    "paper_id": "1812.00281v3",
    "all_figures": {
      "1812.00281v3-Figure5-1.png": {
        "caption": "We reconstruct the body occupancy map and its outer surface using shape-from-silhouette and associate the point cloud with body semantics (head, body, arms, and legs).",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1812.00281v3-Figure6-1.png": {
        "caption": "Distribution of head pose, gaze and eye pose in normalized space for MPII-Gaze, UT-Multiview, RT-GENE and HUMBI. Horizontal and vertical axis represent yaw and pitch angle respectively (unit: degree).",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1812.00281v3-Table2-1.png": {
        "caption": "Table 2: Bias and variance analysis of the distribution of head pose, gaze and eye pose (unit: degree, smallest bias and largest variance in bold, second with underline).",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1812.00281v3-Figure15-1.png": {
        "caption": "The qualitative results of the monocular 3D body prediction network trained with different dataset combination. The top and bottom show the results tested on UP-3D and HUMBI Body respectively.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1812.00281v3-Figure16-1.png": {
        "caption": "Garment silhouette error.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1812.00281v3-Figure2-1.png": {
        "caption": "We present HUMBI that pushes towards two extremes: views and subjects. The view-specific appearance measured by 107 HD cameras regarding five elementary body expressions for 772 distinctive subjects.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1812.00281v3-Table4-1.png": {
        "caption": "Table 4: The mean error of 3D face mesh prediction for cross-data evaluation (unit: pixel).",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1812.00281v3-Table3-1.png": {
        "caption": "Table 3: The mean error of 3D gaze prediction for the cross-data evaluation (unit: degree).",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1812.00281v3-Figure12-1.png": {
        "caption": "The training setup for 3D mesh prediction from a single image.",
        "content_type": "figure",
        "figure_type": "** Schematic"
      },
      "1812.00281v3-Table1-1.png": {
        "caption": "Table 1: Human body expression datasets.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1812.00281v3-Figure11-1.png": {
        "caption": "HUMBI body and cloth reconstruction results.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1812.00281v3-Figure10-1.png": {
        "caption": "Face reconstruction (Section B.3). (Top) Recovered 3D faces with various expressions (Bottom left) Alignment between projected mesh and subject’s face. (Bottom right) Estimated illumination condition.",
        "content_type": "figure",
        "figure_type": "** photograph(s)"
      },
      "1812.00281v3-Table7-1.png": {
        "caption": "Table 7: Cross-data evaluation results of 3D body keypoint prediction. Metric is AUC of PCK calculated over an error range of 0-150 mm.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1812.00281v3-Table8-1.png": {
        "caption": "Table 8: The mean error of 3D body mesh prediction for cross-data evaluation (unit: pixel).",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1812.00281v3-Figure8-1.png": {
        "caption": "We conduct camera-ablation study to evaluate the accuracy of the garment reconstruction in terms of the density (left) and the accuracy (right).",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1812.00281v3-Table6-1.png": {
        "caption": "Table 6: The mean error of 3D hand mesh prediction for cross-data evaluation (unit: pixel).",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1812.00281v3-Figure3-1.png": {
        "caption": "(Top and bottom) HUMBI includes 772 distinctive subjects across gender, ethnicity, age, clothing style, and physical condition, which generates diverse appearance of human expressions. (Middle) For each subject, 107 HD cameras capture her/his expressions including gaze, face, hand, body, and garment.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1812.00281v3-Figure9-1.png": {
        "caption": "Gaze signals computed by our system (Sec. B.2). (Left) 3D demonstration of captured gaze placed on the black dotted body joints. Black arrow is gaze direction. Red, green and blue segment are x, y and z-axis of gaze frame. Brown segment is the center axis of the head cylinder. (Right) Gaze overlaid on a color image. Orange arrow is gaze direction. Dark blue box indicates eye region. Blue box wraps face. Yellow area is projection of the cylinder.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1812.00281v3-Figure4-1.png": {
        "caption": "View-specific appearance rendered from multiview images with median appearance and variance for (a) gaze, (b) face, (c) hand, (d) body.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      }
    },
    "qa": [
      {
        "question": "What is the relationship between the camera yaw angle and the silhouette distance?",
        "answer": "The silhouette distance generally increases as the camera yaw angle increases.",
        "explanation": "The figure shows that the silhouette distance is higher for larger camera yaw angles, indicating that the silhouette is more distorted when the camera is at a greater angle to the person.",
        "reference": "1812.00281v3-Figure16-1.png"
      },
      {
        "question": "How does HUMBI compare to other datasets in terms of the number of subjects?",
        "answer": "HUMBI has the highest number of subjects compared to the other datasets.",
        "explanation": "The figure shows a scatter plot of the number of views vs. the number of subjects for several datasets. HUMBI is located in the upper right corner of the plot, indicating that it has the highest number of views and subjects.",
        "reference": "1812.00281v3-Figure2-1.png"
      },
      {
        "question": "Which dataset has the most concentrated distribution of gaze and head pose?",
        "answer": "MPII-Gaze",
        "explanation": "The figure shows the distribution of gaze, head pose, and eye pose for four different datasets. The MPII-Gaze dataset has the most concentrated distribution of gaze and head pose, as the heatmap is more concentrated around the center.",
        "reference": "1812.00281v3-Figure6-1.png"
      },
      {
        "question": "What are the differences between the results of the monocular 3D body prediction network trained with different dataset combinations?",
        "answer": "The results of the monocular 3D body prediction network trained with different dataset combinations show that the Up3d+HUMBI dataset combination produces the most accurate results. This is evident in the images where the predicted 3D body poses are closer to the ground-truth poses than the other dataset combinations.",
        "explanation": "The figure shows the results of the monocular 3D body prediction network trained with different dataset combinations. The first column shows the input images, the second column shows the ground-truth 3D body poses, and the third, fourth, and fifth columns show the predicted 3D body poses for the Up3d, HUMBI, and Up3d+HUMBI dataset combinations, respectively.",
        "reference": "1812.00281v3-Figure15-1.png"
      },
      {
        "question": "What are the different stages of HUMBI body and cloth reconstruction?",
        "answer": "The different stages of HUMBI body and cloth reconstruction are: \n1. Input image of the person (Ibody)\n2. Keypoint estimation (Kbody)\n3. Occupancy map generation (Obody)\n4. Body model fitting (Mbody)\n5. Cloth model fitting (Mcloth)",
        "explanation": "The figure shows the different stages of HUMBI body and cloth reconstruction. The input image is first used to estimate the keypoints of the body. These keypoints are then used to generate an occupancy map, which is a 3D representation of the body. The body model is then fitted to the occupancy map, and finally, the cloth model is fitted to the body model.",
        "reference": "1812.00281v3-Figure11-1.png"
      },
      {
        "question": "Which dataset performs best when used alone for training a 3D body keypoint prediction model, and how does its performance compare to models trained on combined datasets?",
        "answer": "HUMBI performs best when used alone for training, with an average AUC of 0.399. While this is lower than the average AUC of models trained on combined datasets (0.433 for H36M+HUMBI and 0.413 for MI3D+HUMBI), HUMBI still achieves the highest score among the individual datasets.",
        "explanation": "The table shows the cross-data evaluation results for 3D body keypoint prediction. The diagonal cells represent the performance of models trained and tested on the same dataset. By comparing the average AUC values in the last row, we can see that HUMBI outperforms H36M and MI3D when used alone. However, combining HUMBI with either H36M or MI3D further improves the performance, suggesting that these datasets offer complementary information for training.",
        "reference": "1812.00281v3-Table7-1.png"
      },
      {
        "question": "Which training data configuration resulted in the lowest prediction error for both UP-3D and HUMBI test sets?",
        "answer": "Training with UP-3D + HUMBI resulted in the lowest prediction error for both UP-3D and HUMBI test sets.",
        "explanation": "Table 1 shows the mean error of 3D body mesh prediction for different combinations of training and testing data. For both UP-3D and HUMBI test sets, the lowest error values (**18.4±13.8** and **12.5±8.4** respectively) are observed when the model is trained with **UP-3D + HUMBI**. This suggests that combining both datasets during training leads to better generalization and lower prediction errors compared to using either dataset alone.",
        "reference": "1812.00281v3-Table8-1.png"
      },
      {
        "question": "How does the number of cameras used affect the accuracy of the garment reconstruction?",
        "answer": "The accuracy of the garment reconstruction increases as the number of cameras used increases.",
        "explanation": "The plot on the right shows that the error in the garment reconstruction decreases as the number of cameras used increases. This is because more cameras provide more viewpoints of the garment, which allows the algorithm to better reconstruct the 3D shape of the garment.",
        "reference": "1812.00281v3-Figure8-1.png"
      },
      {
        "question": "How does HUMBI capture diverse appearance of human expressions?",
        "answer": "HUMBI includes 772 distinctive subjects across gender, ethnicity, age, clothing style, and physical condition, which generates diverse appearance of human expressions.",
        "explanation": "The figure shows that HUMBI includes people of different genders, ethnicities, ages, clothing styles, and physical conditions. This diversity helps to capture a wide range of human expressions.",
        "reference": "1812.00281v3-Figure3-1.png"
      },
      {
        "question": "What is the difference between the \"median appearance\" and the \"view-specific appearance\"?",
        "answer": "The median appearance is the average of all the multiview images, while the view-specific appearance is a single image that is rendered from a specific viewpoint.",
        "explanation": "The figure shows that the median appearance is a single image that represents the average of all the multiview images. The view-specific appearance is a single image that is rendered from a specific viewpoint. The variance image shows how much the different multiview images vary from the median appearance.",
        "reference": "1812.00281v3-Figure4-1.png"
      },
      {
        "question": "What is the purpose of the decoder in the 3D mesh prediction pipeline?",
        "answer": "The decoder is responsible for generating the final 3D mesh from the intermediate representations produced by the regression network.",
        "explanation": "The figure shows that the decoder takes the outputs of the regression network (Output 1 and Output 2) as input and produces the final 3D mesh. This suggests that the decoder performs some sort of post-processing or refinement on the intermediate representations to generate the final output.",
        "reference": "1812.00281v3-Figure12-1.png"
      },
      {
        "question": "Which dataset provides data for **both** facial expressions and full-body motion capture, including clothing, in a natural setting (not synthesized)? ",
        "answer": "HUMBI is the only dataset that provides data for both facial expressions and full-body motion capture, including clothing, in a natural setting. ",
        "explanation": "Table 1 summarizes various human body expression datasets and their features. By looking at the columns for \"Face,\" \"Body,\" and \"Cloth,\" we can identify datasets containing relevant data. However, we also need to consider the \"Measurement method\" and the information in the caption to determine if the data is captured in a natural setting or synthesized. \n\n- Several datasets provide facial expression data (e.g., CMU Multi-PIE, 3DMM), but they lack full-body motion capture. \n- Datasets like CMU Mocap and Human 3.6M offer body motion capture but lack facial expression data. \n- While datasets like INRIA and BUFF include both face and body data in a natural setting, they lack clothing information. \n- HUMBI is the only dataset that has checkmarks for \"Face,\" \"Body,\" and \"Cloth\" columns and explicitly mentions \"natural\" in the \"Cloth\" column, indicating that it captures all these aspects in a real-world setting.",
        "reference": "1812.00281v3-Table1-1.png"
      }
    ]
  },
  "1901.00398v2": {
    "paper_id": "1901.00398v2",
    "all_figures": {
      "1901.00398v2-Figure2-1.png": {
        "caption": "Figure 2: Accuracy of human evaluators on individual reviews: H1 - individual votes; H2 - majority votes.",
        "content_type": "figure",
        "figure_type": "** plot"
      },
      "1901.00398v2-Figure8-1.png": {
        "caption": "Figure 8: Screenshot of the instructions presented to Amazon Mechanical Turk workers.",
        "content_type": "figure",
        "figure_type": "\"other\""
      },
      "1901.00398v2-Figure9-1.png": {
        "caption": "Figure 9: Screenshot of the Amazon Mechanical Turk user study interface.",
        "content_type": "figure",
        "figure_type": "photograph(s)"
      },
      "1901.00398v2-Table4-1.png": {
        "caption": "Table 4: Accuracy of deep (LSTM, CNN, CNN & LSTM) and shallow (SVM, RF, NB, XGBoost) meta-adversarial evaluators. The lower the better. Meta-adversarial evaluators do better than humans on individual reviews, with less bias between the two classes. GAN-based generators are considered best by meta-adversarial evaluators.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1901.00398v2-Figure10-1.png": {
        "caption": "Figure 10: Text-Overlap Evaluators (BLEU, ROUGE, METEOR and CIDEr) scores for individual generators. The higher the better. The rankings are overall similar, as GAN-based generators are ranked low.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1901.00398v2-Figure4-1.png": {
        "caption": "Figure 4: Text-Overlap Evaluators (BLEU and CIDEr) scores for individual generators. The higher the better. The rankings are overall similar, as GAN-based generators are ranked low.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1901.00398v2-Figure5-1.png": {
        "caption": "Kendall τ -b between human and automated evaluators. Human’s ranking is positively correlated to text-overlap evaluators and negatively correlated to adversarial evaluators (∗ is p ≤ 0.05).",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1901.00398v2-Figure3-1.png": {
        "caption": "Accuracy of human (H1, H2) and metaadversarial evaluators (LSTM, SVM) on reviews generated by individual generators. The lower the accuracy, the better the generator.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1901.00398v2-Table3-1.png": {
        "caption": "Table 3: Accuracy of deep (LSTM) and shallow (SVM) meta-adversarial evaluators. The lower the better. Meta-adversarial evaluators do better than humans on individual reviews, with less bias between the two classes. GAN-based generators are considered to be the best by meta-adversarial evaluators.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1901.00398v2-Table9-1.png": {
        "caption": "Table 9: BLEU results when evaluating the generated reviews using G-train as the reference corpus (a lower score indicates less n-grams in common between the training set G-train and the generated text). GAN models present low similarity with the training set.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1901.00398v2-Figure6-1.png": {
        "caption": "Kendall τ -b correlation coefficients between human evaluators and automated evaluators, tested on the annotated subset of D-test with majority votes as ground-truth (∗ denotes p ≤ 0.05).",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1901.00398v2-Figure1-1.png": {
        "caption": "Figure 1: Overview of the Experiment Procedure.",
        "content_type": "figure",
        "figure_type": "schematic"
      },
      "1901.00398v2-Table1-1.png": {
        "caption": "Table 1: Candidate models for review generation.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1901.00398v2-Figure12-1.png": {
        "caption": "Figure 12: Kendall τ -b correlation coefficients between BLEU G-train and Self-BLEU rankings, and the three evaluation methods - human evaluators H1, H2, discriminative evaluators and word-overlap based evaluators (* denotes p ≤ 0.05). Meta-discriminators have been trained on D-train, D-valid sets and tested on the annotated D-test set with ground-truth test labels.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1901.00398v2-Figure7-1.png": {
        "caption": "Figure 7: Self-BLEU scores (the lower the more diverse) and lexical diversity scores (the higher the more diverse) are highly correlated in ranking the generators.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1901.00398v2-Table7-1.png": {
        "caption": "Table 7: Kendall tau-b, Spearman and Pearson correlation coefficients between Self-BLEU diversity rankings and the three evaluation methods - human evaluators H1, H2, discriminative evaluators and word-overlap based evaluators (* denotes statistical significant result with p ≤ 0.05). Meta-discriminators have been trained on D-train, D-valid sets and tested on the annotated D-test set with ground-truth test labels.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1901.00398v2-Table5-1.png": {
        "caption": "Table 5: Kendall tau-b, Spearman and Pearson correlation coefficients between human evaluators H1, H2, and discriminative evaluators and word-overlap evaluators (* denotes statistical significant result with p ≤ 0.05).",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1901.00398v2-Table8-1.png": {
        "caption": "Table 8: Kendall tau-b, Spearman and Pearson correlation coefficients between BLEU G-train rankings and the three evaluation methods - human evaluators H1, H2, discriminative evaluators and word-overlap based evaluators (* denotes statistical significant result with p ≤ 0.05). Meta-discriminators have been trained on D-train, D-valid sets and tested on the annotated Dtest set with ground-truth test labels.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1901.00398v2-Table6-1.png": {
        "caption": "Table 6: Self-BLEU diversity scores per generator (the lower the more diverse), and lexical diversity scores (the higher the more diverse). There is high correlation between the two metrics with respect to the rankings of the generative text models.",
        "content_type": "table",
        "figure_type": "N/A"
      },
      "1901.00398v2-Figure11-1.png": {
        "caption": "Accuracy of deep (LSTM) and shallow (SVM) meta-discriminators when tested on the annotated subset of D-test, with majority votes as groundtruth. The lower the better.",
        "content_type": "figure",
        "figure_type": "plot"
      },
      "1901.00398v2-Table2-1.png": {
        "caption": "Table 2: Number of generated reviews by each model.",
        "content_type": "table",
        "figure_type": "N/A"
      }
    },
    "qa": [
      {
        "question": " Which type of review was more accurately identified by the human evaluators, human-written or machine-generated? ",
        "answer": "The human evaluators were more accurate at identifying human-written reviews than machine-generated reviews.",
        "explanation": " The figure shows that the accuracy of the human evaluators was higher for human-written reviews than for machine-generated reviews. For example, the accuracy of H1 for human-written reviews predicted as human-written was 80%, while the accuracy for machine-generated reviews predicted as human-written was only 40%.",
        "reference": "1901.00398v2-Figure2-1.png"
      },
      {
        "question": "Which type of generator generally produced reviews that were most easily identified as machine-generated by the meta-adversarial evaluators?",
        "answer": "MLE SeqGAN and Word LSTM with temperature 1.0.",
        "explanation": "The table shows the accuracy of meta-adversarial evaluators in identifying machine-generated reviews. Lower accuracy indicates that the reviews are more difficult to distinguish from human-written ones. We can see that MLE SeqGAN and Word LSTM with temperature 1.0 consistently achieve the lowest accuracy across most evaluators, implying that these generators produced reviews that were most similar to human-written ones and thus hardest to detect as machine-generated.",
        "reference": "1901.00398v2-Table4-1.png"
      },
      {
        "question": "What is the task that the AMT workers are being asked to do?",
        "answer": "The AMT workers are being asked to decide whether each of twenty one paragraphs extracted from product reviews is real (written by a person) or fake (written by a computer algorithm).",
        "explanation": "The instructions in Figure \\ref{user_instructions} state that the workers will be presented with twenty one paragraphs extracted from product reviews, and that they should try to decide whether each review is real or fake.",
        "reference": "1901.00398v2-Figure8-1.png"
      }
    ]
  }
}