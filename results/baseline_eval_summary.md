# Baseline Evaluation Summary (No Answer Condition)

## Task

Given a student's answer to a question about a scientific figure, can an LLM detect whether the answer is **Correct**, **Partially Correct**, or **Incorrect** — and categorize the error type?

---

## Setup

| Parameter | Value |
|-----------|-------|
| Model | `Qwen/Qwen3-VL-8B-Instruct` (Together.ai) |
| Dataset | 174 augmented QA pairs from SPIQA test-A |
| Eval sample | 50 examples (stratified by verdict × error category) |
| Seed | 42 (same examples across all scenarios) |

### Sample Distribution (50 examples)

| Verdict | Count |
|---------|------:|
| Correct | 9 |
| Partially Correct | 24 |
| Incorrect | 17 |

---

## Scenarios Tested

| Scenario | Caption | Image |
|----------|:-------:|:-----:|
| text_only | ✗ | ✗ |
| caption_only | ✓ | ✗ |
| vision_only | ✗ | ✓ |
| multimodal | ✓ | ✓ |

---

## Results: Verdict Accuracy (No Answer Condition)

| Scenario | Accuracy |
|----------|----------|
| text_only_no_answer | **56%** |
| vision_only_no_answer | 50% |
| caption_only_no_answer | 48% |
| multimodal_no_answer | 48% |

---

## Key Findings

### 1. Text-only wins on verdict accuracy
Without the reference answer, text-only (56%) outperforms all other scenarios. The model relies on linguistic patterns rather than visual grounding.

### 2. Vision doesn't help at 8B scale
Adding the image doesn't improve accuracy — it slightly hurts. The model gets distracted by visual input it can't effectively process.

### 3. Vision-only beats caption-only (50% vs 48%)
When forced to reason without the reference answer, seeing the actual image is marginally better than reading a text description. This gap would likely widen with a stronger model.

### 4. Partially Correct is the hard class
The model tends to collapse Partially Correct into Incorrect. The 3-way distinction is substantially harder than binary classification.

---

## Limitations

| Limitation | Impact |
|------------|--------|
| Synthetic ground truth | Student answers generated by GPT-4.1, not humans |
| Small n (50 examples) | ~7pp confidence intervals; <5pp differences are noise (see below) |
| Weak vision model (8B) | Can't reliably extract fine-grained visual details |

### Statistical Note: Interpreting Small Sample Size

With only 50 examples, there's inherent uncertainty in accuracy measurements:

- **pp = percentage points** (the unit for comparing percentages directly)
- **~7pp confidence interval** means if you measure 50% accuracy, the true value could be ~43–57%
- **<5pp differences are noise** means differences smaller than 5pp aren't statistically meaningful

**Which comparisons are reliable?**

| Comparison | Gap | Interpretation |
|------------|-----|----------------|
| text_only (56%) vs multimodal (48%) | 8pp | Likely real difference |
| text_only (56%) vs caption_only (48%) | 8pp | Likely real difference |
| vision_only (50%) vs caption_only (48%) | 2pp | Noise — not meaningful |
| vision_only (50%) vs multimodal (48%) | 2pp | Noise — not meaningful |

To detect smaller effects reliably, you'd need a larger sample size.

---

## Suggested Next Steps

### High Impact
1. **Stronger vision model (72B+)** — Qwen2.5-VL-72B or GPT-4o
2. **Filter to Factual errors** — where visual grounding matters most
3. **No-answer as primary condition** — more realistic coaching scenario

### Methodological
4. **Chain-of-thought visual grounding** — force model to describe figure first
5. **Image-grounded ICL exemplars** — pass actual images in few-shot examples
6. **Human annotation subset** — validate synthetic labels

---

## Files

| Path | Description |
|------|-------------|
| `data/eval/*_no_answer_results.json` | Raw eval results (50 each) |
| `baseline_findings/FINDINGS.md` | Full analysis |
| `baseline_findings/README.md` | How to run eval scripts |
| `src/eval_*.py` | Evaluation scripts |
| `prompts/incongruence_eval_v3.txt` | System prompt with ICL exemplars |
