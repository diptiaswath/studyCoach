# studyCoach
CMU MultiModal ML Course Project

Contributors: [Budhika, Jiashan, Khubi, Dipti]

## Overview

StudyCoach generates **synthetic student responses** for the SPIQA (Scientific Paper Image Question Answering) dataset using in-context learning. The system creates realistic student errors on figure/table questions, with structured error classification for evaluation research.

### Error Classification System

Three core error types guide all parts of the pipeline:

| Error Type | Definition | Example |
|-----------|-----------|---------|
| **Omission** | Missing key details in the student answer | "The methods are similar" (omits metric comparison details) |
| **Factual** | Misreading visual elements | Misinterpreting axes, legends, or data values in a chart |
| **Conceptual** | Wrong conclusion from correct data | Confusing lower loss with worse performance (inverted logic) |

These error types are:
- **Specified by users** when curating exemplars in seed.py (Step 4)
- **Generated by LLM** in icl.py to create realistic student mistakes (Step 6)
- **Tracked globally** to balance distribution across the dataset

---

## 4-Tuple Structure

Every generated example follows a standardized **4-tuple** format matching the StudyCoach report specification:

| Field | Source | Content | Used By |
|-------|--------|---------|---------|
| **Context** | SPIQA | Visual + textual context (figure + caption) | Model receives as visual input |
| **Question** | SPIQA | The figure-grounded question to answer | Model receives as text input |
| **User Answer** | Generated (synthetic) | A student's explanation (correct/partial/incorrect) | icl.py generates using exemplars |
| **Model Response** | Generated (synthetic) | Verdict + error category + coaching feedback | icl.py produces structured output |

**Structured Model Response Breakdown:**

| Component | Values | Purpose | How It's Generated |
|-----------|--------|---------|-------------------|
| **Verdict** | Correct / Partially Correct / Incorrect | Classifies the student's answer | LLM determines if answer is right/wrong |
| **Error Category** | Omission / Factual / Conceptual | Identifies *why* the answer is wrong | LLM selects from 3 error types (guided by exemplars) |
| **Feedback** | Free-text (2-4 sentences) | Study coach explanation | LLM generates constructive, concise guidance |

---

## Core Scripts

### seed.py - Exemplar Generation

**Purpose:** Generate high-quality exemplar examples that teach icl.py how to generate realistic student errors across the 3 error types.

**Inputs (8 Parameters)**

| # | Parameter | Type | Source |
|---|-----------|------|--------|
| 1 | `image_path` | str | SPIQA image file path |
| 2 | `caption` | str | SPIQA figure caption |
| 3 | `question` | str | SPIQA question text |
| 4 | `answer` | str | SPIQA ground truth answer |
| 5 | `verdict` | str | 'incorrect' or 'partially correct' |
| 6 | `error_category` | str | 'omission', 'factual', or 'conceptual' |
| 7 | `verdict_explanation` | str | Definition of chosen verdict |
| 8 | `error_category_explanation` | str | Definition of chosen error type |

**Outputs (4 Fields)**

| Field | 4-Tuple Role |
|-------|-------------|
| `student_answer` | User Answer |
| `verdict` | Model Response (Verdict) |
| `error_category` | Model Response (Error Category) |
| `feedback` | Model Response (Feedback) |

**Exemplar Distribution**

Target 18 exemplars covering all combinations:

| Figure Type | Error Categories (×3) | Verdicts (×2) |
|---|---|---|
| plot | Omission, Factual, Conceptual | Incorrect, Partially Correct |
| figure | Omission, Factual, Conceptual | Incorrect, Partially Correct |
| table | Omission, Factual, Conceptual | Incorrect, Partially Correct |

**Result: 3 types × 3 categories × 2 verdicts = 18 exemplars**

---

### icl.py - Inference Engine

**Purpose:** Generate synthetic student responses at scale using exemplars from seed.py. Takes curated exemplar set (with all 3 error types represented) and uses them to teach the LLM how to generate realistic student errors across the entire SPIQA Test-A dataset.

**Inputs (4 Parameters)**

| # | Parameter | Type | Source |
|---|-----------|------|--------|
| 1 | `json_path` | str/Path | SPIQA Test-A JSON file (provides context + questions) |
| 2 | `images_root` | str/Path | Directory containing paper image folders (provides visual context) |
| 3 | `exemplars` | dict | seed.py output: `{'plot': [...], 'figure': [...], 'table': [...]}` |
| 4 | `output_path` | str/Path | Where to write the updated JSON with inference results |

**Exemplar Input Format**

Each exemplar tuple teaches one error type:

```python
exemplars = {
    'plot': [
        (user_prompt_omission, assistant_response_omission, image_path_1),
        (user_prompt_factual, assistant_response_factual, image_path_2),
        (user_prompt_conceptual, assistant_response_conceptual, image_path_3)
    ],
    'figure': [(...), (...), (...)],  # Same 3 error types
    'table': [(...), (...), (...)]    # Same 3 error types
}
```

**Outputs (5 Fields Added Per QA)**

| Field | 4-Tuple Role | Description |
|-------|-------------|-------------|
| `student` | User Answer | Synthetic wrong student answer |
| `verdict` | Model Response (Verdict) | Correct / Partially Correct / Incorrect |
| `error_category` | Model Response (Error Category) | Omission / Factual / Conceptual / N/A |
| `feedback` | Model Response (Feedback) | Study coach explanation (2–4 sentences) |
| `correct_answer` | Context (supplementary) | Ground truth for validation |

**Example Output**

```json
{
  "question": "How does the C-Tarone method compare to the binarization method?",
  "answer": "The C-Tarone method has higher precision and F-measure...",
  "reference": "1702.08694v3-Figure3-1.png",
  "student": "The C-Tarone method is generally similar to the binarization method across all datasets.",
  "verdict": "Incorrect",
  "error_category": "Omission",
  "feedback": "Your answer omits key details regarding the differences between the two methods. According to the figure, C-Tarone consistently has higher precision and F-measure than binarization.",
  "correct_answer": "The C-Tarone method has higher precision and F-measure..."
}
```

**Usage**

```bash
python src/icl.py data/SPIQA_testA_part1.json data/test-A/SPIQA_testA_Images \
  --output data/SPIQA_testA_part1_output.json
```