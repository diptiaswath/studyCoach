# studyCoach
CMU MultiModal ML Course Project

Contributors: [Budhika, Jiashan, Khubi, Dipti]

## Overview

StudyCoach generates **synthetic student responses** for the SPIQA (Scientific Paper Image Question Answering) dataset using in-context learning. The system creates realistic student errors on figure/table questions, with structured error classification for evaluation research.

### Error Classification System

Three core error types guide all parts of the pipeline:

| Error Type | Definition | Example |
|-----------|-----------|---------|
| **Omission** | Missing key details in the student answer | "The methods are similar" (omits metric comparison details) |
| **Factual** | Misreading visual elements | Misinterpreting axes, legends, or data values in a chart |
| **Conceptual** | Wrong conclusion from correct data | Confusing lower loss with worse performance (inverted logic) |

These error types are:
- **Specified by users** when curating exemplars in seed.py (Step 4)
- **Generated by LLM** in icl.py to create realistic student mistakes (Step 6)
- **Tracked globally** to balance distribution across the dataset

---

## 4-Tuple Structure

Every generated example follows a standardized **4-tuple** format matching the StudyCoach report specification:

| Field | Source | Content | Used By |
|-------|--------|---------|---------|
| **Context** | SPIQA | Visual + textual context (figure + caption) | Model receives as visual input |
| **Question** | SPIQA | The figure-grounded question to answer | Model receives as text input |
| **User Answer** | Generated (synthetic) | A student's explanation (correct/partial/incorrect) | icl.py generates using exemplars |
| **Model Response** | Generated (synthetic) | Verdict + error category + coaching feedback | icl.py produces structured output |

**Structured Model Response Breakdown:**

| Component | Values | Purpose | How It's Generated |
|-----------|--------|---------|-------------------|
| **Verdict** | Correct / Partially Correct / Incorrect | Classifies the student's answer | LLM determines if answer is right/wrong |
| **Error Category** | Omission / Factual / Conceptual | Identifies *why* the answer is wrong | LLM selects from 3 error types (guided by exemplars) |
| **Feedback** | Free-text (2-4 sentences) | Study coach explanation | LLM generates constructive, concise guidance |

---

## Core Scripts

### seed.py - Exemplar Generation
**Report Reference:** Phase 2, Step 4 — Hand-Curate ICL Seed Set

**Report Requirement:**
> "Manually create 20–30 high-quality 4-tuple examples (5–8 per error category across figure types)."

**Purpose:** Generate high-quality exemplar examples for in-context learning (ICL) bootstrapping. Seed examples teach the LLM how to generate different types of student errors (Omission, Factual, Conceptual) on scientific paper figure questions. Once manually curated, these exemplars are used by icl.py during Phase 2, Step 6 for the main inference pipeline.

**Required Inputs (8 Parameters)**

| # | Parameter | Type | Example | Source |
|---|-----------|------|---------|--------|
| 1 | `image_path` | str | `'data/test-A/SPIQA_testA_Images/1702.08694v3/1702.08694v3-Figure3-1.png'` | SPIQA image file |
| 2 | `caption` | str | `'Figure 3: Results on real data...'` | SPIQA figure metadata |
| 3 | `question` | str | `'How does C-Tarone compare to binarization?'` | SPIQA qa.question |
| 4 | `answer` | str | `'The C-Tarone method has higher precision...'` | SPIQA qa.answer (ground truth) |
| 5 | `verdict` | str | `'incorrect'` or `'partially correct'` | From verdicts dict — one of the 4-tuple values |
| 6 | `error_category` | str | `'omission'`, `'factual'`, or `'conceptual'` | From error_categories dict — one of the 3 error types |
| 7 | `verdict_explanation` | str | `'an answer which gets none of the required key insights correct'` | verdicts[verdict] |
| 8 | `error_category_explanation` | str | `'an error due to omitting key details in the answer'` | error_categories[error_category] |

**Example Function Call**
```python
chart_incorrect_omission = generate_seed_example(
    image_path=chart1,                                    # ← Input 1
    caption=chart1_caption,                               # ← Input 2
    question=chart1_question,                             # ← Input 3
    answer=chart1_answer,                                 # ← Input 4 (ground truth)
    verdict='incorrect',                                  # ← Input 5 (4-tuple component)
    error_category='omission',                            # ← Input 6 (error type from system)
    verdict_explanation=verdicts['incorrect'],            # ← Input 7
    error_category_explanation=error_categories['omission']  # ← Input 8
)
```

**Output Structure**

Generates one exemplar with these 4 fields (the core 4-tuple components):

| Field | Format | Example | 4-Tuple Role |
|-------|--------|---------|-------------|
| `verdict` | Single word | `Incorrect` | Model Response (Verdict) |
| `error_category` | Single word | `Omission` | Model Response (Error Category) |
| `student_answer` | Multi-sentence | `The C-Tarone method is generally similar to the binarization method...` | User Answer |
| `feedback` | Multi-sentence coaching | `Your answer omits key details... According to the figure, C-Tarone consistently has higher precision...` | Model Response (Feedback) |

**Process**
1. Provide image, caption, question, and ground truth answer
2. Specify desired error type (verdict + error_category) — choose from the **3 error types**
3. Call `generate_seed_example()` with all 8 required inputs
4. LLM generates a synthetic wrong student answer + coaching feedback matching the error type
5. Manually review output for quality
6. Hardcode high-quality exemplars into icl.py (currently lines 330–368)

**Curation Guidelines**
- Target: 20–30 total exemplars across all figure types
- Distribution: 5–8 per error category (Omission, Factual, Conceptual)
- Figure types: Include exemplars for 'plot', 'figure', and 'table'
- Quality: Prefer subtle, realistic errors over obvious mistakes
- Manual review: All generated exemplars must be reviewed before use
- Error balance: Ensure all 3 error types are well-represented for Step 6 to learn diverse error patterns

**Usage**
```bash
python src/seed.py
```

**Output Destination**
Generated exemplars should be saved to `data/seed_exemplars.json` for Step 6 (icl.py) to load dynamically during the main inference pipeline.

---

### icl.py - Inference Engine
**Report Reference:** Phase 2, Step 6 — ICL-Based Synthetic Generation

**Report Requirement:**
> "Prompt an LLM with 2–3 seed examples to generate 1–2 incongruent user answers + structured model responses per QA."

**Purpose:** Generate synthetic student responses using in-context learning across entire SPIQA Test-A dataset. Takes curated seed exemplars from Step 4 (with all 3 error types represented) and uses them to teach the LLM how to generate realistic student errors on scientific paper figure questions. Produces 1–2 synthetic incongruent (wrong) answers per QA with structured model responses (verdict + error category + feedback) — all following the 4-tuple format.

**Required Inputs (4 Parameters)**

| # | Parameter | Type | Example | Source | Notes |
|---|-----------|------|---------|--------|-------|
| 1 | `json_path` | str/Path | `'data/SPIQA_testA_part1.json'` | SPIQA Test-A dataset | Nested JSON: paper_id → all_figures → qa list (context + question for 4-tuples) |
| 2 | `images_root` | str/Path | `'data/test-A/SPIQA_testA_Images'` | Image directory | Contains paper folders with figure images (visual context for 4-tuples) |
| 3 | `exemplars` | dict | `{'plot': [...], 'figure': [...], 'table': [...]}` | seed.py output | 2–3 exemplars per figure type; each exemplar has Omission/Factual/Conceptual error example |
| 4 | `output_path` | str/Path | `'data/SPIQA_testA_part1_output.json'` | Output file | Updated JSON with inference results (4-tuples with all fields) |

**Example Command**
```bash
python src/icl.py data/SPIQA_testA_part1.json data/test-A/SPIQA_testA_Images \
  --output data/SPIQA_testA_part1_output.json
```

**Exemplar Format (Input 3)**
Shows how seed.py exemplars feed into icl.py — each exemplar demonstrates one error type:

```python
exemplars = {
    'plot': [
        # Exemplar 1: OMISSION error
        (user_prompt_omission, assistant_response_omission, 'image_path_1.png'),
        # Exemplar 2: FACTUAL error
        (user_prompt_factual, assistant_response_factual, 'image_path_2.png'),
        # Exemplar 3: CONCEPTUAL error
        (user_prompt_conceptual, assistant_response_conceptual, 'image_path_3.png')
    ],
    'figure': [...],  # Same 3 error types
    'table': [...]    # Same 3 error types
}
```

**Output Structure**

Each QA item in the output JSON gains 5 new fields following the 4-tuple + supplementary format:

| Field | Type | Example | 4-Tuple Role | Purpose |
|-------|------|---------|-------------|---------|
| `student` | str | `'The C-Tarone method is generally similar...'` | User Answer | Generated wrong student answer (synthetic) |
| `verdict` | str | `'Incorrect'` | Model Response (Verdict) | Classification: Correct / Partially Correct / Incorrect |
| `error_category` | str | `'Omission'` | Model Response (Error Category) | Error type: Omission / Factual / Conceptual / N/A (from 3-type system) |
| `feedback` | str | `'Your answer omits key details...'` | Model Response (Feedback) | Study coach explanation of the error (2–4 sentences) |
| `correct_answer` | str | `'The C-Tarone method has higher precision...'` | Context (supplementary) | Ground truth answer (from SPIQA) — used for validation |

**Example Output QA Item (Complete 4-Tuple + Context)**
```json
{
  "question": "How does the C-Tarone method compare to the binarization method?",
  "answer": "The C-Tarone method has higher precision and F-measure...",
  "reference": "1702.08694v3-Figure3-1.png",
  "student": "The C-Tarone method is generally similar to the binarization method across all datasets.",
  "verdict": "Incorrect",
  "error_category": "Omission",
  "feedback": "Your answer omits key details regarding the differences between the two methods. According to the figure, C-Tarone consistently has higher precision and F-measure than binarization.",
  "correct_answer": "The C-Tarone method has higher precision and F-measure..."
}
```

**Process**

1. **Load Data** — Reads SPIQA JSON (118 papers, 666 QA pairs) - provides context + questions for 4-tuples
2. **Iterate in Sorted Order** — Processes papers alphabetically (deterministic for reproducibility)
3. **For Each QA:**
   - Extract question, figure reference, caption from SPIQA metadata
   - Look up figure details: content_type, figure_type
   - Normalize to figure_category ('plot', 'figure', or 'table')
   - Load 2–3 seed exemplars matching that figure category (which contain all 3 error types)
   - If no exemplars exist for that figure type, skip QA
4. **Build Multimodal Prompt:**
   - System instructions (with dynamic error counts: $FACTUAL, $OMISSION, $CONCEPTUAL)
   - 2–3 exemplar messages showing Omission/Factual/Conceptual examples (from seed.py)
   - Final user message (caption + question + current image as base64)
5. **Call OpenAI API:**
   - Model: gpt-5.1
   - Endpoint: `client.responses.create()` with `instructions` parameter
   - Include metadata (paper_id, qa_index) for debugging
6. **Parse Output:**
   - Use regex to extract: Student answer, Verdict, Error Category, Feedback (the 4-tuple components)
   - Clean whitespace and handle edge cases
   - If verdict is "Correct", auto-set error_category and feedback to "N/A"
7. **Update JSON:**
   - Add parsed fields to QA item (completes the 4-tuple)
   - Track error distribution by error_category (increment FACTUAL/OMISSION/CONCEPTUAL counters)
8. **Exit Condition:**
   - Stop early when all three error types (FACTUAL > 0, OMISSION > 0, CONCEPTUAL > 0) have been generated at least once
9. **Write Output:**
   - Save updated JSON to output_path
   - Print final error distribution counts showing balance across 3 error types

**Usage**
```bash
python src/icl.py data/SPIQA_testA_part1.json data/test-A/SPIQA_testA_Images \
  --output data/SPIQA_testA_part1_output.json
```