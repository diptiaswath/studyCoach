# studyCoach

CMU MultiModal ML Course Project

Contributors: [Budhika, Jiashan, Khubi, Dipti]

## Overview

StudyCoach generates **synthetic student responses** for the SPIQA (Scientific Paper Image Question Answering) dataset using in-context learning. The system creates realistic student errors - incorrect/partially correct, on figure/table based questions, with structured error classification for evaluation research.

### Error Classification System

Three core error types guide all parts of the pipeline:

| Error Type     | Definition                                | Example                                                      |
| -------------- | ----------------------------------------- | ------------------------------------------------------------ |
| **Omission**   | Missing key details in the student answer | "The methods are similar" (omits metric comparison details)  |
| **Factual**    | Misreading visual elements                | Misinterpreting axes, legends, or data values in a chart     |
| **Conceptual** | Wrong conclusion from correct data        | Confusing lower loss with worse performance (inverted logic) |

These error types are:

- **Specified by users** when curating exemplars in seed.py
- **Generated by LLM** in icl.py where SPIQA is augmented with realistic student mistakes to produce SPIQA+
- **Tracked globally** to balance distribution across the dataset

---

## 4-Tuple Structure (SPIQA+)

Every generated example follows a standardized **4-tuple** format matching the StudyCoach report specification:

| Field              | Source                | Content                                             | Used By                           |
| ------------------ | --------------------- | --------------------------------------------------- | --------------------------------- |
| **Context**        | SPIQA                 | Visual + textual context (figure + caption)         | Model receives as visual input    |
| **Question**       | SPIQA                 | The figure-grounded question to answer              | Model receives as text input      |
| **User Answer**    | Generated (synthetic) | A student's explanation (correct/partial/incorrect) | icl.py generates using exemplars  |
| **Model Response** | Generated (synthetic) | Verdict + error category + coaching feedback        | icl.py produces structured output |

**Structured Model Response Breakdown:**

| Component          | Values                                  | Purpose                              | How It's Generated                                   |
| ------------------ | --------------------------------------- | ------------------------------------ | ---------------------------------------------------- |
| **Verdict**        | Correct / Partially Correct / Incorrect | Classifies the student's answer      | LLM determines if answer is right/wrong              |
| **Error Category** | Omission / Factual / Conceptual         | Identifies _why_ the answer is wrong | LLM selects from 3 error types (guided by exemplars) |
| **Feedback**       | Free-text (2-4 sentences)               | Study coach explanation              | LLM generates constructive, concise guidance         |

---

## Core Scripts

### seed.py - Exemplar Generation

**Purpose:** Generate high-quality exemplar examples that teach icl.py how to generate realistic student errors across the 3 error types.

**Inputs (8 Parameters)**

| #   | Parameter                    | Type | Source                                 |
| --- | ---------------------------- | ---- | -------------------------------------- |
| 1   | `image_path`                 | str  | SPIQA image file path                  |
| 2   | `caption`                    | str  | SPIQA figure caption                   |
| 3   | `question`                   | str  | SPIQA question text                    |
| 4   | `answer`                     | str  | SPIQA ground truth answer              |
| 5   | `verdict`                    | str  | 'incorrect' or 'partially correct'     |
| 6   | `error_category`             | str  | 'omission', 'factual', or 'conceptual' |
| 7   | `verdict_explanation`        | str  | Definition of chosen verdict           |
| 8   | `error_category_explanation` | str  | Definition of chosen error type        |

**Outputs (4 Fields)**

| Field            | 4-Tuple Role                    |
| ---------------- | ------------------------------- |
| `student_answer` | User Answer                     |
| `verdict`        | Model Response (Verdict)        |
| `error_category` | Model Response (Error Category) |
| `feedback`       | Model Response (Feedback)       |

**Exemplar Distribution**

Generate 18 exemplars across three consolidated figure types (based on SPIQA Test-A breakdown):

| Figure Type | Dataset Distribution                                          | Error Categories              | Verdicts                     | Exemplars |
| ----------- | ------------------------------------------------------------- | ----------------------------- | ---------------------------- | --------- |
| **Table**   | 923 items (44.3%)                                             | Omission, Factual, Conceptual | Incorrect, Partially Correct | 6         |
| **Plot**    | 585 items (28.1%)                                             | Omission, Factual, Conceptual | Incorrect, Partially Correct | 6         |
| **Figure**  | 565 items (27.6%): Schematics (363), Photos (173), Other (40) | Omission, Factual, Conceptual | Incorrect, Partially Correct | 6         |

**Result: 3 types × 3 categories × 2 verdicts = 18 exemplars**

---

### icl.py - Inference Engine

**Purpose:** Augment SPIQA dataset with realistic student errors to produce **SPIQA+**. Uses exemplars from seed.py to generate synthetic student responses (incorrect/partially correct answers + structured model responses) at scale across the entire SPIQA Test-A dataset. Outputs SPIQA+ — the enhanced dataset with complete 4-tuple examples ready for evaluation research.

**Inputs (4 Parameters)**

| #   | Parameter     | Type     | Source                                                             |
| --- | ------------- | -------- | ------------------------------------------------------------------ |
| 1   | `json_path`   | str/Path | SPIQA Test-A JSON file (provides context + questions)              |
| 2   | `images_root` | str/Path | Directory containing paper image folders (provides visual context) |
| 3   | `exemplars`   | dict     | seed.py output: `{'plot': [...], 'figure': [...], 'table': [...]}` |
| 4   | `output_path` | str/Path | Where to write the updated JSON with inference results             |

**Exemplar Input Format**

Each exemplar tuple teaches one error type:

```python
exemplars = {
    'plot': [
        (user_prompt_omission, assistant_response_omission, image_path_1),
        (user_prompt_factual, assistant_response_factual, image_path_2),
        (user_prompt_conceptual, assistant_response_conceptual, image_path_3)
    ],
    'figure': [(...), (...), (...)],  # Same 3 error types
    'table': [(...), (...), (...)]    # Same 3 error types
}
```

**Outputs (4 Fields Added Per QA)**

| Field            | 4-Tuple Role                    | Description                             |
| ---------------- | ------------------------------- | --------------------------------------- |
| `student`        | User Answer                     | Synthetic wrong student answer          |
| `verdict`        | Model Response (Verdict)        | Correct / Partially Correct / Incorrect |
| `error_category` | Model Response (Error Category) | Omission / Factual / Conceptual / N/A   |
| `feedback`       | Model Response (Feedback)       | Study coach explanation (2–4 sentences) |

**Example Output**

```json
{
  "question": "What is the relationship between BLEU score and human ranking for CCA and SMT systems?",
  "answer": "The correlation between BLEU scores and human ranking is not high for either CCA or SMT systems.",
  "explanation": "The passage states that the correlation between the x-axis (ranking) and y-axis (BLEU scores) for CCA is 0.3 and for the SMT system 0.31. This indicates a weak positive correlation.",
  "reference": "1608.02784v2-Figure4-1.png",
  "student": "For both CCA and SMT systems, there is basically no relationship between BLEU score and human ranking; the BLEU scores stay about the same regardless of the rating.",
  "verdict": "Incorrect",
  "error_category": "Conceptual",
  "feedback": "You're misunderstanding the overall trend in the scatter plot. Even though the points are scattered, BLEU scores for both CCA (red) and SMT (blue) tend to be higher at higher human ratings."
}
```

**Usage**

```bash
python src/icl.py data/SPIQA_testA_part1.json data/test-A/SPIQA_testA_Images \
  --output data/SPIQA_testA_part1_output.json
```

## Usage – Evaluation and Human Annotation

### 1. Automatic Metric Scoring

We compute the following metrics between:

- `ground_truth.feedback`
- `predicted.feedback`

Metrics:

- F1 (token-level)
- ROUGE-L
- BLEU

#### Run Scoring

From the project root:

```bash
python -m src.eval.score_feedback \
  --in_dir data/eval \
  --out_scored_dir data/eval_scored \
  --out_summary_dir data/eval_summary
```

#### Outputs

Per-model scored files:

```
data/eval_scored/<model>.scored.jsonl
```

Summary table:

```
data/eval_summary/eval_metrics_summary.csv
```

Each scored record includes:

- `metrics.feedback.f1`
- `metrics.feedback.rougeL`
- `metrics.feedback.bleu`

---

### 2. Human Annotation (Shared Sampling)

To fairly compare models, we use shared sampling:

- The same `k` records are sampled once
- The same indices are applied to all model files
- Annotation is performed file-by-file

Labels:

- `m` → match
- `u` → unmatched
- `p` → partial / unclear

#### Run Annotation

```bash
python -m src.eval.annotate_feedback \
  --in_dir data/eval_scored \
  --out_dir data/human_annotated \
  --k 10 \
  --seed 42
```

This will:

- Sample 10 shared indices
- Prompt for annotation per model file
- Save annotated outputs to:

```
data/human_annotated/<model>.annotated.jsonl
```

Each annotated record includes:

- `human_label` ("match", "unmatched", "partial/unclear")
- `human_match` (1, 0, 2)
- `human_notes` (optional)
- `sample_index` (shared across models)

The shared sampling indices are stored in:

```
data/human_annotated/annotation_index.json
```

---

### 3. Resume Annotation

If annotation is interrupted, you can resume safely:

```bash
python -m src.eval.annotate_feedback \
  --in_dir data/eval_scored \
  --out_dir data/human_annotated \
  --k 10 \
  --seed 42 \
  --resume
```

This will:

- Reuse the same shared indices
- Skip already annotated files

---

### 4. Human vs. Automatic Metrics Comparison

After completing human annotation, you can generate a summary comparison between:

- Human judgments (`match / unmatched / partial`)
- Automatic metrics (F1, ROUGE-L, BLEU)

Run:

```bash
python -m src.eval.summarize_human_vs_metrics \
  --in_dir data/human_annotated \
  --out_dir data/eval_summary
```
